---
title: "Statistics Session 02: Data Types"
subtitle: "Data Types, Central Tendancy"
categories: [statistics]
---



# Agenda

1. Types of Data
2. Central Tendancy measurs
3. Intro to visualization


## What is Statistics?

Statistics is the branch of mathematics that transforms numbers (data) into useful information
fohr decision makers.

**Why learn Statistics?**

1. statistics helps you make better sense of the world.
2. statistics helps you make better business decisions.

The statistical methods you use for these tasks come from one of the two branches of statistics:
*descriptive statistics* and *inferential statistics.*


## Basic Vocabulary of Statistics

- **Variable:** A variable is a characteristic of an item or individual.
- **Data:** Data are the different values associated with a variable.
- **Population:** A population consists of all the items or individuals about which you want to
reach conclusions.
- **Sample:** A sample is the portion of a population selected for analysis.
- **Parameter:** A parameter is a measure that describes a characteristic of a population.
- **Statistic:** A statistic is a measure that describes a characteristic of a sample.


# Types of Data

Understanding the **types of data** is the foundation of every analysis or visualization.  
Different data types require different summary statistics, charts, and interpretations.

---

## Why Data Types Matter?

Before calculating averages or plotting charts, it’s essential to recognize what kind of data you’re working with.  
The classification of a variable determines:

- Which **summary statistics** are meaningful  
- Which **visualizations** can be used  
- How relationships between variables should be **interpreted**

---

## Qualitative (Categorical) Data

Qualitative data describe **qualities, categories, or labels** rather than numbers.

| **Subtype** | **Definition** | **Examples** |
|--------------|----------------|---------------|
| **Nominal** | Categories with **no natural order** | Gender (Male/Female), City (Paris, Yerevan, Tokyo) |
| **Ordinal** | Categories with a **meaningful order**, but **unequal spacing** | Education Level (High < Bachelor < Master < PhD), Satisfaction (Low–Medium–High) |

> **Remember:**  
> *Nominal* → just names  
> *Ordinal* → names **with rank**

---

## Quantitative (Numerical) Data

Quantitative data represent **measurable quantities** that can be used in arithmetic operations.

| **Subtype** | **Definition** | **Examples** |
|--------------|----------------|---------------|
| **Discrete** | Countable numbers, often integers | Number of customers, Complaints per day |
| **Continuous** | Measured values within a range | Temperature, Age, Revenue, Weight |

### Interval Data
Interval values represent **ordered units that have the same difference**.  
A variable is considered interval data when it contains numeric values that are ordered and where the exact differences between values are known.

**Example (Temperature in Celsius):**  

[-15, -10, -5, **0,** 5, 10, 15, 20, 25, 30]

A key limitation of interval data is that it **does not have a true zero**.  
In the Celsius example, there is no such thing as “no temperature.”

### Ratio Data

Ratio values are also **ordered units that have the same difference**, similar to interval values.  
The key distinction is that ratio data **has an absolute zero**.

**Examples:** Income, Weight, Length, etc.


> **Tip:**  
> *Discrete* → counting  
> *Continuous* → measuring

---

## Additional Data Types (Common in Analytics)

| **Type** | **Definition** | **Example Applications** |
|-----------|----------------|---------------------------|
| **Binary** | Two possible outcomes (Yes/No, 0/1) | Subscription status, churn indicator |
| **Time-Series** | Observations recorded sequentially over time | Daily sales, hourly temperature |
| **Textual / Unstructured** | Words, sentences, or documents | Customer reviews, tweets |
| **Spatial / Geographical** | Location-based information | Store coordinates, delivery zones |


```{mermaid}
%%{init: {"theme": "default", "logLevel": "fatal"}}%%
graph TD
    A["Data Types"] --> B["Qualitative (Categorical)"]
    A --> C["Quantitative (Numerical)"]
    A --> H["Other Types"]

    B --> D["Nominal"]
    B --> E["Ordinal"]

    C --> F["Discrete"]
    C --> G["Continuous"]

    
    H --> I["Binary"]
    H --> J["Time-Series"]
    H --> K["Textual"]
    H --> L["Spatial"]
```

# Measures of Central Tendency

Once we understand our data types, the next step is to summarize them with simple, representative numbers.  
These are called **measures of central tendency**, and they tell us where the “center” of our data lies.

The three most common measures are:

- **Mean or Average** — the average value  
- **Median** — the middle value  
- **Mode** — the most frequent value  

---

## Mean

The **mean** is the sum of all values divided by the number of observations.


$$
\text{Mean} = \frac{\sum_{i=1}^{n} x_i}{n}
$$


| Example | Calculation |
|----------|--------------|
| Data: 5, 7, 8, 10 | Mean = (5 + 7 + 8 + 10) / 4 = **7.5** |


#### Mean for Sample



$$
\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}
$$


#### Population Mean  
(same, only notation is different)


$$
\mu = \frac{\sum_{i=1}^{N} x_i}{N}
$$

### Weighted Mean  

| Type      | Score | Weight (%) |
|-----------|-------|------------|
| Exam      | 94    | 50         |
| Project   | 92    | 35         |
| Homework  | 100   | 15         |


(*Weights do not need to add up to one!*)

$$
\bar{x} = 
\frac{\displaystyle \sum_{i=1}^{n} (w_i x_i)}
     {\displaystyle \sum_{i=1}^{n} w_i}
$$


### Sample Mean Example



> **When to Use:**  
> - Conveinent measurement clear to everybody
> - Works well with **continuous or discrete numerical data**.  
> - Sensitive to **outliers** (extreme values can distort the result).

Given the 5 observations:


$$
\bar{x} 
= \frac{\sum_{i=1}^{5} x_i}{5} 
= \frac{14.2 + 19.6 + 22.7 + 13.1 + 20.9}{5} 
= \frac{90.5}{5}
= 18.1 \
$$


---

$$
\mu
= \frac{\sum_{i=1}^{5} x_i}{5}
= \frac{300 + 320 + 270 + 210 + 8{,}000}{5}
= \frac{9{,}100}{5}
= 1{,}820
$$

---

## Median

The **median** is the value that separates the dataset into two equal halves.

**Steps to calculate:**

1. Order the data from smallest to largest.  
2. If the number of observations is odd → middle value.  
3. If even → average of the two middle values.

| Example | Calculation |
|----------|--------------|
| Data: 2, 5, 7, 9, 12 | Median = 7 |
| Data: 3, 5, 8, 10 | Median = (5 + 8)/2 = **6.5** |

> **When to Use:**  
> 
> - Better than the mean for **skewed data** or when **outliers** are present.  
> - Common for **income, property prices**, etc.

---

## Mode

The **mode** is the value that appears most often.

| Example | Calculation |
|----------|--------------|
| Data: 2, 3, 3, 4, 5, 5, 5, 7 | Mode = **5** |

> **When to Use:**  
> - Ideal for **categorical or discrete data**.  
> - A dataset can have:
>   - **One mode** → unimodal  
>   - **Two modes** → bimodal  
>   - **More than two** → multimodal

---

## Comparison of Mean, Median, and Mode

| **Measure** | **Best For** | **Sensitive to Outliers?** | **Data Type** | **Example Context** |
|--------------|--------------|-----------------------------|----------------|----------------------|
| **Mean** | Symmetrical distributions | Yes | Continuous, Discrete | Average income |
| **Median** | Skewed distributions | No | Continuous | Typical housing price |
| **Mode** | Categorical / Repeated values | No | Nominal, Ordinal | Most common product category |

> **Visual Insight:**  
> In a **perfectly symmetrical** distribution → Mean = Median = Mode  
> In a **right-skewed** distribution → Mean > Median > Mode  
> In a **left-skewed** distribution → Mean < Median < Mode


## Frequency Distributions

### Symmetric Distribution {.smaller}

Mean ~ Median

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1)

# Symmetric data
data_sym = np.random.normal(loc=50, scale=10, size=500)

mean_sym = np.mean(data_sym)
median_sym = np.median(data_sym)

fig, ax = plt.subplots(figsize=(7,4))
ax.hist(data_sym, bins=20, edgecolor="black", alpha=0.7)

# Add vertical lines
ax.axvline(mean_sym, color="blue", linestyle="--", linewidth=2, label=f"Mean = {mean_sym:.2f}")
ax.axvline(median_sym, color="red", linestyle="-", linewidth=2, label=f"Median = {median_sym:.2f}")

ax.set_title("Symmetric Distribution (Mean ≈ Median)")
ax.set_xlabel("Value")
ax.set_ylabel("Frequency")
ax.legend()

plt.show()
```


### Left Skewed Distributions

```{python}
#| echo: false
# Left-skewed distribution (reversed gamma)
data_left = 100 - np.random.gamma(shape=5, scale=4, size=500)

mean_left = np.mean(data_left)
median_left = np.median(data_left)

fig, ax = plt.subplots(figsize=(7,4))
ax.hist(data_left, bins=20, edgecolor="black", alpha=0.7)

ax.axvline(mean_left, color="blue", linestyle="--", linewidth=2, label=f"Mean = {mean_left:.2f}")
ax.axvline(median_left, color="red", linestyle="-", linewidth=2, label=f"Median = {median_left:.2f}")

ax.set_title("Left-Skewed Distribution (Mean < Median)")
ax.set_xlabel("Value")
ax.set_ylabel("Frequency")
ax.legend()

plt.show()
```


### Right Skewed Distributions

```{python}
#| echo: false
# Right-skewed distribution (gamma)
data_right = np.random.gamma(shape=4, scale=4, size=500)

mean_right = np.mean(data_right)
median_right = np.median(data_right)

fig, ax = plt.subplots(figsize=(7,4))
ax.hist(data_right, bins=20, edgecolor="black", alpha=0.7)

ax.axvline(mean_right, color="blue", linestyle="--", linewidth=2, label=f"Mean = {mean_right:.2f}")
ax.axvline(median_right, color="red", linestyle="-", linewidth=2, label=f"Median = {median_right:.2f}")

ax.set_title("Right-Skewed Distribution (Mean > Median)")
ax.set_xlabel("Value")
ax.set_ylabel("Frequency")
ax.legend()

plt.show()
```



## Symmetric, Left-Skewed, and Right-Skewed Distributions  


```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1)

# ---- Generate data ----
data_sym  = np.random.normal(loc=50, scale=10, size=500)
data_left = 100 - np.random.gamma(shape=5, scale=4, size=500)
data_right = np.random.gamma(shape=4, scale=4, size=500)

datasets = [
    ("Symmetric\nMean ≈ Median", data_sym),
    ("Left-Skewed\nMean < Median", data_left),
    ("Right-Skewed\nMean > Median", data_right)
]

# ---- Create subplots ----
fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # <-- Reduced width + height
bins = 18  # Slightly fewer bins to clean visuals

for ax, (title, data) in zip(axes, datasets):

    mean_val = np.mean(data)
    median_val = np.median(data)

    ax.hist(data, bins=bins, edgecolor="black", alpha=0.7)

    # Mean line (blue dashed)
    ax.axvline(mean_val, color="blue", linestyle="--", linewidth=1.8, label=f"Mean")

    # Median line (red solid)
    ax.axvline(median_val, color="red", linestyle="-", linewidth=1.8, label=f"Median")

    ax.set_title(title, fontsize=11)
    ax.set_xlabel("Value", fontsize=9)
    ax.set_ylabel("Frequency", fontsize=9)

    ax.tick_params(axis='both', which='major', labelsize=8)
    ax.legend(fontsize=8, loc="upper right")

plt.tight_layout(pad=1)
plt.show()
```


## Enriched Plot

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

np.random.seed(1)

# ---- Generate data ----
data_sym  = np.random.normal(loc=50, scale=10, size=500)
data_left = 100 - np.random.gamma(shape=5, scale=4, size=500)
data_right = np.random.gamma(shape=4, scale=4, size=500)

datasets = [
    ("Symmetric\nMean ≈ Median", data_sym),
    ("Left-Skewed\nMean < Median", data_left),
    ("Right-Skewed\nMean > Median", data_right)
]

# ---- Create subplots ----
fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))  # slightly wider for annotations
bins = 20

for ax, (title, data) in zip(axes, datasets):

    mean_val = np.mean(data)
    median_val = np.median(data)

    # Histogram
    counts, bins_hist, patches = ax.hist(
        data, bins=bins, alpha=0.6, edgecolor="black"
    )

    # KDE smooth density
    kde = gaussian_kde(data)
    xline = np.linspace(min(data), max(data), 400)
    ax.plot(xline, kde(xline) * len(data) * (bins_hist[1] - bins_hist[0]),
            color="black", linewidth=1.5, label="KDE")

    # Mean line (blue dashed)
    ax.axvline(mean_val, color="blue", linestyle="--", linewidth=2)
    ax.annotate("Mean", xy=(mean_val, max(counts)*0.9),
                xytext=(mean_val, max(counts)*1.1),
                ha="center", color="blue", fontsize=8,
                arrowprops=dict(arrowstyle="->", color="blue"))

    # Median line (red solid)
    ax.axvline(median_val, color="red", linestyle="-", linewidth=2)
    ax.annotate("Median", xy=(median_val, max(counts)*0.9),
                xytext=(median_val, max(counts)*1.1),
                ha="center", color="red", fontsize=8,
                arrowprops=dict(arrowstyle="->", color="red"))

    # Shaded tails depending on skew
    if "Left" in title:
        ax.axvspan(min(data), mean_val, alpha=0.12, color="blue")
    elif "Right" in title:
        ax.axvspan(mean_val, max(data), alpha=0.12, color="blue")

    # Titles & labels
    ax.set_title(title, fontsize=12)
    ax.set_xlabel("Value", fontsize=9)
    ax.set_ylabel("Frequency", fontsize=9)
    ax.tick_params(axis="both", labelsize=8)

    # Summary table under plot
    table_data = [
        ["Mean", f"{mean_val:.2f}"],
        ["Median", f"{median_val:.2f}"],
        ["Std Dev", f"{np.std(data):.2f}"]
    ]
    table = ax.table(
        cellText=table_data,
        colLabels=["Statistic", "Value"],
        cellLoc="center",
        loc="bottom",
        fontsize=8
    )
    table.scale(1, 1.1)

plt.tight_layout(pad=2)
plt.show()

```



# Measures of Variability

## Measures of Variability

- Range
- Variance 
- Standard Deviation (SD)


## Range

$$
\text{Range = Highest Value - Lowest Value}
$$

> - Think about where is it applicable?
> - Are there any limitations

## Variance

**Sample Variance:**

$$
s^{2} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^{2}}{n - 1}
$$

**Population Variance**

$$
\sigma^{2}
=
\frac{
\displaystyle \sum_{i=1}^{N} (x_i - \mu)^{2}
}{
N
}
$$

## Standard Deviation

**Sample Standard Deviation (Standard Error)**

$$
s \;=\; \sqrt{s^{2}}
\;=\;
\sqrt{
\frac{
\displaystyle \sum_{i=1}^{n} (x_i - \bar{x})^{2}
}{
n - 1
}
}
$$

**Population standard deviation**

$$
\sigma
=
\sqrt{
\frac{
\displaystyle \sum_{i=1}^{N} (x_i - \mu)^{2}
}{
N
}
}
$$

## How to compare variability?

In which cases you can compare standard deviations?

| Box     | Sample 1  | Sample 2  |
|---------|---------------|----------------|
| Box 1   | 14.2          | 18.2           |
| Box 2   | 19.6          | 17.9           |
| Box 3   | 22.7          | 18.1           |
| Box 4   | 13.1          | 18.1           |
| Box 5   | 20.9          | 18.2           |
| **Mean** | **18.1**       | **18.1**        |
| **Standard deviation** | **4.23** | **0.12** |


### Visual Comparison of Variability

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt

# Data from the table
sample1 = np.array([14.2, 19.6, 22.7, 13.1, 20.9])
sample2 = np.array([18.2, 17.9, 18.1, 18.1, 18.2])

fig, ax = plt.subplots(figsize=(8,4))

ax.scatter(np.ones_like(sample1), sample1, color="red", label="Sample 1 (High Variability)", s=70)
ax.scatter(2*np.ones_like(sample2), sample2, color="blue", label="Sample 2 (Low Variability)", s=70)

# Add jitter for aesthetics
ax.scatter(1 + np.random.uniform(-0.03, 0.03, len(sample1)), sample1, color="red", s=70)
ax.scatter(2 + np.random.uniform(-0.03, 0.03, len(sample2)), sample2, color="blue", s=70)

ax.set_xticks([1,2])
ax.set_xticklabels(["Sample 1", "Sample 2"])

ax.set_ylabel("Weight (oz)")
ax.set_title("Visual Comparison of Variability")

ax.legend()
plt.show()
```

### When variability is **bad**?

- When consistency and quality control are important  
- Product weights, drug dosages, machine precision  
- Delivery times, service response times  
- Financial risk (greater uncertainty)

### When variability is **good**?

- Biological diversity and adaptability  
- Marketing segmentation & A/B testing  
- Creativity and innovation  
- Investments seeking higher upside  
- Identifying top performers (sports, hiring)

### When variability is **neutral**?

- Natural randomness (weather, height, sampling variation)

 
>**Variability is not inherently good or bad—its value depends on what you are trying to achieve.**


## Why $N-1$?

A **degree of freedom (df)** is an independent piece of information that can vary freely.  
Whenever we estimate a parameter from the sample, we introduce a constraint.  
Each constraint removes one degree of freedom.

The universal rule is:

$$
df = n - \text{(number of estimated parameters)}.
$$

During the mean estimation, since we have one constraint we remove only 1 hence

$$
df = n - 1
$$

<!-- TODO -->


## Coefficient of Variation (Normalization)

The coefficient of variation (CV) is a measure of *relative variability*.  
It allows us to compare the variability of two datasets **even when their units or scales differ**.

---

**Data**

| Date               | Nike  | Google |
|--------------------|----------|------------|
| September 14, 2012 | 48.32    | 709.68     |
| October 15, 2012   | 47.81    | 740.98     |
| November 15, 2012  | 45.42    | 647.26     |
| December 14, 2012  | 48.46    | 701.96     |
| January 15, 2013   | 53.64    | 724.93     |
| February 15, 2013  | 54.95    | 792.89     |
| **Mean**           | **49.77** | **719.62** |
| **Standard deviation** | **3.70** | **47.96** |

---

### Formula for the Coefficient of Variation

$$
\text{CV} = \frac{s}{\bar{x}} \times 100
$$

Where:

- $s$ = standard deviation  
- $\bar{x}$ = mean  
- CV is expressed as a **percentage**

---

**Nike**

$$
\text{CV}_{\text{Nike}}
= \frac{3.70}{49.77} \times 100
\approx 7.4\%
$$


**Google**

$$
\text{CV}_{\text{Google}}
= \frac{47.96}{719.62} \times 100
\approx 6.7\%
$$


Although Google’s stock prices vary by a much larger **dollar amount**, its **relative variability** is smaller.

- Nike CV = **7.4%**  
- Google CV = **6.7%**

This means that, compared to its average price, **Google's stock price is more stable** than Nike's.

The CV helps us compare variability across different scales and units—something raw standard deviations alone cannot do.


## Z-score

Number of standard deviations that particular value is farther from the mean of its population or sample:

**Population:**

$$
z = \frac{x - \mu}{\sigma}
$$

**Sample:**

$$
z = \frac{x - \bar{x}}{s}
$$

```bash
=STANDARDIZE(x, mean, standard_deviation)
```

Suppose we have:

- x = 540
- mean = 776.3
- standard deviation = 385.1

**Rule of Thumb for identifying outliers:** Data values that have z-scores above `+3` and below `-3` can be categorized as outliers.


<!-- 
## Empirical Rule

- When data is close to bell-shaped and symmetrical:
- 68-95-97.5 rule usually applies
- Exam average 80, standard deviation 5:



<!-- todo add plot -->

<!-- ## Chebyshev’s Theorem

Chebyshev’s Theorem applies to **any distribution**, not only symmetric or bell-shaped ones.  
It states that for any value of \( z > 1 \), **at least** the following percentage of observations lie within  
\( z \) standard deviations of the mean:

$$
\left( 1 - \frac{1}{z^2} \right) \times 100
$$ --> 



## Usefull materials

- [Business Statistics 2](https://drive.google.com/file/d/1hpsygZCLHf0kV75Muy0Hn9xiS-AqPCZ9/view?usp=drive_link) Chapter 3 and Chapter 4
- [Here](https://public.tableau.com/app/profile/glowbyte.consulting/viz/ChartChooser_15550897459460/ChartChooser)