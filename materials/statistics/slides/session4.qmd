---
title: "Discrete Distributions"
subtitle: "Probabilistic Distributions"
author: "Karen Hovhannisyan"
date: last-modified
format:
  revealjs: 
    touch: true
    slide-number: false
    chalkboard: true

    preview-links: auto
    width: 1500
    transition: slide
    logo: ../../../img/favicon.png
    footer: "<a href=\"../session4.html\">Back to chapter</a>"
    # css: ../../../reveal_styles.css
    include-in-header:
      - text: |
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
          <style>
            .icon-xxl { font-size: 84px; }
            .icon-xl  { font-size: 64px; }
            .icon-lg  { font-size: 48px; }
            .muted { opacity: .8 }
            .pill { border-radius: 10px; }
            .circle { border-radius: 50%; }
          </style>
---



## Topics

- Discrete Distributions
- Poisson Distribution
- Bernuli Distribution
- Central Limit Theorem | CLT

# The Properties of Discrete Distributions


## When do we have a Discrete Distributions? {.smaller}

A distribution is **discrete** if:

- The random variable takes **countable values**
- Probabilities are assigned to **exact outcomes**
- The total probability is obtained by **summing**, not integrating

**Examples of Discrete Outcomes**

. . .

- Number of purchases made today  
- Number of customers who churned this month  
- Number of defects in a production batch  
- Whether a user clicked an ad (yes / no)  


## Discrete Random Variables


A **discrete random variable** is a numerical description of a countable outcome.

**Typical forms:**

- Binary outcomes: $X \in \{0,1\}$
- Counts: $X = 0,1,2,3,\dots$

**Examples:**

. . .

- $X = 1$ if a customer churns, $0$ otherwise  
- $X$ = number of purchases per user  
- $X$ = number of support tickets per day  


## Probability Mass Function {.smaller}

Discrete distributions are described by a **Probability Mass Function (PMF)**.

**The PMF answers the question:**

> What is the **probability** that the random variable equals a specific value?

. . .

**Mathematically:**

$$
P(X = x)
$$

**Core Properties of a PMF**

$$
\sum_x P(X = x) = 1
$$

- $0 \le P(X = x) \le 1$
- Probabilities are assigned to **exact values**
- The total probability sums to 1:

---

> - Հավանականության խտության ֆունկից ա; 
> - Հավանականության զանգվածի ֆունկցիա

# Poisson Distribution

Poisson models turn **randomness** into **operational clarity**.

---

**How many times does an event occur within a fixed time (or space) interval?**

## Imagine {.smaller}

Imagine a retail store observing customer foot traffic.

Every **10 minutes**, a random number of customers enter the store.

**Over many days, management notices:**

::: {.incremental}
-   Some intervals have 1–2 customers\
-   Some have 5–6 customers\
-   Occasionally, none
:::

. . .

Yet the **average** stays roughly the same.


## Probability Mass Function


$$
P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}, \quad x = 0,1,2,\dots
$$


. . .

This formula gives the probability of seeing *exactly* $x$ events


## Expected Value and Variance {.smaller}


$$
E[X] = \lambda
$$



$$
Var(X) = \lambda
$$

-   The **average** number of events equals $\lambda$\
-   The **uncertainty** (variance) grows at the same rate

. . .

-   Large $\lambda$ → frequent events\
-   Small $\lambda$ → rare events
  

---

-   $\lambda = 2$ → about 2 calls per hour\
-   $\lambda = 15$ → about 15 website visits per minute\
-   $\lambda = 0.2$ → a rare failure, once every 5 intervals
  

## Business Applications of Poisson Distribution {.smaller}

Poisson models are widely used in practice for:

-   customer arrivals and foot traffic\
-   call center volume estimation\
-   website click and impression counts\
-   manufacturing defects\
-   incident and outage reporting


## Poisson Process

-   Exponential distribution models **time between events**
-   Poisson distribution models **number of events**

They describe the **same process** from different perspectives.

---

[Distributions Spreadsheets](https://docs.google.com/spreadsheets/d/1tJNjOvn8m393UluPVnIAT5W30NrreMnWx3Zhzkj6_WM/edit?usp=sharing)



## Visualization

-   Left plot → PMF for $\lambda = 3$ and $\lambda = 10$\
-   Right plot → likelihood as a function of $\lambda$

```{python}
#| echo: false
#| fig-width: 18
#| fig-height: 5
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from math import factorial, exp

# Observed customer counts
obs = np.array([3, 5, 2, 4, 6, 3])
n = len(obs)
S = np.sum(obs)          # total number of customers

def poisson_pmf(k, lam):
    return (lam**k * np.exp(-lam)) / factorial(k)

# PMF range for display
k = np.arange(0, 21)

lam1 = 3
lam2 = 10

p1 = np.array([poisson_pmf(i, lam1) for i in k])
p2 = np.array([poisson_pmf(i, lam2) for i in k])

# Likelihood curve for λ
lam_range = np.linspace(0.1, 15, 500)
L = (lam_range**S) * np.exp(-n * lam_range)  # ignoring factorial constants

# MLE
lam_hat = S / n

fig, axes = plt.subplots(1, 2, figsize=(14, 4))

# ---- Left: PMF comparison ----
axes[0].bar(k, p1, alpha=0.7, label="λ = 3")
axes[0].bar(k, p2, alpha=0.7, label="λ = 10")
axes[0].set_title("Poisson PMF for Two Rates")
axes[0].set_xlabel("k (customer count)")
axes[0].set_ylabel("Probability")
axes[0].legend()

# ---- Right: Likelihood curve ----
axes[1].plot(lam_range, L, linewidth=3) 
axes[1].set_title("Likelihood of λ Given Retail Count Data")
axes[1].set_xlabel("λ (average customers per interval)")
axes[1].set_ylabel("Likelihood")

axes[1].axvline(lam_hat, linestyle="--", linewidth=2)
axes[1].text(
    lam_hat, max(L)*0.8,
    f"MLE λ = {lam_hat:.2f}",
    ha="left"
)

plt.tight_layout()
plt.show()
```


# Bernoulli Distribution

The **Bernoulli Distribution** models the most fundamental probabilistic question in data analytics and business decision-making:


## Two Possible Outcomes {.smaller}

> **Did an event happen or not?**

. . .

A Bernoulli random variable has only **two possible outcomes**:

- success  
- failure  

**These outcomes can be represented in multiple equivalent ways depending on context:**

:::{.incremental}
- yes / no  
- 1 / 0  
- click / no click  
- purchase / no purchase  
- tail / head (ղուշ / գիր)  
:::


## Mathematical Definition

A random variable $X$ follows a **Bernoulli distribution** if:

$$
X \sim \text{Bernoulli}(p)
$$

where:

- $p \in [0,1]$ is the **probability of success**


## What Is a Bernoulli Trial

A **Bernoulli trial** is a single experiment with:

- exactly **one attempt**
- exactly **two possible outcomes**
- a **fixed probability** of success  

Each trial is assumed to be **independent**.

## Examples of Bernoulli Trials

Common real-world Bernoulli trials include:

- Did a user click an advertisement?  
- Did a customer complete a purchase?  
- Did a transaction fail?  
- Did a device respond to a health check?  

Each produces a **binary outcome**.

## Real-World Narrative

Consider an online store tracking customer conversions.

For **each user session**:

- Purchase → `1`  
- No purchase → `0`  

**Across thousands of sessions:**

- Some users convert  
- Most users do not  

Each session is evaluated **independently**, with the same underlying probability of conversion.

## Business Use Cases

Typical Bernoulli use cases in analytics:

- ad click behavior  
- conversion events  
- email engagement  
- fraud detection flags  
- churn indicators  

Bernoulli is the **building block** for many advanced models.

## Probability Mass Function (PMF)

Because Bernoulli is a **discrete** distribution, it is described by a **Probability Mass Function (PMF)**:

$$
P(X = x) = p^x (1-p)^{1-x}, \quad x \in \{0,1\}
$$

## PMF Interpretation

**This formula yields exactly two probabilities:**

- $P(X = 1) = p$  
- $P(X = 0) = 1 - p$  



## Expected Value and Variance

**For a Bernoulli random variable:**

$$
E[X] = p
$$

$$
Var(X) = p(1-p)
$$

## Interpretation of Moments

- Expected value equals the **probability of success**
- Variance is largest at $p = 0.5$
- Variance shrinks as outcomes become more certain

. . .

- Large $p$ → success likely  
- Small $p$ → success rare  

## Concrete Examples

- $p = 0.02$ → 2% conversion rate  
- $p = 0.35$ → 35% email open rate  
- $p = 0.90$ → highly reliable system  



## Business Context

An e-commerce company records whether each visitor completes a purchase.

Each session is encoded as:

- `1` → purchase  
- `0` → no purchase  

### Observed Data

{.smaller}

| Session | Purchase |
|--------:|---------:|
| 1 | 0 |
| 2 | 1 |
| 3 | 0 |
| 4 | 0 |
| 5 | 1 |
| 6 | 0 |
| 7 | 1 |
| 8 | 0 |
| 9 | 0 |
|10 | 1 |

## Summary Statistics {.smaller}

Let $X$ be the purchase indicator per session.

- Number of sessions: $n = 10$  
- Total purchases: $S = 4$  

Sample mean:

$$
\bar{x} = \frac{4}{10} = 0.4
$$

Estimated probability:

$$
\hat{p} = 0.4
$$

## Defining the Random Variable 

$$
X =
\begin{cases}
1 & \text{if a purchase occurs} \\
0 & \text{otherwise}
\end{cases}
$$


. . .

> A Bernoulli model is an appropriate first representation of this process.


# So Far ...


## Final Intuition Summary

-   Use **Uniform** for “random within limits.”
-   Use **Exponential** for “time until next event.”
-   Use **Poisson** for “number of events per interval.”
-   Use **Bernoulli** for “one yes/no outcome".
-   

# CLT

 *"Even if you're not normal, the average is normal"* 



## The Magic of the Normal Distribution

The Normal distribution appears **everywhere in data**.


## What the Central Limit Theorem Says

**The Central Limit Theorem states:**

- When we take the **mean of many independent random variables**
- regardless of the shape of the original distribution
- provided they have finite mean and variance
- the distribution of the **sample mean** becomes approximately Normal as the sample size grows

## in other words...

![.](../../../img/statistics/clt.png)


## Intuition Behind the CLT

- Real-world data often come from skewed or irregular distributions.
- But when we **average many small effects**, the result tends toward a bell shape.
- This “averaging effect” is why the Normal distribution is so common.

## Why This Matters in Analytics

**The CLT allows us to:**

- build **confidence intervals**
- perform **hypothesis tests**
- use Normal-based statistical tools
- approximate distributions of sums and averages

Even when the original data are not Normal.

## Non-Normal Data Example {.smaller}

**Think about:**

. . .

- waiting times (often skewed)
- revenue per customer (heavy-tailed)
- customer arrival counts (discrete)

Individually, these can be far from Normal.


**When we repeatedly take many samples and compute means:**

- the distribution of the means becomes more symmetric
- it approaches the characteristic bell curve
- this happens regardless of the original shape

## Simulation: Visualizing CLT

```{python}
#| echo: false
#| fig-width: 18
#| fig-height: 5
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

population = np.random.exponential(scale=2, size=100000)

sample_sizes = [1, 5, 30, 100]
fig, axes = plt.subplots(1, 4, figsize=(20, 4))

for i, n in enumerate(sample_sizes):
    means = [np.mean(np.random.choice(population, n)) for _ in range(2000)]
    axes[i].hist(means, bins=30, density=True, edgecolor="black")
    axes[i].set_title(f"n = {n}")
    axes[i].set_xlabel("Sample mean")
    axes[i].set_ylabel("Density")

plt.tight_layout()
plt.show()
```

## What the Simulation Shows

- For **small n**, the distribution of sample means is skewed.
- As **n increases**:
  - the distribution becomes more symmetric
  - it approaches a bell shape
  - even though the original data were not Normal

## Conditions for CLT

**Before applying CLT:**

- samples should be **independent**
- sample size should be **sufficiently large**
- underlying distribution must have **finite variance**

These ensure the sample mean behaves approximately `Normal`.

## Practical Rules of Thumb (heuristics in analytics)

- If original data are nearly Normal → small n is ok  
- If data are skewed → larger n is needed  
- n ≥ 30 often yields good Normal approximation


## Applying CLT in Practice


- confidence interval construction  
- hypothesis testing  
- A/B testing rules  
- regression inference  

These methods assume approximate Normality of averages.

---
 

> *“Even if you're not normal, the average is normal.”*