---
title: "Hypothesis Testing"
subtitle: "Statistical Thinking"
author: "Karen Hovhannisyan"
date: last-modified
format:
  revealjs: 
    touch: true
    slide-number: false
    chalkboard: true

    preview-links: auto
    width: 1500
    transition: slide
    logo: ../../../img/favicon.png
    footer: "<a href=\"../session6.html\">Back to chapter</a>"
    # css: ../../../reveal_styles.css
    include-in-header:
      - text: |
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
          <style>
            .icon-xxl { font-size: 84px; }
            .icon-xl  { font-size: 64px; }
            .icon-lg  { font-size: 48px; }
            .muted { opacity: .8 }
            .pill { border-radius: 10px; }
            .circle { border-radius: 50%; }
          </style>
---


## Hypothesis Testing | Intuition


We Data Analyists/Statisticians like to make an assumptions about the value of a population parameter, and then:

. . .

- collect a sample from that population, 
- measure the sample, 
- declare in ascholarly manner, whether the sample supports the original assumption. 


## Stating the Hypothesis

A recent Wall Street Journal article titled **“Does the Internet Make You Smarter or
Dumber?”** posed the possibility that online activities turn us into shallow thinkers. 

The article cited a statistic claiming that the average time an American spends looking at a
Web page is `56 seconds`.

>A researcher at a local university would like to test this claim using a hypothesis test.


## Null Hypothesis

The null hypothesis $H_0$, *represents the status quo* and involves stating the belief that the population parameter is **$\le,=, \ge$** a specific value.

. . .

The null hypothesis is believed to be true unless there is overwhelming evidence to the contrary.


## Alternative hypothesis

The alternative hypothesis, $H_1$, represents the opposite of the null hypothesis and is believed to be true if the null hypothesis is found to be false.

. . .

The alternative hypothesis always states that the population parameter is **$\gt,\ne, \lt$**  a specific value.

. . .

You need to be careful how you state the null and alternative hypotheses.


## Analyogy with the Legal System | 1


*We can either reject it or not reject it (fail to reject it)*

. . . 

The court system assumes a person is innocent until proven guily, the hypothesis test is formulated as follows:

- $H_0:$ the defendant is innocent (status quo)
- $H_1:$ the defendant is guilty


## Analyogy with the Legal System | 2

**The court system might have two conclusions:**

::: {.incremental}
1. **Reject the Null Hypothesis** $\rightarrow$ the defendant is guilty
2. **Fail to reject the Null Hypothesis** $\rightarrow$ the defendant is guilty
:::


## Steps In Hypothesis Testing

::: {.incremental}

1. Identify **the null** and **alternative hypotheses**
2. Set a value for the **significance level** $\alpha$
3. Determine the appropriate **critical value**
4. Calculate the appropriate **test statistic**
5. Compare the **test statistics** with the **critical score**
6. State your conclusion

:::


## Hypothesis Stamenet 

In this example the Internet users spend an average timeof `56 seconds` on a Web page.


**Status Quo**

. . .

$$
H_0: \mu = 56 \text{ seconds (status quo)}
$$

**Alternative:**

. . .

$$
H_1: \mu \ne  56 \text{ seconds}
$$


## All the Combinations

|               | Two-Tailed Test                     | Left-Tailed Test                  | Right-Tailed Test                 |
|---------------|--------------------------------------|-----------------------------------|-----------------------------------|
| **Null**      | $$H_0: \mu = 56$$                    | $$H_0: \mu \ge 56$$               | $$H_0: \mu \le 56$$               |
| **Alternative** | $$H_1: \mu \ne 56$$                | $$H_1: \mu < 56$$                 | $$H_1: \mu > 56$$                 |


## Set Significance Level

The level of significance represents the *probability of making a Type I error.*
A Type I error occurs when we reject the null hypothesis but it is actually true. 

**The common Value:** $\alpha = 0.05$

. . .

>In the scope the above example `Type I Error` we conclude that the true average time spent on a webpage is not 56 seconds, even though in reality it is 56 seconds.

## In Our Hypothesis Context

- The researcher conducts a sample study.
- The sample mean differs enough from `56` seconds to fall outside the acceptance region.
- The test rejects the null hypothesis at the `5%` level.
- But in truth, Americans do spend exactly `56` seconds on average.


- - -

| Alpha (α) | Tail | Critical z-Score | Critical t-Score **(df = 20)** |
|-----------|------|------------------|-----------------------------|
| 0.01      | One  | 2.33             | 2.528                       |
| 0.01      | Two  | 2.575            | 2.845                       |
| 0.02      | One  | 2.05             | 2.312                       |
| 0.02      | Two  | 2.33             | 2.528                       |
| 0.05      | One  | 1.645            | 1.725                       |
| 0.05      | Two  | 1.96             | 2.086                       |
| 0.10      | One  | 1.28             | 1.325                       |
| 0.10      | Two  | 1.645            | 1.725                       |


## Type I vs Type II

![](../../../img/statistics/type_I_and_type_II.png)


## Determining Critical Values | z-test {.smaller}

**When Variance is known**

- test statistic would be $z_{\bar{x}}$  
- critical value would be $z_{\alpha}$  


$$
z_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{\sigma / \sqrt{n}}
$$


.  . .

- **Claimed mean:** $\mu_{H_0} = 56$ seconds  
- **Sample mean:** $\bar{x} = 62$ seconds  
- **Population standard deviation (historical estimate):** $\sigma = 18$ seconds  
- **Sample size:** $n = 45$  

$$
z_{\bar{x}}
= \frac{\bar{x} - \mu_{H_0}}{\sigma / \sqrt{n}}
= \frac{62 - 56}{18 / \sqrt{45}}
= \frac{6}{2.683}
= 2.24
$$


## Interpretation | z-test

A test statistic of **2.24** lies well into the rejection region (far from 0).

- For a **two-tailed test** at $\alpha = 0.05$, the critical values are $\pm 1.96$.  
- Since **2.24 > 1.96**, we **reject $H_0$**.

**Interpretation:**  

>There is statistically significant evidence that the true average time spent on a webpage is **different** from 56 seconds.

## Graphical Representation

```{python}
#| echo: false
#| fig-width: 5
#| fig-height: 3
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Shared parameters
mu = 8000
sigma = 100
alpha = 0.05

# Domain
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)
y = norm.pdf(x, mu, sigma)

# Critical values for two-tail
z_crit_two = norm.ppf(1 - alpha/2)
crit_right_two = mu + z_crit_two * sigma
crit_left_two = mu - z_crit_two * sigma

# One-tail critical
z_crit_one = norm.ppf(1 - alpha)
crit_right_one = mu + z_crit_one * sigma
crit_left_one = mu - z_crit_one * sigma

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(20, 5))

# ------------------- TWO-TAILED -------------------
ax = axes[0]
ax.plot(x, y, linewidth=2)

xr = np.linspace(crit_right_two, mu + 4*sigma, 200)
xl = np.linspace(mu - 4*sigma, crit_left_two, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_right_two)
ax.axvline(crit_left_two)

ax.set_title("Two-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- RIGHT-TAILED -------------------
ax = axes[1]
ax.plot(x, y, linewidth=2)

xr = np.linspace(crit_right_one, mu + 4*sigma, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)

ax.axvline(crit_right_one)

ax.set_title("Right-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- LEFT-TAILED -------------------
ax = axes[2]
ax.plot(x, y, linewidth=2)

xl = np.linspace(mu - 4*sigma, crit_left_one, 200)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_left_one)

ax.set_title("Left-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

plt.tight_layout()
plt.show()


```



## Z Value {.smaller}

| Test Type      | Hypotheses                                   | Condition                         | Conclusion        |
|----------------|-----------------------------------------------|------------------------------------|-------------------|
| **Two-tail**   | $H_0: \mu = \mu_0$                            | $\lvert z_x \rvert > z_{\alpha/2}$ | Reject $H_0$      |
|                | $H_1: \mu \ne \mu_0$                          | $\lvert z_x \rvert \le z_{\alpha/2}$ | Do not reject $H_0$ |
| **One-tail** | $H_0: \mu \le \mu_0$                    | $z_x > z_\alpha$                   | Reject $H_0$      |
|                | $H_1: \mu > \mu_0$                            | $z_x \le z_\alpha$                 | Do not reject $H_0$ |
| **One-tail** | $H_0: \mu \ge \mu_0$                    | $z_x < -z_\alpha$                  | Reject $H_0$      |
|                | $H_1: \mu < \mu_0$                            | $z_x \ge -z_\alpha$                | Do not reject $H_0$ |


## P Value

It is more convenient to use `p-value` apporach, as it helps us to rememeber the decission rule easily.

If the `p-value` is less than $\alpha$, there is little chance of observing the sample mean from
the population on which it is based if the null hypothesis were actually true. We therefore
reject the null hypothesis under this condition.

| **Condition**        | **Conclusion**        |
|----------------------|------------------------|
| $p\text{-value} \ge \alpha$ | Do not reject $H_0$ |
| $p\text{-value} < \alpha$   | Reject $H_0$         |


## Visual Representation


```{python}
#| echo: false
#| fig-width: 5
#| fig-height: 3
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Shared parameters
mu = 8000
sigma = 100
alpha = 0.05

# Domain
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)
y = norm.pdf(x, mu, sigma)

# Critical values for two-tail
z_crit_two = norm.ppf(1 - alpha/2)
crit_right_two = mu + z_crit_two * sigma
crit_left_two = mu - z_crit_two * sigma

# One-tail critical
z_crit_one = norm.ppf(1 - alpha)
crit_right_one = mu + z_crit_one * sigma
crit_left_one = mu - z_crit_one * sigma

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(20, 5))

# ------------------- TWO-TAILED -------------------
ax = axes[0]
ax.plot(x, y, linewidth=2)

# Shade rejection regions
xr = np.linspace(crit_right_two, mu + 4*sigma, 200)
xl = np.linspace(mu - 4*sigma, crit_left_two, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_right_two)
ax.axvline(crit_left_two)

# p-value annotations
ax.text(mu, max(y)*0.75, "p ≥ α\nFail to Reject $H_0$",
        ha='center', fontsize=13)

ax.text(crit_right_two + 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='left')

ax.text(crit_left_two - 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='right')

ax.set_title("Two-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- RIGHT-TAILED -------------------
ax = axes[1]
ax.plot(x, y, linewidth=2)

# Shade rejection region
xr = np.linspace(crit_right_one, mu + 4*sigma, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)

ax.axvline(crit_right_one)

# p-value annotations
ax.text(mu - 0.5*sigma, max(y)*0.75,
        "p ≥ α\nFail to Reject $H_0$", fontsize=13)

ax.text(crit_right_one + 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='left')

ax.set_title("Right-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- LEFT-TAILED -------------------
ax = axes[2]
ax.plot(x, y, linewidth=2)

# Shade rejection region
xl = np.linspace(mu - 4*sigma, crit_left_one, 200)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_left_one)

# p-value annotations
ax.text(mu + 0.5*sigma, max(y)*0.75,
        "p ≥ α\nFail to Reject $H_0$", fontsize=13)

ax.text(crit_left_one - 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='right')

ax.set_title("Left-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

plt.tight_layout()
plt.show()

```


## Type II Error

**Type II error**, which occurs when the `null hypothesis is really false and we fail to reject it`. The probability of a Type II error is known as $\beta$.


# Case Study

## Average Speed in Yerevan {.smaller}

Urban mobility reports often claim that the average weekday driving speed in **Yereva**n is `15 km/h.` A transportation analyst wants to test this claim using recent `GPS` data from ride-sharing vehicles.

This example is ideal for explaining all three hypothesis-test formulations:

::: {.incremental}
- **Two-Tailed:** Is the true average speed different from 15 km/h?
- **Right-Tailed:** Are drivers going faster than 15 km/h?
- **Left-Tailed:** Are drivers going slower than 15 km/h?
:::


## Two Tailed Hypotheses

Since the population standard deviation is unknown, a **one-sample t-test** is appropriate to use.

- $H_0: \mu = 15$
- $H_1: \mu \ne 15$


## What do we have ? 


- **Sample size:** $n = 1000$
- **Sample mean:** $\bar{x} = 16.2 \text{ km/h}$
- **Sample standard deviation:** $s = 5.1 \text{ km/h}$
- **Null hypothesis mean:** $\mu_{H_0} = 15$
- **Significance level:** $\alpha = 0.05$ 
- **Degrees of freedom:** $df = 999$

**The test statistic is:**

$$
t_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{s/\sqrt{n}}
$$

## Visual Representation

```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

# Parameters
n = 1000
xbar = 16.2
mu0 = 15
s = 5.1
df = n - 1
alpha = 0.05

# t statistic
t_stat = (xbar - mu0) / (s / np.sqrt(n))

# domain for plotting
x = np.linspace(-4, 4, 500)
y = t.pdf(x, df)

# critical value
crit = t.ppf(1 - alpha/2, df)

# figure
plt.plot(x, y, linewidth=2)

# rejection regions
plt.fill_between(x[x > crit], y[x > crit], alpha=0.30, color="gray")
plt.fill_between(x[x < -crit], y[x < -crit], alpha=0.30, color="gray")

# p-value shading
plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.50, color="red")

# lines
plt.axvline(t_stat, color="red")
plt.axvline(crit, color="black", linestyle="--")
plt.axvline(-crit, color="black", linestyle="--")

plt.title("Two-Tailed t-Test (α = 0.05): p-Value Shaded")
plt.xticks([]); plt.yticks([])
plt.show()
```


## Interpretation of the Two-Tailed Tests {.smaller}

- **The computed test statistic is:** $t_{\bar{x}} = 7.44$
- For a two-tailed test with $\alpha = 0.05$ and $df = 999$ $\rightarrow$ $t_{\alpha/2, df} = \pm 1.96$ :
- **p-value:** $p = 2 \cdot P(T > 7.44)\approx 1 \times 10^{-10}$

$$\downarrow$$

$|7.44| > 1.96 \rightarrow$ $p < 0.05$  

. . .

We **reject the null hypothesis**. There is very strong evidence that the true average weekday driving speed in Yerevan is **not equal** to 15 km/h.


## Right Tailed Test

We want to test whether the true average weekday driving speed in Yerevan is **greater** than  
the stated value of **15 km/h**.


$$H_0: \mu \le 15$$  
$$H_1: \mu > 15$$  


## What do we have?

- **Sample size:** $n = 1000$
- **Sample mean:** $\bar{x} = 16.2$ km/h
- **Sample standard deviation:** $s = 5.1$ km/h
- **Null hypothesis mean:** $\mu_{H_0}=15$
- **Significance level:** $\alpha=0.05$
- **Degrees of freedom:** $df = 999$


## Visual Representation

```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

n = 1000
xbar = 16.2
mu0 = 15
s = 5.1
df = n - 1
alpha = 0.05

t_stat = (xbar - mu0) / (s / np.sqrt(n))

x = np.linspace(-4, 4, 500)
y = t.pdf(x, df)

crit = t.ppf(1 - alpha, df)

plt.plot(x, y, linewidth=2)
plt.fill_between(x[x > crit], y[x > crit], alpha=0.3, color="gray")
plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.5, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, color="black", linestyle="--")

plt.title("Right-Tailed t-Test (α = 0.05): p-Value Shaded")
plt.xticks([]); plt.yticks([])
plt.show()

```

## Interpretation

- **Test statistic:** $t_{\bar{x}} = 7.44$
- **Critical value:** $t_{\alpha, 999} = 1.645$
- **p-value:** $p \approx 5\times 10^{-11}$

$$\downarrow$$

Since $7.44 > 1.645$ and $p < 0.05$, we **reject $H_0$**.

. . .

**Conclusion:** There is extremely strong evidence that **drivers in Yerevan drive faster than 15 km/h** on average.

## Left-Tailed Test

We want to test whether the true average weekday driving speed in Yerevan is **less** than  
the stated value of **15 km/h**.


$$H_0: \mu \ge 15$$  
$$H_1: \mu < 15$$  

## What do we have?

- **Sample size:** $n = 1000$
- **Sample mean:** $\bar{x} = 16.2$ km/h
- **Sample standard deviation:** $s = 5.1$ km/h
- **Null hypothesis mean:** $\mu_{H_0} = 15$
- **Significance level:** $\alpha = 0.05$
- **Degrees of freedom:** $df = 999$

## Visual Representation

```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

n = 1000
xbar = 16.2
mu0 = 15
s = 5.1
df = n - 1
alpha = 0.05

t_stat = (xbar - mu0) / (s / np.sqrt(n))

x = np.linspace(-4, 4, 500)
y = t.pdf(x, df)

crit = t.ppf(alpha, df)  # negative

plt.plot(x, y, linewidth=2)
plt.fill_between(x[x < crit], y[x < crit], alpha=0.3, color="gray")

# p-value shading (entire left region)
plt.fill_between(x[x < t_stat], y[x < t_stat], alpha=0.2, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, color="black", linestyle="--")

plt.title("Left-Tailed t-Test (α = 0.05): p-Value Shaded")
plt.xticks([]); plt.yticks([])
plt.show()

```



## Interpretation

- **Test statistic:** $t_{\bar{x}} = 7.44$  
- **Critical value:** $t_{\alpha, 999} = -1.645$  
- **p-value:** $p = P(T < 7.44) \approx 1.00$

$$\downarrow$$

Since $7.44$ is **not less** than $-1.645$ and $p > 0.05$, we **do NOT reject $H_0$**.

. . .

**Conclusion:** There is **no evidence** that drivers in Yerevan are slower than 15 km/h.  In fact, the sample strongly indicates the opposite.


# Two Sample t-Test (A/B test)


## Case Study {.smaller}

A telecom company is testing **two versions of its mobile self-care app**:

- **Version A (control)** — current user interface  
- **Version B (treatment)** — redesigned dashboard  

The team wants to know whether **average daily user engagement (minutes/day)** differs
between the two versions.

. . .

Does Variant B change average daily engagement?

## Hypothesis

Since population SD is unknown for both groups, we use a **two-sample Welch t-test**.

$$H_0: \mu_A = \mu_B$$  
$$H_1: \mu_A \ne \mu_B$$  


## Results

- Version A: $n_A = 800$, $\bar{x}_A = 12.5$, $s_A = 6.2$
- Version B: $n_B = 850$, $\bar{x}_B = 13.4$, $s_B = 6.8$


## Visual Representation

```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

# sample values
nA, xA, sA = 800, 12.5, 6.2
nB, xB, sB = 850, 13.4, 6.8

# t statistic
t_stat = (xA - xB)/np.sqrt(sA**2/nA + sB**2/nB)

# df (Welch)
df = (sA**2/nA + sB**2/nB)**2 / ((sA**2/nA)**2/(nA-1) + (sB**2/nB)**2/(nB-1))

alpha = 0.05

# domain
x = np.linspace(-4,4,500)
y = t.pdf(x, df)

crit = t.ppf(1 - alpha/2, df)

plt.plot(x, y, linewidth=2)

# rejection regions
plt.fill_between(x[x > crit], y[x > crit], alpha=0.3, color="gray")
plt.fill_between(x[x < -crit], y[x < -crit], alpha=0.3, color="gray")

# p-value shading
if t_stat > 0:
    plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.5, color="red")
else:
    plt.fill_between(x[x < t_stat], y[x < t_stat], alpha=0.5, color="red")

# vertical lines
plt.axvline(t_stat, color="red")
plt.axvline(crit, linestyle="--")
plt.axvline(-crit, linestyle="--")

plt.title("Two-Sample t-Test (Two-Tailed)")
plt.xticks([]); plt.yticks([])
plt.show()

```

## Interpretation {.smaller}

- Test statistic: $t = -2.93$
- Critical values: $t_{\alpha/2, df} \approx \pm 1.96$
- p-value: $p = 0.0034$

$$\downarrow$$

Because $|t| > 1.96$ and $p < 0.05$, we **reject $H_0$**.

. . .

**Version B** produces a **statistically significant difference** in average daily engagement compared to **Version A.**


## Right-Tailed Test

*Does Variant B increase engagement?*

Now the team wants a directional test:

. . .

**Does Version B strictly increase user engagement?**

## Visual Representation

```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

nA, xA, sA = 800, 12.5, 6.2
nB, xB, sB = 850, 13.4, 6.8

t_stat = (xA - xB)/np.sqrt(sA**2/nA + sB**2/nB)

df = (sA**2/nA + sB**2/nB)**2 / ((sA**2/nA)**2/(nA-1) + (sB**2/nB)**2/(nB-1))

alpha = 0.05

x = np.linspace(-4,4,500)
y = t.pdf(x, df)

crit = t.ppf(1 - alpha, df)

plt.plot(x, y)
plt.fill_between(x[x > crit], y[x > crit], alpha=0.3, color="gray")

# shade p-value
plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.5, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, linestyle="--")

plt.title("Right-Tailed Two-Sample t-Test")
plt.xticks([]); plt.yticks([])
plt.show()

```

## Interpretation

- Test statistic: $t = -2.93$
- Critical value: $t_{0.95, df} = 1.645$
- p-value: $p = 0.998$

$$\downarrow$$

We **fail to reject $H_0$**.

. . .

There is **no evidence** that *Version B increases user engagement*.


## Left Tail Test

`Does Variant B reduce app load time?`

Load time is a **negative metric**: smaller = better.

$$H_0: \mu_B \ge \mu_A$$  
$$H_1: \mu_B < \mu_A$$  

- Version A: $\bar{x}_A = 3.2$ sec, $s_A = 1.4$, $n_A = 600$  
- Version B: $\bar{x}_B = 2.9$ sec, $s_B = 1.3$, $n_B = 620$

## Visual Representation

```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

nA, xA, sA = 600, 3.2, 1.4
nB, xB, sB = 620, 2.9, 1.3

t_stat = (xA - xB)/np.sqrt(sA**2/nA + sB**2/nB)

df = (sA**2/nA + sB**2/nB)**2 / ((sA**2/nA)**2/(nA-1) + (sB**2/nB)**2/(nB-1))

alpha = 0.05

x = np.linspace(-4,4,500)
y = t.pdf(x, df)

crit = t.ppf(alpha, df)

plt.plot(x, y)
plt.fill_between(x[x < crit], y[x < crit], alpha=0.3, color="gray")

plt.fill_between(x[x < t_stat], y[x < t_stat], alpha=0.5, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, linestyle="--")

plt.title("Left-Tailed Two-Sample t-Test")
plt.xticks([]); plt.yticks([])
plt.show()
```


## Interpretation


- Test statistic: $t = 4.37$
- Critical value: $t_{0.05, df} = -1.645$
- p-value: $p = 1.00$

$$\downarrow$$

. . .

We **fail to reject $H_0$**.

There is no evidence that Version B reduces app load time.


##  Summary {.smaller}


| Test Type        | Scenario                                         | Hypotheses                                                                 | Test Statistic | Critical Value(s)                 | p-Value            | Decision              | Interpretation                                                                 |
|------------------|--------------------------------------------------|-----------------------------------------------------------------------------|----------------|-----------------------------------|---------------------|------------------------|-------------------------------------------------------------------------------|
| **Two-Tailed**   | \ Difference between App A and App B    | $H_0:\mu_A=\mu_B$  <br> $H_1:\mu_A\ne\mu_B$                                 | $t=-2.93$      | $\pm 1.96$                        | $p=0.003$          | Reject           | Engagement is **significantly different** between A and B.                    |
| **Right-Tailed** | Does B *increase* engagement vs A?               | $H_0:\mu_B\le\mu_A$ <br> $H_1:\mu_B>\mu_A$                                  | $t=-2.93$      | $t_{0.95}=1.645$                  | $p=0.99$           | Fail to reject   | No evidence that Version B **increases** engagement.                          |
| **Left-Tailed**  | Does B *reduce* app load time vs A?              | $H_0:\mu_B\ge\mu_A$ <br> $H_1:\mu_B<\mu_A$                                  | $t=4.37$       | $t_{0.05}=-1.645$                 | $p=1.00$           | Fail to rejectS   | No evidence that Version B **reduces** load time.                             |



