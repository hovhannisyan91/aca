---
title: "Statistics Session 04: Inference Essentials"
subtitle: "Sorytelling with Data"
categories: [statistics]
---

## Agenda

- Sampling
- Confidence Intervals
- Hypothesis Testing
- Storytelling With Data

## Sampling 

### Population vs. Sample  

In statistics:

- a **population** is the full group we care about (all customers, all transactions, all students, all calls).  
- a **sample** is a **smaller, representative portion** of that population.

For example, when a market research company measures TV viewership, the *population* is every viewer, but the *sample* is only the households that have a Nielsen people-meter installed. 

>The goal is to learn something about everyone by studying just the selected few. 

---

### Why Not Measure Everyone?  

At first glance, it may seem logical to *"measure everything"* But in reality, studying an entire population is often:

- **Too expensive**  
- **Too time-consuming**  
- **Physically impossible**  
- **Damaging to the items being measured** (quality control settings)

---

Imagine you want to understand **student attitudes toward academic integrity** at ACA.  

You `could` send a survey to all students, but:

- Not everyone will respond.  
- Collecting, cleaning, and processing thousands of responses would take weeks.  
- In many cases, a **properly selected sample** (e.g.,`15–25%` of students) is enough to accurately represent the entire student body.

A similar real-world example from the textbook shows how a college committee surveyed only a subset of students yet confidently inferred the attitudes of the whole institution. 

---

### Why Businesses Prefer Sampling?  

Sampling allows organizations to:

1. **Save money and time**  
2. **Check quality without damaging products**  
3. **Launch decisions faster**  
4. **Work with continuous data streams** when populations grow too fast  
5. **Avoid unnecessary measurements** when additional data adds little value

In telecom, banking, or marketing environments—like you often cannot measure every interaction or manually label every event. Sampling helps you make reliable conclusions with controlled effort.

---

### The Risks of Sampling  

Sampling introduces **uncertainty**. Estimates may deviate from the true population values.  

**However:**

- Statistics gives us tools to **quantify** this risk  
- Many sampling errors are **small and predictable**  
- Well-designed sampling keeps accuracy high while effort stays low

>*Sampling is not a shortcut.*
>It is a **scientifically grounded method** for making reliable decisions when full measurement is unnecessary, impractical, or impossible.  
>In both academic settings and real business operations, **sampling enables accurate, fast, and cost-effective insights**.

## Types of Sampling

There are many options available for gathering samples from a population. The two basic types that we will discuss are:

- probability sampling
- nonprobability sampling


### Probability Sampling

A **probability sample** is a sample in which each member of the population has a *known, nonzero, chance* of being selected for the sample. 

There are **five** main types of probability sampling techniques statisticians use: 

- simple random 
- systematic
- stratified
- cluster
- resampling


#### Simple Random Sampling


A **simple random sample** is a sample in which every member of the population has an **equal chance** of being chosen.

![](../../img/statistics/simple_random_sampling.png)

#### Systematic Sampling

In **systematic sampling**, every `k-th` member of the population is chosen for the sample. The value of `k` is determined by dividing the size of the population `(N)` by the size of the sample `(n)`.


But how do you know whcih value of `k` to choose? The value of k is the systematic sampling constant:

$$k = \frac{N}{n}$$

where:

- **N:** Size of the population
- **n:** Size of the sample

Using my academic integrity survey, with a population of `1,800` students and a sample
of `10`:

$$k = \frac{N}{n} = \frac{1,800}{10}=180$$

From a listing of the entire population, I would choose every **180th** student to be included in the sample.

![](../../img/statistics/systematic_sampling.png)

Systematic sampling is often easier to implement than a simple random sample.  

<!-- TODO try to put in a box -->

**Problem**

A problem can arise if the population contains a repeating pattern that aligns with the interval \(k\). This **periodicity** can produce a biased sample.  
**Example:** selecting data every 4th week in an 8-week term may always fall on midterms or finals, leading to inflated study-hour data.

**Solution**

The first selected unit does not have to be the first entry.We can randomly choose a starting position and then pick every \(k\)-th observation after that.


#### Stratified Sampling

Stratified sampling is used when *specific subgroups within a population differ in ways that
are important to the analysis*. If these subgroups are not represented proportionally in the
sample, the results may become biased—even when random sampling procedures are used.

Consider a cybersecurity compliance study within a large technology company. Previous
internal audits show that **senior engineers** tend to follow security protocols far more
consistently than **junior engineers**. If we rely on simple random or systematic sampling,
it is possible—simply by chance—that 45% of the selected employees will be senior
engineers, even though they represent only 15% of the workforce. Such a sample would
lead us to overestimate overall compliance levels.

**Stratified sampling prevents this type of distortion. **The population is divided into
meaningful groups (such as junior, mid-level, senior, and management), and each group is
sampled in proportion to its actual size. This ensures that the final sample mirrors the true
composition of the population and that no subgroup with unusually high or low compliance
influences the results disproportionately.

In contexts such as academic integrity studies, organizational culture assessments,
customer-segmentation research, or ACA-style analytics projects, stratified sampling
provides a more **balanced, representative, and trustworthy** basis for drawing conclusions.



| Stratum (Role Level) | Population Size | Population % | Sample Size (n = 200) | Calculation |
|----------------------|-----------------|--------------|------------------------|-------------|
| Junior Engineers     | 1,200           | 40%          | 80                     | $0.40 \times 200$ |
| Mid-Level Engineers  | 900             | 30%          | 60                     | $0.30 \times 200$ |
| Senior Engineers     | 450             | 15%          | 30                     | $0.15 \times 200$ |
| Management           | 450             | 15%          | 30                     | $0.15 \times 200$ |
| **Total**            | **3,000**       | **100%**     | **200**                | — |


![](../../img/statistics/stratified_sampling.png)


#### Cluster Sampling

Cluster sampling divides the population into **mutually exclusive groups**—called clusters—
where each cluster is intended to be a small-scale representation of the entire population.
Instead of sampling individuals directly, we randomly select entire clusters and then either
survey every member of those clusters or take a simple random sample within them.

Clusters are often based on **geography or natural groupings** to simplify data collection.
For example, in an academic integrity study, classrooms at specific times of day could serve
as clusters. Once a set of classrooms is randomly selected, all students in those rooms (or a
sample of them) can be surveyed.

A common source of confusion is the difference between **strata** and **clusters**:

- **Strata** are groups whose members share a common characteristic (e.g., all freshmen).
  Strata are typically **homogeneous** within each group and are used to ensure proper
  representation in the sample.
  
- **Clusters**, by contrast, are “mini-populations”—often **heterogeneous** mixtures of
  different types of individuals (e.g., a classroom containing freshmen, sophomores,
  juniors, and seniors). The goal is convenience and efficiency, not homogeneity.

Cluster sampling is widely used in business settings, especially when **test-marketing new
products**. Companies select cities or regions as clusters and gather feedback from customers
within those areas. When designed carefully, cluster sampling provides a cost-effective way
to collect a probability sample while maintaining representativeness.


![](../../img/statistics/cluster_sampling.png)


#### Resampling (Bootstrap)



Resampling is a statistical approach in which multiple samples are repeatedly drawn from an
observed dataset to **better understand the variability of a statistic.** One of the most widely
used resampling techniques is the **bootstrap method**, introduced by Bradley Efron at Stanford University.

Bootstrapping offers a practical, assumption-light method for estimating the uncertainty around statistical measures. 

Bootstrapping works by drawing many samples **with replacement** from the original
dataset. *For each resample, we compute a statistic of interest—such as a mean, median,
proportion, or variance.* By repeating this process thousands of times, we obtain a distribution
of the statistic, which helps us understand how much it varies and where the true population
value is likely to fall.

Bootstrapping is flexible and powerful. Unlike many traditional statistical methods, it does
**not** require strong assumptions about the underlying population distribution. It can be
applied to estimate population parameters, standard errors, and measures of uncertainty for
a wide variety of statistics.

##### Website Conversion Rates (Digital Marketing)

A marketing team measures the conversion rate of a landing page based on 600 user
sessions. They use bootstrapping to repeatedly resample these outcomes and estimate the
distribution of the conversion rate. This helps them compare campaign performance
confidently without relying on theoretical formulas.

##### Model Performance Metrics (Machine Learning)

A data science team evaluates a predictive model using a dataset of 10,000 labeled
observations. They apply bootstrapping to estimate the variability of accuracy, recall, or AUC
scores. This produces more reliable estimates of model performance under different
resampling conditions.

---

![](../../img/statistics/resampling_sampling.png)

#### Simple Random vs Systematic vs Stratified vs Cluster

![](../../img/statistics/comparison_sampling.png)


### Nonprobability Sampling 


**A nonprobability sample** is a sample
in which the probability of a population
member being selected for the sample **is
not known.**

A common type of nonprobability sample is a **convenience sample.**
This type of sampling is useful
when you are simply trying to gather some general information about the population.

- **Convenience sample:** A convenience sample is used when members of the population are chosen to become
part of the sample simply because they are easily accessible. 
- **Internet Poll:** Most individuals who participate in Internet polls have very little statistical training. They are impressed when they see thousands of responses recorded in real time and tend to believe the results.

## Sampling and Nonsampling Errors

When we rely on a sample instead of measuring an entire population, some level of error
is unavoidable. Population values—known as **parameters**—are typically unknown, so we
use **statistics** from our sample to estimate them. Because a sample represents only part of
the population, the statistic will almost never match the parameter exactly. This difference is
called **sampling error**, and it reflects the natural variability that occurs in any sampling
process.

$$\text{Sampling Error}=\bar{x}-\mu$$

where 

- $\bar{x}:$ sample mean
- $\mu:$ population mean

Sampling error is expected and tends to decrease when the sample size increases.
However, data collection can also be affected by **nonsampling errors**, which arise from
factors unrelated to the sampling procedure. These include biased survey questions,
measurement mistakes, data entry errors, or low response rates. Unlike sampling errors,
nonsampling errors cannot be reduced simply by collecting more data.

>Recognizing the difference between sampling and nonsampling errors is essential for evaluating how accurate and reliable a study’s conclusions are.


## Sampling Distribution

### Distirbution of the Mean with Finite Population

Up to this point, we have assumed that samples are drawn from a very large—or effectively
infinite—population. In such cases, the sample represents only a tiny fraction of the
population, so the usual standard error formula is appropriate.

However, when the population is **finite** and the sample makes up a noticeable portion of
that population, the standard error of the mean must be adjusted. This situation arises when
the ratio of sample size to population size: $n/N \gt 5$.

In these cases, we use the **finite population correction factor (FPC)** to account for the fact
that sampling without replacement reduces variability. The corrected standard error becomes
smaller because the sample is capturing a larger portion of the population.

For example, if a population contains 100 individuals, and we sample more than 5 of them
(i.e., $n > 5$), the finite population correction factor should be applied to compute an accurate
standard error. Ignoring the correction would **overestimate** the sampling error and lead to
incorrect probability calculations.


$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} \sqrt{\frac{N - n}{N - 1}}
$$


**where:**

- $\sigma:$ Population standard deviation
- $N:$ Population size
- $n:$ Sample size
- $\frac{N-n}{N-1}:$ finite population correction factor. 

This adjustment ensures that probability calculations and confidence intervals accurately
reflect the reduced sampling variability when the sample is a sizable proportion of the
population.

**Example**

Consider a consulting firm serving **120 clients**. Historically, the average satisfaction rating
is **7.3**. A recent survey of **50 clients** shows an average of **7.8**.

Because the sample is more than 5% of the population, the finite population correction must
be applied:

- The sample represents a large share of all clients.
- Sampling without replacement reduces variability.
- Using the FPC provides a more accurate standard error.
- A low resulting probability suggests the true population mean has improved.

This type of adjustment is essential in practical business settings—small customer lists,
internal employee populations, specialized cohorts, or any ACA-based system where the
population size is limited and known.

**Effect of the Correction Factor**

The following table illustrates how the standard error changes when the population size is
100 and the sample size increases. As $n$ grows, the correction factor becomes smaller,
reducing the standard error significantly.

| Sample Size (n) | Standard Error | Finite Correction Factor | Standard Error with FPC |
|-----------------|----------------|---------------------------|---------------------------|
| 40              | 0.111          | 0.778                     | 0.086                     |
| 60              | 0.090          | 0.636                     | 0.057                     |
| 80              | 0.078          | 0.449                     | 0.035                     |
| 100             | 0.070          | 0                         | 0                         |

As shown, when the sample covers the entire population ($n = 100$), the corrected standard
error becomes zero, as no sampling error exists in a census.



### Distribution of the Proportion

In many real-world applications, we analyze proportions rather than means—for example,
the % of customers who churn, voters who support a candidate, or households that
watch a specific media event. In these situations, the statistic of interest is the **sample
proportion**, and its behavior across repeated samples is described by the **sampling
distribution of the proportion**.


uppose a national streaming analytics firm claims that **52% of U.S. households** watched a
large live sports event in 2024. A media agency wants to verify the claim and surveys **300
randomly selected households**. In the sample, **147 households** watched the event.

To evaluate whether this sample supports the firm’s claim, we check whether the sample size
is large enough to use the normal approximation to the binomial distribution. The following
conditions must hold:

$$
np \ge 5
$$

$$
n(1 - p) \ge 5
$$

Using the claimed proportion $p = 0.52$ and sample size $n = 300$:

$$
np = 300(0.52) = 156 \ge 5
$$

$$
n(1 - p) = 300(0.48) = 144 \ge 5
$$

Thus, the normal approximation is appropriate.

### Sample Proportion

The sample proportion is:

$$
\hat{p} = \frac{x}{n}
$$

For this survey:

$$
\hat{p} = \frac{147}{300} = 0.49
$$

#### Standard Error of the Proportion

The standard error reflects how much sample proportions vary across repeated samples:

$$
\sigma_{\hat{p}} = \sqrt{ \frac{p(1 - p)}{n} }
$$

#### z-Score for the Sample Proportion

To evaluate how far the sample proportion is from the claimed population proportion:

$$
z_{\hat{p}} = \frac{\hat{p} - p}{\sigma_{\hat{p}}}
$$

A large positive or negative value indicates that the observed sample proportion would be
unlikely if the claimed population proportion were correct.

The sampling distribution of the proportion allows us to determine how probable it is to
observe a sample proportion like the one in our study. If the probability is small, the sample
provides evidence against the population claim. If the probability is reasonable, the sample
is consistent with the claim.

In this example, the observed sample proportion (0.49) is close to the claimed population
proportion (0.52), and with a moderate sample size, the difference is well within the range
of expected sampling variability.


####

```{mermaid}
flowchart TD
A["Population with true proportion p"] --> B["Select a random sample"]
B --> C["Count successes in the sample"]
C --> D["Compute the sample proportion"]
D --> E["Repeat the sampling process"]
E --> F["Distribution of sample proportions"]
F --> G["Shape: approximately normal"]
F --> H["Center equals p"]
F --> I["Spread depends on p and n"]

```


## Confidence Intervals

One of the most important roles statistics plays in today’s world is to:

- gather information from a sample,
- use that information to make a statement about the population from which the sample was chosen.

>Understanding *confidence intervals* helps quantify how certain we are about the population parameter based on sample data.lets 

### Point Estimates

A **point estimate** is a single value that best describes the population parameter of interest.  

The most common point estimates are:

- the **sample mean** ($\bar{x}$)
- the **sample proportion** ($\hat{p}$)

>Point estimates are easy to calculate, but they provide **no information** about the accuracy or uncertainty of the estimate. 
>They simply give one number — a `“best guess”` of the true population value.

::: {.callout-note appearance="simple" title="Confidence Interval"} 
A **confidence interval for the mean** is an interval estimate around a sample mean that provides a range within which the true population mean is expected to lie.
:::

::: {.callout-note appearance="simple" title="Confidence Level"} 
A **confidence level** is the probability that the interval constructed from sample data will contain the population parameter of interest.
:::


The purpose of generating a `confidence interval` is to provide an estimate for the true population mean by combining:

- the sample mean $\bar{x}$  
- the critical $z$-value  
- the standard error $\sigma_{\bar{x}}$

A `90%` CI means that **90% of such intervals—constructed from repeated samples—would contain the true population mean.**

 
Typically, confidence levels are set by the statistician at `90%` or `95%` and will occasionally go as high as `99%`. 

| Confidence Level | α (Significance Level) | α/2 (Each Tail) | Lower z<sub>α/2</sub> | Upper z<sub>α/2</sub> |
|------------------|-------------------------|------------------|------------------------|------------------------|
| 90%              | 0.10                    | 0.05             | -1.645                 | 1.645                  |
| 95%              | 0.05                    | 0.025            | -1.960                 | 1.960                  |
| 99%              | 0.01                    | 0.005            | -2.576                 | 2.576                  |


### Confidence Intervals for the Mean | known SD

<!-- TODO name normal name instead of QVC -->

Let’s say we want to construct a `confidence interval` **for the average order** size of the QVC customer based on my sample mean of `$129.20` with a `90% confidence level`. To determine this interval, we need two more pieces of information: 

1. the sample size: $\bar{x}$  
2. the population standard deviation $\sigma$

Suppose the sample mean was based on the 32 orders ($n=32$). Also, we’ll assume the population standard deviation is equal to $\sigma=40.602$ 

::: {.callout-note appearance="simple" title="About known Standard Deviation"} 
Usually domain experts are familiar with the value of SD
:::

**Standard Error (SE):**

$$
\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} = \frac{\$ 40.602}{\sqrt{32}} = \$7.173
$$


**Margin of Error (MoE):**

A **margin of error** represents the
width of the confidence interval
`between a sample mean and its
upper limit or between a sample
mean and its lower limit`. Notice
that the confidence interval is
**symmetrical** around the sample
mean.

$$MoE_{\bar{x}} = z_{\frac{\alpha}{2}}\sigma_{\bar{x}}$$

**Confidence Levels:**

::: {.columns}
::: {.column width="50%"}

$$UCL = \bar{x} + MoE$$

:::

::: {.column width="50%"}

$$LCL = \bar{x} - MoE$$

:::
:::


$$ \Downarrow $$

$$MoE_{90} = 1.645 \cdot 7.176 = 11.80$$


::: {.columns}
::: {.column width="50%"}

$$UCL = 129.2 + 11.8 = 141$$

:::

::: {.column width="50%"}

$$LCL = 129.2 - 11.8 = 117$$

:::
:::


| CL | $z_{α/2}$ | SE | MoE | LCL | UCL |
|------------------|---------|------------|------------------|-------------|-------------|
| 90%              | 1.645   | 7.173      | 11.80 | 117 | 141 |
| 95%              | 1.960   | 7.173      |14.06 | 115 |143 |
| 99%              | 2.576   | 7.174      |18.47 | 110 | 147|

>A `99%` confidence interval is even **wider** than the `95%` and `90%` intervals because a greater level of confidence requires covering a larger portion of the sampling distribution.

#### Sample Means and 90% Confidence Intervals

A confidence interval does not guarantee that the true population mean will fall inside the
interval calculated from a single sample. In the example, `Sample 1` produced a 90%
confidence interval that correctly captured the true population mean of `125`. However,
`Sample 5` produced an interval that entirely missed the true mean.

This outcome is expected. A 90% confidence interval means that **the method**, when
applied repeatedly across many random samples of the same size, will produce intervals that
contain the population mean about 90% of the time. It does **not** mean that any individual
interval has a 90% chance of containing the true value, nor does it guarantee that 9 out of
10 intervals from a small set of samples will succeed.


| Sample | Sample Mean | Margin of Error| Lower Limit | Upper Limit  |
|--------|------------------|----------------------|------------------|------------------|
| 1      | 129.20           | 11.80               | 117.40          | 141.00          |
| 2      | 132.00           | 11.80               | 120.20          | 143.80          |
| 3      | 117.50           | 11.80               | 105.70          | 129.30          |
| 4      | 128.20           | 11.80               | 116.40          | 140.00          |
| **5**      | **108.80**           | **11.80**               | **97.00**           | **120.60**          |
| 6      | 130.10           | 11.80               | 118.30          | 141.90          |
| 7      | 117.90           | 11.80               | 106.10          | 129.70          |
| 8      | 120.10           | 11.80               | 108.30          | 131.90          |
| 9      | 133.80           | 11.80               | 122.00          | 145.60          |
| 10     | 119.00           | 11.80               | 107.20          | 130.80          |


**The key idea:** confidence intervals describe the **long-run performance** of the estimation
procedure, not certainty about the result of a single sample.

```{python}
#| echo: false
#| fig-width: 18
#| fig-height: 5
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import matplotlib.pyplot as plt

samples = list(range(1,11))
lower=[117.40,120.20,105.70,116.40,97.00,118.30,106.10,108.30,122.00,107.20]
means=[129.20,132.00,117.50,128.20,108.80,130.10,117.90,120.10,133.80,119.00]
upper=[141.00,143.80,129.30,140.00,120.60,141.90,129.70,131.90,145.60,130.80]

pop_mean=125

fig, ax = plt.subplots(figsize=(8,6))

for i,(l,m,u) in enumerate(zip(lower,means,upper)):
    if i == 4:  # Sample 5 (index 4)
        ax.hlines(y=i, xmin=l, xmax=u, color='red', linewidth=3)
        ax.plot(m, i, 'o', color='red')
    else:
        ax.hlines(y=i, xmin=l, xmax=u, color='orange')
        ax.plot(m, i, 'o')

ax.axvline(pop_mean, color='green', linestyle='--')
ax.set_yticks(range(10))
ax.set_yticklabels([f"Sample {i}" for i in range(1,11)])
ax.set_xlabel("Order Value")
ax.set_title("Confidence Intervals vs Population Mean")
plt.tight_layout()
plt.show()


```

### Confidence Intervals for the Mean | Unknown SD

So far our confidence-interval examples have assumed that the population
standard deviation ($\sigma$), is known. In practice, this is rarely the case. Most of the time,
we only have access to the sample itself, and we must estimate the population standard
deviation using the **sample standard deviation**, $s$.

**The sample standard deviation can always be computed directly from the data:**

$$
s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n - 1}}
$$

Since $s$ is based on the sample, it is only an estimate of the true population standard
deviation. As a result, when $\sigma$ is unknown, we cannot rely on the normal distribution
to calculate a confidence interval for the mean. 

Instead, we use the **Student’s t-distribution**,
which adjusts for the uncertainty created by estimating $\sigma$ with $s$.

The `t-distribution` behaves like the normal distribution when sample sizes are large, but it
has heavier tails for smaller sample sizes. This accounts for the additional variability that
comes from estimating the standard deviation. As the sample size increases, the t-distribution
approaches the standard normal distribution.

The overall structure of the confidence-interval formula remains the same:

-  $\sigma \Rightarrow s$  
- `normal distribution` $\Rightarrow$ `t-distribution`
- `z-table` $\Rightarrow$ `t-tabe`

This adjustment allows us to construct valid confidence intervals in the realistic case where the population standard deviation is not available.

::: {.callout-note appearance="tip" title="Interesting fact about T-distribution"} 
Please watch [this video.](https://www.youtube.com/watch?v=32CuxWdOlow)  
:::


#### Student's t-distribution

- A bell-shaped and symmetric, similar to the normal distribution.
- Its exact shape depends on the **degrees of freedom**, defined as $n - 1$ for a sample of size $n$.
- The total area under the curve is `1.0`, just like any valid probability distribution.
- Because it is wider and flatter than the normal distribution, the critical values from the
  t-distribution are **larger** than z-critical values for the same confidence level. This leads
  to wider confidence intervals, reflecting the extra uncertainty created by estimating the
  population standard deviation with the sample standard deviation.
- The t-distribution is actually a **family** of distributions—one for each degree of freedom.
  As the degrees of freedom increase, the t-distribution becomes more similar to the normal
  distribution. With more than 100 degrees of freedom, the two are practically identical.


```{python}

#| echo: false
#| fig-width: 18
#| fig-height: 5
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t, norm

# Sample sizes
ns = [15, 30, 100]
dfs = [n - 1 for n in ns]

x = np.linspace(-4, 4, 400)

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for ax, df, n in zip(axes, dfs, ns):
    ax.plot(x, t.pdf(x, df), label=f"t-dist (df={df})")
    ax.plot(x, norm.pdf(x), linestyle='--', label="Normal")
    ax.set_title(f"Sample Size n = {n}")
    ax.legend()
    ax.grid(True)

plt.tight_layout()
plt.show()

```


#### Calculating the Confidence Intervals

Is it is mentioned above, the calculation methodology is the same:

1. Calculate Sample Mean
2. Calcualate the Standard Error
3. Calculate the Margin of Error
4. Caluclate the Confidence Inervals

>Note the Confidence Level/Significance Level ($\alpha$) must be given. Let's assume that the $\alpha = 0.05$

##### Weekly Visitors

The shop owner expects to have more than **90** visiters per week for the sustainable develepment.

| Week | Visitors |
|------|----------|
| 1    | 116      |
| 2    | 83       |
| 3    | 89       |
| 4    | 87       |
| 5    | 81       |
| 6    | 109      |
| 7    | 114      |
| 8    | 123      |
| 9    | 102      |
| 10   | 131      |
| 11   | 96       |
| 12   | 74       |
| 13   | 109      |
| 14   | 106      |
| 15   | 118      |
| 16   | 78       |
| 17   | 91       |
| 18   | 98       |

In order to calculate the `sample mean` and the `sample standard deviation` we can use excel:

- **DF:** `COUNT(B2:B19)-1 = 17`
- **Mean:** `AVERAGE(B2:B19) = 100.3`
- **Standard Deviation | Sample:** `STDEV.S(B2:B19) = 16.6`
- **Critical t-score:** `T.INV.2T(alpha, df) = T.INV.2T(0.05,17) = 2.11`
- **Margin of Error:**  `CONFIDENCE.T(alpha, sd,n) = CONFIDENCE.T(0.05, 16.6,18) = 8.25`


Margine Of Error

$$
\hat{\sigma}_{\bar{x}} = \frac{s}{\sqrt{n}} = 3.92
$$

**Confidence Levels:**

::: {.columns}
::: {.column width="50%"}

$$UCL = \bar{x} + MoE$$

:::

::: {.column width="50%"}

$$LCL = \bar{x} - MoE$$

:::
:::

$$ \Downarrow $$

::: {.columns}
::: {.column width="50%"}

$$UCL = 100.3 + 8.25 = 108.55$$

:::

::: {.column width="50%"}

$$LCL = 100.3 - 8.25 = 92.05$$

:::
:::




::: {.callout-note appearance="simple" title = "Conclusion"}

Based on the result we are `95% confident` that the true population mean for the number of visits per week is between `92` and `108`.
Because the entire interval exceeds 90 patients per week, it apears that the financial goals are being met.

:::


::: {.callout-note appearance="tip" title = "Remember"}

The properties of the confidence intervals using the t-distribution are the same for those constructed using the normal distribtuion:

- Increasing the confidence level will result in a wider (less precise) confidence interval.
- Increasing the sample size will result in a narrower (more precise) confidence interval.
- The margin of error is given by: $\text{Margin of Error} = t_{\alpha/2} \, \hat{\sigma}_{\bar{x}}$

:::

### Confidence Intervals for Proportions

The confidence interval for the proportion is an interval estimate around a sample proportion that provides us  with a range of where the true population proportion lies.


Recall that the proportion data follow the binomial distribution, which can be approximated by the normal distribution under the following condition:

$$np \ge 5 \text{ and } n(1-n) \ge 5$$

where:

- **p:** probability of a success in the population
- **n:** sample size



::: {.callout-note appearance="simple" title="Mini Case Study"} 
Suppose the online channel would like to estimate the proportion of customers who are female in order to improve the channel's advertising effectiveness. 

Let's say from a random sample of `175` customers, `116` were female.
:::

**Sample Proportion**

$$\bar{p} = \frac{x}{n} = \frac{116}{175}=0.663$$

where:

- **x:** the number of observations of interest in the sample (successes)
- **n:** the sample size



To construct a confidence interval, we need the **standard error of the proportion**.

The true standard error (using the unknown population proportion $p$) is:

$$
\sigma_{\bar{p}} = \sqrt{\frac{p(1 - p)}{n}}
$$

We do not know $p$, yet the formula for standard error requires $p$.

We can use the sample proportion $\bar{p}$ to approximate $p$.  This gives the **approximate standard error**:

$$
\hat{\sigma}_{\bar{p}} = \sqrt{\frac{\bar{p}(1 - \bar{p})}{n}}
$$

After calculated the standard error, can calculate the `99%` confidence interval for the proportion of female shopping customers using `MoE` and calculating `UCL` and `LCL`.

$$
\hat{\sigma}_{\bar{p}} 
= \sqrt{\frac{\bar{p}(1 - \bar{p})}{n}} 
= \sqrt{\frac{0.663(1 - 0.663)}{175}}
= \sqrt{\frac{0.223}{175}}
= \sqrt{0.001274}
= 0.0357
$$



$$ \Downarrow $$


$$
UCL_{\bar{p}} = \bar{p} + z_{\alpha/2}\hat{\sigma}_{\bar{p}} = 0.663 + (2.575)(0.0357) =  0.755 
$$


$$
LCL_{\bar{p}} = \bar{p} - z_{\alpha/2}\hat{\sigma}_{\bar{p}} = 0.663 - (2.575)(0.0357) =0.571
$$

Based on our sample proportion of `0.663`, we are `99%` confident that the proportion of female shoppers is between `0.572  and 0.755`,

::: {.callout-caution title = "Why is it wider?"}
The confidence interval seems wider as we want to be 99% confident of capturing the population proportion we need a wide interval
:::

### Determining the Required Sample Size

So far, we have focused on calculating a confidence interval and margin of error for a population when we know:

- The confidence level  
- The sample size  
- The standard deviation  

**Now we will try to reverse the process.** 
Instead of computing the margin of error from the sample size, we determine **the sample size needed** to achieve a desired margin of error, given:

- The confidence level  
- The population standard deviation  

This procedure is extremely useful because one of the first practical questions in statisticalwork is: *How large should the sample be?*

- **too large:** wastes resources.  
- **too small:** produces estimates that are not precise enough.

We will begin by determining the required sample size for estimating a population **mean** and **proportion**.

#### Sample Size | Estimating Population Mean

::: {.callout-tip title = "At&T Case Study"}

A major telecom operator in 2024 needed an accurate estimate of average monthly mobile data
usage. With heavy-streaming customers generating most of the traffic, the operator aimed to
set plans that balance cost and capacity.

To achieve this at a **95% confidence level** with a **$\pm2$ GB margin of error**, the operator
calculated the sample size required to estimate the population mean efficiently and
reliably.

Based on internal analytics, the population standard deviation is believed to be $\sigma = 8$ GB.
:::


In order to identify the Sample Size we need to do minor algebra  for MoE Equation:

$$MoE_\bar{x} = z_{\frac{\alpha}{2}} \sigma_{\bar{x}} = z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \Rightarrow  \sqrt{n} = \frac{z_{\frac{\alpha}{2}}\sigma}{MoE}$$


$$
n = \left( \frac{z_{\alpha/2}\, 2\sigma}{\text{MoE}} \right)^2
$$


Remember for a 95% confidence level critical z-value would be $z_{\alpha/2} = 1.96$

**Calculate and round up:**

$$
n = \left( \frac{1.96 \cdot 8}{2} \right)^2 = 61.46 = 62
$$


Thus, a sample of **62 customers** is required to estimate the average monthly mobile data usage with a **95% confidence interval** and a **$\pm2$ GB margin of error**.


#### Sample Size | Estimating Proportion

::: {.callout-tip title="FinTech Case Study: Biometric Login Adoption"}

A leading FinTech app in 2025 wants to estimate the proportion of users who **enable biometric login**  
(FaceID/TouchID). This metric is crucial for security-feature adoption and for planning future upgrades.

To achieve a **95% confidence level** with a **$\pm2$% margin of error**, the company needs to determine the
minimum sample size required to estimate this proportion accurately.

A quick pilot test of 200 users revealed that **24%** had biometric login enabled, giving  
$$\bar{p} = 0.24$$
:::



$$
n = \frac{z_{\alpha/2}^2 \, \bar{p}(1 - \bar{p})}{(MoE_{\bar{p}})^2}
$$

For a **95% confidence level**, we use:

- $z_{\alpha/2} = 1.96$
- $\bar{p} = 0.24$
- $MoE_{\bar{p}} = 0.02$

**Thus**

$$
n \approx \frac{(1.96)^2 (0.24)(0.76)}{(0.02)^2} \approx 1402.6 \Rightarrow 1403
$$


A sample of **1,403 users** is required to estimate the true proportion of biometric-enabled users within  
a **95% confidence interval** and **$\pm2$ percentage points of precision**.


<!-- TODO Calculate for finit population -->


## Hypothesis Testing

In the **statistical world**, a **hypothesis** is an assumption about a specific population parameter, such as a mean, a proportion, or a standard deviation. 

We data Analyists/Statistitians like to make an assumptions about the value of a population parameter, and then:

- collect a sample from that population, 
- measure the sample, 
- declare in ascholarly manner, whether the sample supports the original assumption. 

**This, in a nutshell, is what hypothesis testing is all about.**

<!-- TODO -->

### Stating The Hypothesis


A recent Wall Street Journal article titled **“Does the Internet Make You Smarter or
Dumber?”** posed the possibility that online activities turn us into shallow thinkers. The
article cited a statistic claiming that the average time an American spends looking at a
Web page is `56 seconds`.
A researcher at a local university would like to test this claim using a hypothesis test.

#### Null Hypothesis

The null hypothesis denoted by $H_0$, *represents the status* quo and involves stating the belief that the population parameter is **$\le,=, \ge$** a specific value.
The null hypothesis is believed to be true unless there is overwhelming evidence to the contrary.

#### Alternative hypothesis

The alternative hypothesis, denoted by $H_1$, represents the opposite of the null hypothesis and is believed to be true if the null hypothesis is found to be false.
The alternative hypothesis always states that the population parameter is **$\gt,\ne, \lt$**  a specific value.


You need to be careful how you state the null and alternative hypotheses. Your decision
will depend on the nature of the test and the motivation of the person conducting it. Suppose
the purpose of the test is to determine if the population mean is equal to a specific value, which
is what we would want to test based on our previous Wall Street Journal article


::: {.callout-important title = "Analogy with the Legan System"}
**We can never accept the Null Hypothesis!**

In order to completely understand the process consider the analogy with the legal system.

*We can either reject it or not reject it (fail to reject it)*

The court system assumes a persion is innocent until proben guily, the hypothesis test is formulated as follows

- $H_0:$ the defendant is innocent (sttus quo)
- $H_1:$ the defendant is guilty

TH court system might have two conclusions:

1. **Reject the Null Hypothesis** $\rightarrow$ the defendant is guilty
2. **Fail to reject the Null Hypothesis** $\rightarrow$ the defendant is guilty

:::


### Steps In hypothesis testing

1. Identify **the null** and **alternative hypotheses**
2. Set a value for the **significance level** $\alpha$
3. Determine the appropriate **critical value**
4. Calculate the appropriate **test statistic**
5. Compare the **test statistics** with the **critical score**
6. State your conclusion

#### Identifying the Null Hypothesis

You need to be careful how you state the null and alternative hypotheses. Your decision
will depend on the nature of the test and the motivation of the person conducting it. Suppose
the purpose of the test is to determine if the population mean is equal to a specific value, which
is what we would want to test based on our previous Wall Street Journal article. We would then
assign this statement as the null hypothesis, which results in the following equation:

In this example the Internet users spend an average timeof `56 seconds` on a Web page. Thhus the status qua would be

$$
H_0: \mu = 56 \text{ seconds (status quo)}
$$

$$
H_1: \mu \ne  56 \text{ seconds}
$$

##### All combinations

|               | Two-Tailed Test                     | Left-Tailed Test                  | Right-Tailed Test                 |
|---------------|--------------------------------------|-----------------------------------|-----------------------------------|
| **Null**      | $$H_0: \mu = 56$$                    | $$H_0: \mu \ge 56$$               | $$H_0: \mu \le 56$$               |
| **Alternative** | $$H_1: \mu \ne 56$$                | $$H_1: \mu < 56$$                 | $$H_1: \mu > 56$$                 |


#### Set Significance Level


The level of significance represents the *probability of making a Type I error.* A Type I error occurs when we reject the null hypothesis but it is
actually true. I the scope of the program we will set $\alpha = 0.05$ which is a common value used in hypothesis testing. 

>In the scope the above exampl `Type I Error` we conclude that the true average time spent on a webpage is not 56 seconds, even though in reality it is 56 seconds.


| Alpha (α) | Tail | Critical z-Score | Critical t-Score **(df = 20)** |
|-----------|------|------------------|-----------------------------|
| 0.01      | One  | 2.33             | 2.528                       |
| 0.01      | Two  | 2.575            | 2.845                       |
| 0.02      | One  | 2.05             | 2.312                       |
| 0.02      | Two  | 2.33             | 2.528                       |
| 0.05      | One  | 1.645            | 1.725                       |
| 0.05      | Two  | 1.96             | 2.086                       |
| 0.10      | One  | 1.28             | 1.325                       |
| 0.10      | Two  | 1.645            | 1.725                       |



**In practical terms:**

- The researcher conducts a sample study.
- The sample mean differs enough from 56 seconds to fall outside the acceptance region.
- The test rejects the null hypothesis at the `5% `level.
- But in truth, Americans do spend exactly `56` seconds on average.

![](../../img/statistics/type_I_and_type_II.png)

#### Determining Critical Values and the Test Statistics

::: {.callout-note appearance="simple" title="In case of z-test"} 

**When Variance is known**

- test statistic would be $z_{\bar{x}}$  
- critical value would be $z_{\alpha}$  

$$
z_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{\sigma / \sqrt{n}}
$$

**Given**

- Claimed mean: $\mu_{H_0} = 56$ seconds  
- Sample mean: $\bar{x} = 62$ seconds  
- **Population standard deviation (historical estimate):** $\sigma = 18$ seconds  
- Sample size: $n = 45$  

$$
z_{\bar{x}}
= \frac{\bar{x} - \mu_{H_0}}{\sigma / \sqrt{n}}
= \frac{62 - 56}{18 / \sqrt{45}}
= \frac{6}{2.683}
= 2.24
$$

A test statistic of **2.24** lies well into the rejection region (far from 0).

- For a **two-tailed test** at $\alpha = 0.05$, the critical values are $\pm 1.96$.  
- Since **2.24 > 1.96**, we **reject $H_0$**.

**Interpretation:**  
There is statistically significant evidence that the true average time spent on a webpage is **different** from 56 seconds.


:::



::: {.callout-note appearance="simple" title="In case of t-test"} 

**When Variance is Unknown**

- test statistic would be $t_{\bar{x}}$  
- critical value would be $t_{\alpha}$ (from the t-table)  

$$
t_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{s / \sqrt{n}}
$$

**Given**

- Claimed mean: $\mu_{H_0} = 56$ seconds  
- Sample mean: $\bar{x} = 62$ seconds  
- **Sample standard deviation:** $s = 20$ seconds  
- Sample size: $n = 30$  
- Degrees of freedom: $df = n - 1 = 29$

**Test Statistic**

$$
t_{\bar{x}}
= \frac{62 - 56}{20 / \sqrt{30}}
= \frac{6}{3.651}
= 1.64
$$


**Critical Value**

For a **two-tailed test** at $\alpha = 0.05$ with $df = 29$:

$$f
t_{\alpha/2, \, df} = t_{0.025,29} = 2.045
$$

**Decision**

Since **1.64 < 2.045**, we **fail to reject $H_0$**. There is **not enough statistical evidence** to conclude that the true average time spent on a webpage is different from **56 seconds**.  The sample mean is slightly higher, **but not far enough from 56** to be statistically significant given the sample size and sample variability.


:::

In the scope of this program we are going to conduct two hypothesis testing

1. Hypothesis Testing for a Single Population
2. Hypothesis Testing for two Samples (A/b testing)
<!-- TODO -->

### Graphical Representation


```{python}
#| echo: false
#| fig-width: 5
#| fig-height: 3
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Shared parameters
mu = 8000
sigma = 100
alpha = 0.05

# Domain
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)
y = norm.pdf(x, mu, sigma)

# Critical values for two-tail
z_crit_two = norm.ppf(1 - alpha/2)
crit_right_two = mu + z_crit_two * sigma
crit_left_two = mu - z_crit_two * sigma

# One-tail critical
z_crit_one = norm.ppf(1 - alpha)
crit_right_one = mu + z_crit_one * sigma
crit_left_one = mu - z_crit_one * sigma

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(20, 5))

# ------------------- TWO-TAILED -------------------
ax = axes[0]
ax.plot(x, y, linewidth=2)

xr = np.linspace(crit_right_two, mu + 4*sigma, 200)
xl = np.linspace(mu - 4*sigma, crit_left_two, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_right_two)
ax.axvline(crit_left_two)

ax.set_title("Two-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- RIGHT-TAILED -------------------
ax = axes[1]
ax.plot(x, y, linewidth=2)

xr = np.linspace(crit_right_one, mu + 4*sigma, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)

ax.axvline(crit_right_one)

ax.set_title("Right-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- LEFT-TAILED -------------------
ax = axes[2]
ax.plot(x, y, linewidth=2)

xl = np.linspace(mu - 4*sigma, crit_left_one, 200)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_left_one)

ax.set_title("Left-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

plt.tight_layout()
plt.show()


```

#### Decision Rules

##### Z Value

| Test Type      | Hypotheses                                   | Condition                         | Conclusion        |
|----------------|-----------------------------------------------|------------------------------------|-------------------|
| **Two-tail**   | $H_0: \mu = \mu_0$                            | $\lvert z_x \rvert > z_{\alpha/2}$ | Reject $H_0$      |
|                | $H_1: \mu \ne \mu_0$                          | $\lvert z_x \rvert \le z_{\alpha/2}$ | Do not reject $H_0$ |
| **One-tail** | $H_0: \mu \le \mu_0$                    | $z_x > z_\alpha$                   | Reject $H_0$      |
|                | $H_1: \mu > \mu_0$                            | $z_x \le z_\alpha$                 | Do not reject $H_0$ |
| **One-tail** | $H_0: \mu \ge \mu_0$                    | $z_x < -z_\alpha$                  | Reject $H_0$      |
|                | $H_1: \mu < \mu_0$                            | $z_x \ge -z_\alpha$                | Do not reject $H_0$ |

##### P Value


| Test Type      | Hypotheses                                   | Condition                                    | Conclusion            |
|----------------|-----------------------------------------------|-----------------------------------------------|------------------------|
| **Two-tail**   | $H_0: \mu = \mu_0$                            | $\lvert t_{\bar{x}} \rvert > t_{\alpha/2,\;df}$  | Reject $H_0$          |
|                | $H_1: \mu \ne \mu_0$                          | $\lvert t_{\bar{x}} \rvert \le t_{\alpha/2,\;df}$ | Do not reject $H_0$   |
| **Right-tail** | $H_0: \mu \le \mu_0$                          | $t_{\bar{x}} > t_{\alpha,\;df}$                 | Reject $H_0$          |
|                | $H_1: \mu > \mu_0$                            | $t_{\bar{x}} \le t_{\alpha,\;df}$                | Do not reject $H_0$   |
| **Left-tail**  | $H_0: \mu \ge \mu_0$                          | $t_{\bar{x}} < -t_{\alpha,\;df}$                | Reject $H_0$          |
|                | $H_1: \mu < \mu_0$                            | $t_{\bar{x}} \ge -t_{\alpha,\;df}$               | Do not reject $H_0$   |

##### P Value

It is mor convenient the use `p-value` apporach, as it helps us to rememeber the decission rule easily.

If the `p-value` is less than $\alpha$, there is little chance of observing the sample mean from
the population on which it is based if the null hypothesis were actually true. We therefore
reject the null hypothesis under this condition.

| **Condition**        | **Conclusion**        |
|----------------------|------------------------|
| $p\text{-Value} \ge \alpha$ | Do not reject $H_0$ |
| $p\text{-Value} < \alpha$   | Reject $H_0$         |

###### Visuall Representation


```{python}
#| echo: false
#| fig-width: 5
#| fig-height: 3
#| out-width: 100%
#| out-height: auto
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Shared parameters
mu = 8000
sigma = 100
alpha = 0.05

# Domain
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 500)
y = norm.pdf(x, mu, sigma)

# Critical values for two-tail
z_crit_two = norm.ppf(1 - alpha/2)
crit_right_two = mu + z_crit_two * sigma
crit_left_two = mu - z_crit_two * sigma

# One-tail critical
z_crit_one = norm.ppf(1 - alpha)
crit_right_one = mu + z_crit_one * sigma
crit_left_one = mu - z_crit_one * sigma

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(20, 5))

# ------------------- TWO-TAILED -------------------
ax = axes[0]
ax.plot(x, y, linewidth=2)

# Shade rejection regions
xr = np.linspace(crit_right_two, mu + 4*sigma, 200)
xl = np.linspace(mu - 4*sigma, crit_left_two, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_right_two)
ax.axvline(crit_left_two)

# p-value annotations
ax.text(mu, max(y)*0.75, "p ≥ α\nFail to Reject $H_0$",
        ha='center', fontsize=13)

ax.text(crit_right_two + 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='left')

ax.text(crit_left_two - 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='right')

ax.set_title("Two-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- RIGHT-TAILED -------------------
ax = axes[1]
ax.plot(x, y, linewidth=2)

# Shade rejection region
xr = np.linspace(crit_right_one, mu + 4*sigma, 200)
ax.fill_between(xr, norm.pdf(xr, mu, sigma), alpha=0.3)

ax.axvline(crit_right_one)

# p-value annotations
ax.text(mu - 0.5*sigma, max(y)*0.75,
        "p ≥ α\nFail to Reject $H_0$", fontsize=13)

ax.text(crit_right_one + 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='left')

ax.set_title("Right-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

# ------------------- LEFT-TAILED -------------------
ax = axes[2]
ax.plot(x, y, linewidth=2)

# Shade rejection region
xl = np.linspace(mu - 4*sigma, crit_left_one, 200)
ax.fill_between(xl, norm.pdf(xl, mu, sigma), alpha=0.3)

ax.axvline(crit_left_one)

# p-value annotations
ax.text(mu + 0.5*sigma, max(y)*0.75,
        "p ≥ α\nFail to Reject $H_0$", fontsize=13)

ax.text(crit_left_one - 0.3*sigma, max(y)*0.25,
        "p < α\nReject $H_0$", fontsize=13, ha='right')

ax.set_title("Left-Tailed Test (α = 0.05)")
ax.set_yticks([])
ax.set_xticks([])

plt.tight_layout()
plt.show()

```



<!-- HERE -->

### Type II Error

We can also be guilty of **Type II error**, which occurs when the `null hypothesis is really false and we fail to reject it`. The probability of a Type II error is
known as $\beta$.

![](../../../img/statistics/type_I_and_type_II.png)

When we decide to **reject the null hypothesis, there is always the chance, with** probability equal to $\alpha$, that we are wrong! 
In the Nissan Leaf example, a is the probability of concluding that the average driving distance does exceed 100 miles when, in fact, it does not exceed 100 miles.



### Case Study

::: {.callout-tip title="Urban Traffic Analytics: Average Speed in Yerevan"}

Urban mobility reports often claim that the average weekday driving speed in Yerevan is `15 km/h`.
A transportation analyst wants to test this claim using recent GPS data from ride-sharing vehicles.

This example is ideal for explaining all **three hypothesis-test formulations:**

- **Two-Tailed:** Is the true average speed different from `15 km/h`?
- **Right-Tailed:** Are drivers going faster than `15 km/h`?
- **Left-Tailed:** Are drivers going slower than `15 km/h`?

:::


#### Two-Tailed Test: Is the true average speed different from 15 km/h?

We want to test whether the true average weekday driving speed in Yerevan differs
from the commonly stated value of **15 km/h**.

##### Hypotheses

$$H_0: \mu = 15$$  
$$H_1: \mu \ne 15$$  

Since the population standard deviation is unknown, a **one-sample t-test** is appropriate.

**Given**

- **Sample size:** $n = 1000$
- **Sample mean:** $\bar{x} = 16.2 \text{ km/h}$
- **Sample standard deviation:** $s = 5.1 \text{ km/h}$
- **Null hypothesis mean:** $\mu_{H_0} = 15$
- **Significance level:** $\alpha = 0.05$ 
- **Degrees of freedom:** $df = 999$

**The test statistic is:**

$$
t_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{s/\sqrt{n}}
$$



```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

# Parameters
n = 1000
xbar = 16.2
mu0 = 15
s = 5.1
df = n - 1
alpha = 0.05

# t statistic
t_stat = (xbar - mu0) / (s / np.sqrt(n))

# domain for plotting
x = np.linspace(-4, 4, 500)
y = t.pdf(x, df)

# critical value
crit = t.ppf(1 - alpha/2, df)

# figure
plt.plot(x, y, linewidth=2)

# rejection regions
plt.fill_between(x[x > crit], y[x > crit], alpha=0.30, color="gray")
plt.fill_between(x[x < -crit], y[x < -crit], alpha=0.30, color="gray")

# p-value shading
plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.50, color="red")

# lines
plt.axvline(t_stat, color="red")
plt.axvline(crit, color="black", linestyle="--")
plt.axvline(-crit, color="black", linestyle="--")

plt.title("Two-Tailed t-Test (α = 0.05): p-Value Shaded")
plt.xticks([]); plt.yticks([])
plt.show()
```

**Interpretation of the Two-Tailed Test**

- The computed test statistic is: $t_{\bar{x}} = 7.44$
- For a two-tailed test with $\alpha = 0.05$ and $df = 999$ $\rightarrow$ $t_{\alpha/2, df} = \pm 1.96$ :
- **p-value:** $p = 2 \cdot P(T > 7.44)\approx 1 \times 10^{-10}$

**Decision**

$|7.44| > 1.96 \rightarrow$ $p < 0.05$  

We **reject the null hypothesis**. There is very strong evidence that the true average weekday driving speed in Yerevan is **not equal** to 15 km/h.


<!-- Right Tailed -->

#### **Right-Tailed Test: Are drivers going faster than 15 km/h?**

We want to test whether the true average weekday driving speed in Yerevan is **greater** than  
the stated value of **15 km/h**.

##### Hypotheses

$$H_0: \mu \le 15$$  
$$H_1: \mu > 15$$  

**Given**

- **Sample size:** $n = 1000$
- **Sample mean:** $\bar{x} = 16.2$ km/h
- **Sample standard deviation:** $s = 5.1$ km/h
- **Null hypothesis mean:** $\mu_{H_0}=15$
- **Significance level:** $\alpha=0.05$
- **Degrees of freedom:** $df = 999$

**The test statistic is:**

$$
t_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{s/\sqrt{n}}
$$



```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

n = 1000
xbar = 16.2
mu0 = 15
s = 5.1
df = n - 1
alpha = 0.05

t_stat = (xbar - mu0) / (s / np.sqrt(n))

x = np.linspace(-4, 4, 500)
y = t.pdf(x, df)

crit = t.ppf(1 - alpha, df)

plt.plot(x, y, linewidth=2)
plt.fill_between(x[x > crit], y[x > crit], alpha=0.3, color="gray")
plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.5, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, color="black", linestyle="--")

plt.title("Right-Tailed t-Test (α = 0.05): p-Value Shaded")
plt.xticks([]); plt.yticks([])
plt.show()

```

#### **Interpretation of the Right-Tailed Test**

- **Test statistic:** $t_{\bar{x}} = 7.44$
- **Critical value:** $t_{\alpha, 999} = 1.645$
- **p-value:** $p \approx 5\times 10^{-11}$

**Decision**

Since $7.44 > 1.645$ and $p < 0.05$, we **reject $H_0$**.

**Conclusion:**  
There is extremely strong evidence that **drivers in Yerevan drive faster than 15 km/h** on average.


#### Left-Tailed Test: Are drivers slower than 15 km/h?

We want to test whether the true average weekday driving speed in Yerevan is **less** than  
the stated value of **15 km/h**.

##### Hypotheses

$$H_0: \mu \ge 15$$  
$$H_1: \mu < 15$$  

**Given**

- **Sample size:** $n = 1000$
- **Sample mean:** $\bar{x} = 16.2$ km/h
- **Sample standard deviation:** $s = 5.1$ km/h
- **Null hypothesis mean:** $\mu_{H_0} = 15$
- **Significance level:** $\alpha = 0.05$
- **Degrees of freedom:** $df = 999$

**The test statistic is:**

$$
t_{\bar{x}} = \frac{\bar{x} - \mu_{H_0}}{s/\sqrt{n}}
$$



```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

n = 1000
xbar = 16.2
mu0 = 15
s = 5.1
df = n - 1
alpha = 0.05

t_stat = (xbar - mu0) / (s / np.sqrt(n))

x = np.linspace(-4, 4, 500)
y = t.pdf(x, df)

crit = t.ppf(alpha, df)  # negative

plt.plot(x, y, linewidth=2)
plt.fill_between(x[x < crit], y[x < crit], alpha=0.3, color="gray")

# p-value shading (entire left region)
plt.fill_between(x[x < t_stat], y[x < t_stat], alpha=0.2, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, color="black", linestyle="--")

plt.title("Left-Tailed t-Test (α = 0.05): p-Value Shaded")
plt.xticks([]); plt.yticks([])
plt.show()

```


#### Interpretation of the Left-Tailed Test

- **Test statistic:** $t_{\bar{x}} = 7.44$  
- **Critical value:** $t_{\alpha, 999} = -1.645$  
- **p-value:** $p = P(T < 7.44) \approx 1.00$

**Decision:**

Since $7.44$ is **not less** than $-1.645$ and $p > 0.05$, we **do NOT reject $H_0$**.

**Conclusion:**  

There is **no evidence** that drivers in Yerevan are slower than 15 km/h.  In fact, the sample strongly indicates the opposite.


## Comparing Two Population Means

The **sampling distribution for the
difference in means** is the result of
subtracting the sampling distribution for the
mean of one population from the sampling
distribution for the mean of a second
population.

![](../../img/statistics/two_sample.png)


### Two-Sample t-Test (A/B Test)

Does Variant B change average daily engagement?

A telecom company is testing **two versions of its mobile self-care app**:

- **Version A (control)** — current user interface  
- **Version B (treatment)** — redesigned dashboard  

The team wants to know whether **average daily user engagement (minutes/day)** differs
between the two versions.

#### Hypotheses

$$H_0: \mu_A = \mu_B$$  
$$H_1: \mu_A \ne \mu_B$$  

Since population SD is unknown for both groups, we use a **two-sample Welch t-test**.

#### Given

- Version A: $n_A = 800$, $\bar{x}_A = 12.5$, $s_A = 6.2$
- Version B: $n_B = 850$, $\bar{x}_B = 13.4$, $s_B = 6.8$

#### Test Statistic

$$
t = \frac{\bar{x}_A - \bar{x}_B}{\sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}}
$$

Degrees of freedom use the **Welch approximation**.




```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

# sample values
nA, xA, sA = 800, 12.5, 6.2
nB, xB, sB = 850, 13.4, 6.8

# t statistic
t_stat = (xA - xB)/np.sqrt(sA**2/nA + sB**2/nB)

# df (Welch)
df = (sA**2/nA + sB**2/nB)**2 / ((sA**2/nA)**2/(nA-1) + (sB**2/nB)**2/(nB-1))

alpha = 0.05

# domain
x = np.linspace(-4,4,500)
y = t.pdf(x, df)

crit = t.ppf(1 - alpha/2, df)

plt.plot(x, y, linewidth=2)

# rejection regions
plt.fill_between(x[x > crit], y[x > crit], alpha=0.3, color="gray")
plt.fill_between(x[x < -crit], y[x < -crit], alpha=0.3, color="gray")

# p-value shading
if t_stat > 0:
    plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.5, color="red")
else:
    plt.fill_between(x[x < t_stat], y[x < t_stat], alpha=0.5, color="red")

# vertical lines
plt.axvline(t_stat, color="red")
plt.axvline(crit, linestyle="--")
plt.axvline(-crit, linestyle="--")

plt.title("Two-Sample t-Test (Two-Tailed)")
plt.xticks([]); plt.yticks([])
plt.show()

```


#### Interpretation

- Test statistic: $t = -2.93$
- Critical values: $t_{\alpha/2, df} \approx \pm 1.96$
- p-value: $p = 0.0034$

#### Decision
Because $|t| > 1.96$ and $p < 0.05$, we **reject $H_0$**.

#### Conclusion

**Version B** produces a **statistically significant difference** in average daily engagement compared to **Version A.**


###  Two Sample t-Test | Right-Tailed Test

*Does Variant B increase engagement?*

Now the team wants a directional test:

**Does Version B strictly increase user engagement?**

#### Hypotheses

$$H_0: \mu_B \le \mu_A$$  
$$H_1: \mu_B > \mu_A$$  

#### Given
Same data as Example 1:

- $n_A = 800$, $\bar{x}_A = 12.5$, $s_A = 6.2$  
- $n_B = 850$, $\bar{x}_B = 13.4$, $s_B = 6.8$

#### Test Statistic

$$
t = \frac{\bar{x}_A - \bar{x}_B}{\sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}}
$$


```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

nA, xA, sA = 800, 12.5, 6.2
nB, xB, sB = 850, 13.4, 6.8

t_stat = (xA - xB)/np.sqrt(sA**2/nA + sB**2/nB)

df = (sA**2/nA + sB**2/nB)**2 / ((sA**2/nA)**2/(nA-1) + (sB**2/nB)**2/(nB-1))

alpha = 0.05

x = np.linspace(-4,4,500)
y = t.pdf(x, df)

crit = t.ppf(1 - alpha, df)

plt.plot(x, y)
plt.fill_between(x[x > crit], y[x > crit], alpha=0.3, color="gray")

# shade p-value
plt.fill_between(x[x > t_stat], y[x > t_stat], alpha=0.5, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, linestyle="--")

plt.title("Right-Tailed Two-Sample t-Test")
plt.xticks([]); plt.yticks([])
plt.show()

```



#### Interpretation (Right-Tailed Test)

- Test statistic: $t = -2.93$
- Critical value: $t_{0.95, df} = 1.645$
- p-value: $p = 0.998$

#### Decision

We **fail to reject $H_0$**.

#### Conclusion  

There is **no evidence** that *Version B increases user engagement*.



### Two Sample t-Test | Left-Tailed Test

`Does Variant B reduce app load time?`

Load time is a **negative metric**: smaller = better.

#### Hypotheses

$$H_0: \mu_B \ge \mu_A$$  
$$H_1: \mu_B < \mu_A$$  

#### Given

- Version A: $\bar{x}_A = 3.2$ sec, $s_A = 1.4$, $n_A = 600$  
- Version B: $\bar{x}_B = 2.9$ sec, $s_B = 1.3$, $n_B = 620$

#### Test Statistic

$$
t = \frac{\bar{x}_A - \bar{x}_B}{\sqrt{s_A^2/n_A + sB^2/n_B}}
$$


```{python}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import t

nA, xA, sA = 600, 3.2, 1.4
nB, xB, sB = 620, 2.9, 1.3

t_stat = (xA - xB)/np.sqrt(sA**2/nA + sB**2/nB)

df = (sA**2/nA + sB**2/nB)**2 / ((sA**2/nA)**2/(nA-1) + (sB**2/nB)**2/(nB-1))

alpha = 0.05

x = np.linspace(-4,4,500)
y = t.pdf(x, df)

crit = t.ppf(alpha, df)

plt.plot(x, y)
plt.fill_between(x[x < crit], y[x < crit], alpha=0.3, color="gray")

plt.fill_between(x[x < t_stat], y[x < t_stat], alpha=0.5, color="red")

plt.axvline(t_stat, color="red")
plt.axvline(crit, linestyle="--")

plt.title("Left-Tailed Two-Sample t-Test")
plt.xticks([]); plt.yticks([])
plt.show()
```

#### Interpretation (Left-Tailed Test)

- Test statistic: $t = 4.37$
- Critical value: $t_{0.05, df} = -1.645$
- p-value: $p = 1.00$

#### Decision

We **fail to reject $H_0$**.

#### Conclusion

There is no evidence that Version B reduces app load time.



### Combined Summary Table

| Test Type        | Scenario                                         | Hypotheses                                                                 | Test Statistic | Critical Value(s)                 | p-Value            | Decision              | Interpretation                                                                 |
|------------------|--------------------------------------------------|-----------------------------------------------------------------------------|----------------|-----------------------------------|---------------------|------------------------|-------------------------------------------------------------------------------|
| **Two-Tailed**   | \ Difference between App A and App B    | $H_0:\mu_A=\mu_B$  <br> $H_1:\mu_A\ne\mu_B$                                 | $t=-2.93$      | $\pm 1.96$                        | $p=0.003$          | Reject           | Engagement is **significantly different** between A and B.                    |
| **Right-Tailed** | Does B *increase* engagement vs A?               | $H_0:\mu_B\le\mu_A$ <br> $H_1:\mu_B>\mu_A$                                  | $t=-2.93$      | $t_{0.95}=1.645$                  | $p=0.99$           | Fail to reject   | No evidence that Version B **increases** engagement.                          |
| **Left-Tailed**  | Does B *reduce* app load time vs A?              | $H_0:\mu_B\ge\mu_A$ <br> $H_1:\mu_B<\mu_A$                                  | $t=4.37$       | $t_{0.05}=-1.645$                 | $p=1.00$           | Fail to rejectS   | No evidence that Version B **reduces** load time.                             |

