[
  {
    "objectID": "materials/syllabus.html",
    "href": "materials/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term/Year: 2025/2026 Subject Code and Course Number: [#856; #867]  Course Title: Data Analytics Instructor: Karen Hovhannisyan Email: khovhannisyan91@gmail.com  Phone: +374 77 600 945 Start date: 18-12-2025 End date: 01-05-2026 Class Schedule:\n- Tuesdays 19:30‚Äì22:00, Saturdays 15:00‚Äì17:30 (Cohort #856)\n- Thursdays 18:30‚Äì21:00, Sundays 10:00‚Äì12:30 (Cohort #867)\nMaterials: Reading Materials Technical stack: excel, PostgreSQL, Python (Data Analytics toolpack), Tableau, Docker, Git Prerequisites: Curiosity; Analytical Thinking",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "materials/syllabus.html#general-information",
    "href": "materials/syllabus.html#general-information",
    "title": "Syllabus",
    "section": "",
    "text": "Term/Year: 2025/2026 Subject Code and Course Number: [#856; #867]  Course Title: Data Analytics Instructor: Karen Hovhannisyan Email: khovhannisyan91@gmail.com  Phone: +374 77 600 945 Start date: 18-12-2025 End date: 01-05-2026 Class Schedule:\n- Tuesdays 19:30‚Äì22:00, Saturdays 15:00‚Äì17:30 (Cohort #856)\n- Thursdays 18:30‚Äì21:00, Sundays 10:00‚Äì12:30 (Cohort #867)\nMaterials: Reading Materials Technical stack: excel, PostgreSQL, Python (Data Analytics toolpack), Tableau, Docker, Git Prerequisites: Curiosity; Analytical Thinking",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "materials/syllabus.html#schedule-topics",
    "href": "materials/syllabus.html#schedule-topics",
    "title": "Syllabus",
    "section": "Schedule & Topics",
    "text": "Schedule & Topics\n\nIntroduction to Data Analytics & Statistical Thinking\n\n\nDuration: 4 weeks (8 sessions / 20 hours)\nGoal: Build analytical thinking and statistics foundation before tools. Adding light visualization early so students see value in data storytelling immediately.\n\n\n\nWeek 1: Foundations of Analytics & Data Thinking\n\nGetting familiarized with each other\n\nCourse logistics\n\nWhat is data analytics? Real business use cases\n\nTypes of analytics (descriptive, predictive, prescriptive)\n\nData lifecycle and roles (Analyst, Engineer, Scientist)\n\nUnderstanding data sources and structures\nHands-on: Map the data analytics process for a telecom or retail case\n\n\n\nWeek 2: Descriptive Statistics + Intro to Data Visualization\n\nTypes of data\n\nMean, median, mode\n\nVariance, standard deviation\n\nHistogram, bar chart, box plot\n\nWhen to use each chart type\n\nHands-on: basic charts in Excel or Sheets\n\nHands-on: summarize dataset (pandas describe)\n\n\n\nWeek 3: Probability & Distributions + Visualizing Distributions\n\nProbability basics\n\nNormal, Binomial, Poisson distributions\n\nCLT, z-scores, outliers\n\nVisualizing distributions: hist, KDE, scatter, density plots\n\nHands-on: dice/coin simulations in Python or Excel\n\n\n\nWeek 4: Inference & Analytical Thinking + Visual Storytelling\n\nSampling methods\n\nConfidence intervals, margin of error\n\nHypothesis testing (t-test, chi-square)\n\nInterpreting p-values and effect size\n\nBias\nStorytelling with charts\n\nMini project: GameCo analysis\n\n\n\n\n\nSQL\n\n\nDuration: 6 weeks (12 sessions / 30 hours)\nGoal: Build solid SQL and database skills by designing schemas, querying data, performing analytical transformations, and working with real PostgreSQL environments to support end-to-end analytics workflows\n\n\n\nWeek 1: Environment Setup ‚Äî Part 1\n\nInstall Docker Desktop\n\nPull & run PostgreSQL container\n\nInstall pgAdmin or configure psql\n\nInstall VS Code + DB/Git extensions\n\nInstall Git, create GitHub account\n\nCreate project folder structure\n\nInitialize Git repo\n\nVerify DB connection\n\n\n\nWeek 2: Environment Setup ‚Äî Part 2 + SQL Kickoff\n\nPostgres lifecycle in Docker\n\nBasic PostgreSQL config\n\nSchemas, tables\n\nInsert sample rows\n\nSELECT statements\n\nLoad CSV to DB\n\n\n\nWeek 3: Filtering, Aggregation, and Grouping\n\nSELECT, WHERE, ORDER BY, DISTINCT\n\nSUM, AVG, COUNT, MIN, MAX\n\nGROUP BY, HAVING\n\nCASE, COALESCE, CAST\n\nCommit SQL to GitHub\n\n\n\nWeek 4: Joins, Subqueries, and CTEs\n\nKeys & relationships\n\nINNER, LEFT, RIGHT, FULL\n\nSubqueries\n\nCTEs for pipelines\n\nExport results to CSV\n\n\n\nWeek 5: Window Functions & Analytical Scenarios\n\nOVER() clause\n\nRANK, ROW_NUMBER, DENSE_RANK\n\nRunning totals, moving averages, lag/lead\n\nTelecom churn & retention cases\n\nERD + data dictionary\n\n\n\nWeek 6: Functions, Procedures, and Advanced Aggregations\n\nStored procedures & UDFs\n\nParameterized queries\n\nCUBE, ROLLUP, GROUPING SETS\n\nMaterialized views\n\n\n\n\n\nPython for Data Analytics\n\n\nDuration: 7 weeks (14 sessions / 35 hours)\nGoal: Learn to connect data sources, build analytical visualizations, and design interactive dashboards that communicate insights effectively for business decisions.\n\n\n\nWeek 1: Python Fundamentals & Environment Setup\n\nInstall Miniconda\n\nCreate environment\n\nInstall pandas, numpy, matplotlib, seaborn, psycopg2\n\nInstall Jupyter Notebook\n\nConnect Python ‚ÜîÔ∏é PostgreSQL\n\nPython basics: syntax, loops, functions, types\n\nRun notebooks and scripts\n\n\n\nWeek 2: pandas & Data Visualization\n\nLoad, clean, transform datasets\n\ndescribe, info, shape, value_counts\n\nAggregation, filtering, merging\n\nVisualizations: hist, bar, box, line charts\n\n\n\nWeek 3: A/B Testing & Regression\n\nExperiments, control/treatment\n\nHypothesis testing, p-values\n\nt-test in Python\n\nLinear regression (scikit-learn)\n\nR¬≤, coefficients, interpretation\n\n\n\nWeek 4: Clustering & Segmentation\n\nK-Means clustering\n\nElbow method\n\nVisualizing clusters\n\nBusiness segmentation interpretation\n\n\n\nWeek 5: SQLAlchemy Integration\n\nConnect Python & PostgreSQL via SQLAlchemy\n\nRead SQL tables into pandas\n\nExecute SQL from Python\n\nWrite processed data back to DB\n\n\n\nWeek 6: Streamlit Fundamentals + Prototyping\n\nIntro to Streamlit: layout, widgets, state\n\nProject structure for apps (app.py, pages/, assets/)\n\nConnect to PostgreSQL from Streamlit (SQLAlchemy)\n\nDisplay DataFrames, filters, and basic charts\n\nCaching data queries and expensive operations\n\nPrototype: single‚Äëpage KPI dashboard\n\n\n\nWeek 7: Final Streamlit Data Dashboard\n\nDesign multi‚Äëpage app (navigation via pages/)\n\nInteractive filtering, parameters, and URL/query state\n\nCharts with seaborn/matplotlib or Plotly; export to CSV\n\nSession state, forms, and callbacks for smooth UX\n\nEnvironment configs (.env) and secrets management\n\nOptional deployment: Streamlit Community Cloud or Docker\n\nDeliverable: Final dashboard demo with README instructions\n\n\n\n\n\nTableau\n\n\nDuration: 4 weeks (8 sessions / 20 hours)\nGoal: Develop hands-on expertise in data visualization and dashboard design, connecting to real data sources and building interactive, business-ready dashboards.\n\n\n\nWeek 1: Intro\n\nConnect to basic data sources (Excel, CSV)\nWorkbooks\nData types\nDimensions vs.¬†Measures\nDiscrete vs.¬†Continuous fields\nTableau interface (shelves, marks, filters, dashboards, buttons)\nCreate basic charts: bar, line, pie, scatter\nUse filters, groups, sets, and sorting\nTask: Publish your first simple dashboard\n\n\n\nWeek 2: Intermediate Visual Analytics\n\nConnect to databases (PostgreSQL)\nWork with data joins, blends, unions, and relationships\nGet CSV from SQL Stored Procedure using Python\nDual-axis charts, histograms, boxplots, heatmaps\nCalculated fields (row-level vs aggregate)\nParameters (interactive filtering)\nTypes of filters (including dashboard actions)\nTask: Build dashboards with interactivity (actions, filters, tooltips)\n\n\n\nWeek 3: Advanced Analytics\n\nComplex Calculations\nTable calculations (running totals, percent of total, rank)\nDate functions\nDate parameters\nLevel of Detail (LOD) expressions (FIXED, INCLUDE, EXCLUDE)\nCohort and retention analysis in Tableau\nSpatial Analytics, spatial joins\nSpatial functions, connecting to Google Maps\nData Modeling\nData prep with Tableau Prep\nCleaning and reshaping data\nBuild dashboards for KPIs, cohort tables, and advanced heatmaps\nWork on telecom/finance/marketing datasets\n\n\n\nWeek 4: Dashboard Design & Performance\n\nDesign Principles\nVisual best practices (color, layout, storytelling)\nTypes of dashboards\nInteractive dashboards\nPerformance Optimization\nReduce extract size\nOptimize calculations\nMinimize dashboard load time\nTask: Build an end-to-end business dashboard (filters, parameters, highlights) and add interactivity (URL actions, sheet swapping)\n\n\n\n\n\nCapstone\n\n\nDuration: 3 weeks (8 sessions / 15 hours)\nGoal: Build and present a complete data analytics project demonstrating end-to-end skills across data modeling, ETL, SQL analysis, Python insights, and dashboard storytelling.\n\n\n\nProblem Definition: Identify business question\nData Planning & Schema Design: Define ERD & tables\nETL using Python: Extract, clean, and load data to SQL\nAnalytical Layer (SQL): Create stored procedures, summary tables\nExploration & Predictive Analytics (Python): Analyze and model data\nExport Final Data (CSV): prepare dataset for visualization\nDashboard Creation (Tableau): Final visual storytelling",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "materials/capstone/slides/session2.html#comming.-soon",
    "href": "materials/capstone/slides/session2.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon"
  },
  {
    "objectID": "materials/tableau/session3.html",
    "href": "materials/tableau/session3.html",
    "title": "Tableau Session 03:",
    "section": "",
    "text": "Tableau Tutorial\nComming Soon",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 03:"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session3.html#slide-1",
    "href": "materials/tableau/slides/session3.html#slide-1",
    "title": "Data Analyst",
    "section": "Slide 1",
    "text": "Slide 1",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session3.html#slide-2",
    "href": "materials/tableau/slides/session3.html#slide-2",
    "title": "Data Analyst",
    "section": "Slide 2",
    "text": "Slide 2",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#overview",
    "href": "materials/tableau/slides/session1.html#overview",
    "title": "Data Analyst",
    "section": "Overview",
    "text": "Overview\nThis session introduces the foundations of Tableau ‚Äî the leading tool for interactive data visualization and business intelligence.\nStudents learn how to connect to data, explore the Tableau interface, and create their first simple dashboard.\n\nBy the end of the class, you should be able to connect to common data sources, understand Tableau‚Äôs data model, and build your first visualizations using key chart types.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#what-is-tableau",
    "href": "materials/tableau/slides/session1.html#what-is-tableau",
    "title": "Data Analyst",
    "section": "What Is Tableau?",
    "text": "What Is Tableau?\nTableau is a data visualization and analytics platform designed to help people see and understand their data.\nIt transforms raw datasets into meaningful visual stories that support data-driven decision-making.\nTableau connects easily to spreadsheets, databases, or live cloud data, enabling analysts to explore data visually without writing complex code.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#tableau-products",
    "href": "materials/tableau/slides/session1.html#tableau-products",
    "title": "Data Analyst",
    "section": "Tableau Products",
    "text": "Tableau Products\nEach Tableau product serves a distinct purpose within the analytics lifecycle.\n\n\n\n\n\n\n\nProduct\nDescription\n\n\n\n\nTableau Desktop\nAuthoring tool for building visualizations and dashboards.\n\n\nTableau Server\nOn-premise platform for sharing dashboards securely within an organization.\n\n\nTableau Cloud\nCloud-hosted version of Tableau Server ‚Äî no infrastructure required.\n\n\nTableau Public\nFree, cloud-based platform for publishing dashboards publicly.\n\n\nTableau Prep\nTool for cleaning, combining, and reshaping data before analysis.\n\n\nTableau Next\nExperimental module for automated insight generation and trend detection.\n\n\n\nüí° Tip:\nTableau Public is ideal for practice and portfolio building. Use Tableau Desktop Public Edition to create and publish dashboards online.\nDownload Tableau Public\nAccess Sample Datasets\nDataset used in class: airbnb.xlsx\n\nConnect to Excel",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#tableau-file-types",
    "href": "materials/tableau/slides/session1.html#tableau-file-types",
    "title": "Data Analyst",
    "section": "Tableau File Types",
    "text": "Tableau File Types\n\n\n\n\n\n\n\n\nFile Type\nExtension\nDescription\n\n\n\n\nWorkbook\n.twb\nXML file storing visualizations and connections.\n\n\nPackaged Workbook\n.twbx\nWorkbook + data files (used for sharing).\n\n\nData Source\n.tds\nMetadata definition for a connection.\n\n\nPackaged Data Source\n.tdsx\n.tds plus data extract.\n\n\nExtract File\n.hyper\nOptimized local data extract.\n\n\nBookmark\n.tbm\nStores a single sheet.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#tableau-interface-overview",
    "href": "materials/tableau/slides/session1.html#tableau-interface-overview",
    "title": "Data Analyst",
    "section": "Tableau Interface Overview",
    "text": "Tableau Interface Overview\nThe Tableau interface is divided into several main components that allow you to manage data, build visuals, and combine them into dashboards.\n\n\n\n\n\n\n\nSection\nFunction\n\n\n\n\nData Pane (A)\nContains all fields (dimensions, measures, parameters).\n\n\nShelves (B)\nControl how fields are displayed in a view (Rows, Columns, Filters).\n\n\nMarks Card (C)\nManages visual elements such as color, size, shape, and labels.\n\n\nFilters Shelf (D)\nRestricts data displayed in a worksheet.\n\n\nDashboard Pane (E)\nCombines multiple sheets into one unified view.\n\n\nToolbar & Buttons (F)\nProvides shortcuts for key actions (undo, redo, sort, save).\n\n\n\n\nTableau Interface",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#core-tableau-concepts",
    "href": "materials/tableau/slides/session1.html#core-tableau-concepts",
    "title": "Data Analyst",
    "section": "Core Tableau Concepts",
    "text": "Core Tableau Concepts\nDimensions vs Measures\n\nDimensions are qualitative fields used to categorize data (e.g., Region, Category, Date).\n\nMeasures are quantitative fields that can be aggregated (e.g., Sales, Profit, Quantity).\n\n\n\n\nExample\nType\n\n\n\n\nCustomer Name\nDimension\n\n\nSUM(Sales)\nMeasure\n\n\nAVG(Profit)\nMeasure",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#creating-your-first-visualization",
    "href": "materials/tableau/slides/session1.html#creating-your-first-visualization",
    "title": "Data Analyst",
    "section": "Creating Your First Visualization",
    "text": "Creating Your First Visualization\nStep 1: Connect to Data\n\nGo to Data ‚Üí Connect to Data\n\nSelect Excel or Text File\n\nChoose airbnb.xlsx and drag the sheet into the workspace.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#assignment",
    "href": "materials/tableau/slides/session1.html#assignment",
    "title": "Data Analyst",
    "section": "Assignment",
    "text": "Assignment\nCreate a simple Tableau dashboard using the Airbnb dataset.\n\nConnect to airbnb.xlsx\n\nBuild at least three visualizations (bar, line, scatter, or pie)\n\nAdd a filter or group for interactivity\n\nPublish your dashboard to Tableau Public\n\nInclude a short reflection (2‚Äì3 sentences) on what your visualization shows.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#recommended-resources",
    "href": "materials/tableau/slides/session1.html#recommended-resources",
    "title": "Data Analyst",
    "section": "Recommended Resources",
    "text": "Recommended Resources\nArticles 1. Getting Started with Tableau\n2. Chart Chooser for Tableau\nVideos 1. Introduction to Tableau for Beginners",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session1.html#preview-of-next-class",
    "href": "materials/tableau/slides/session1.html#preview-of-next-class",
    "title": "Data Analyst",
    "section": "Preview of Next Class",
    "text": "Preview of Next Class\nIn the next session, we‚Äôll explore data connections and relationships in Tableau ‚Äî including joins, blends, and unions ‚Äî and learn how to combine multiple data sources effectively..",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html",
    "href": "materials/tableau/session1.html",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "This chapter sets the foundation for working with Tableau ‚Äî from connecting to data, through building your first visuals, to publishing a simple dashboard.\nYou‚Äôll learn core concepts (dimensions, measures, discrete/continuous), get oriented in the interface, and apply best practices while creating your first views.\n\nBy the end of the class, you should be able to connect to common data sources, explain Tableau‚Äôs core data concepts, and publish a simple, well-structured dashboard with basic interactivity.\n\n\n\n\nTableau is a visual analytics platform that turns data into insight through interactive charts and dashboards.\nIn practice, Tableau bridges raw tables and business understanding, enabling you to explore, explain, and share findings quickly.\nThe typical Tableau workflow follows five stages:\n\nConnect ‚Äì link to files (Excel/CSV), databases, or cloud sources.\n\nPrepare ‚Äì profile fields, set data types, create relationships/joins, and fix quality issues.\n\nVisualize ‚Äì build views on shelves with Marks (color/size/label/shape).\n\nCompose ‚Äì assemble sheets in dashboards; add filters, actions, and layout.\n\nPublish & Share ‚Äì deliver to Tableau Public, Cloud, or Server for feedback and decision-making.\n\n\nThis flow is iterative: each visualization raises new questions ‚Äî adjust, test, and refine.\n\n\n\n\n\nTableau supports decision-making across multiple industries:\n\nTelecommunications: network KPIs, churn monitoring, device mix, and customer segmentation.\n\nRetail & E-commerce: category performance, retention cohorts, and funnel analysis.\n\nFinance: credit risk dashboards, revenue tracking, portfolio monitoring.\n\nMarketing: campaign attribution, A/B test results, and media mix performance.\n\nHealthcare & Public Sector: patient outcomes, resource management, and geographic analytics.\n\nUnifying theme: Tableau accelerates how analysts move from questions ‚Üí visuals ‚Üí decisions.\n\n\n\n\n\n\n\n\n\n\n\nProduct\nDescription\n\n\n\n\nTableau Desktop\nThe primary tool for creating visualizations and dashboards.\n\n\nTableau Prep\nA data preparation tool for cleaning and reshaping data before visualization.\n\n\nTableau Server\nOn-premises platform for sharing dashboards securely within organizations.\n\n\nTableau Cloud\nCloud-based alternative for publishing and managing dashboards without server setup.\n\n\nTableau Public\nFree, web-based platform for publishing dashboards publicly.\n\n\nTableau Next\nA new feature for automated insight generation and trend detection.\n\n\n\nDownload: Tableau Public\nSample Datasets: Sample Data\nDataset used: airbnb.xlsx\n\n\n\nConnect to Excel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Type\nExtension\nDescription\n\n\n\n\nWorkbook\n.twb\nContains worksheets, dashboards, and connections (without data).\n\n\nPackaged Workbook\n.twbx\nWorkbook + embedded data and images (portable).\n\n\nData Source\n.tds\nContains metadata and field properties.\n\n\nPackaged Data Source\n.tdsx\nData source + data extract (portable).\n\n\nExtract\n.hyper\nOptimized extract of data for faster performance.\n\n\nBookmark\n.tbm\nSaves a single worksheet for reuse.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nNumber (Decimal)\nPrice\nQuantitative data with decimals.\n\n\nNumber (Whole)\nHost ID\nInteger data without decimals.\n\n\nDate / Date & Time\nHost Since\nTemporal data.\n\n\nString\nProperty Type\nTextual or categorical values.\n\n\nGeographic Role\nZip Code\nFields that can be mapped geographically.\n\n\nBoolean\nTrue/False\nLogical values used in filters or calculations.\n\n\n\n\n\n\n\n\nDimensions contain qualitative fields (categories such as names, dates, or geographic data).\n‚Üí Used to group, filter, and categorize data.\n\nMeasures contain quantitative, numeric fields.\n‚Üí Used to aggregate and analyze data (SUM, AVG, COUNT).\n\n\n\n\n\n\n\n\n\n\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nDiscrete\nDistinct categories, shown as separate labels or headers.\nCustomer Name, Region\n\n\nContinuous\nNumeric or time fields forming an unbroken scale.\nSUM(Sales), Order Date\n\n\n\nTip: You can convert measures or dimensions between discrete and continuous using the field dropdown.\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous\nDiscrete\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSection\nFunction\n\n\n\n\nA. Data Pane\nContains all fields (dimensions, measures, parameters).\n\n\nB. Shelves\nControl how fields appear in the view (Rows, Columns, Filters).\n\n\nC. Marks Card\nControls visual elements (Color, Size, Label, Shape, Tooltip).\n\n\nD. Filters Shelf\nLimits the data shown in the visualization.\n\n\nE. Dashboard Pane\nCombines multiple sheets into a single interactive view.\n\n\nF. Toolbar & Buttons\nQuick access to tools (undo, redo, sort, swap, etc.).\n\n\n\n\n\n\nWorksheet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar Chart\nLine Chart\nPie Chart\nScatter Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilters\nGroups\nSets\nSorting\n\n\n\n\n\n\n\nCreate a simple Tableau dashboard using the Airbnb dataset.\n\nConnect to airbnb.xlsx\n\nBuild at least three visualizations (bar, line, scatter, or pie)\n\nAdd a filter or group for interactivity\n\nPublish your dashboard to Tableau Public\n\nInclude a short reflection (2‚Äì3 sentences) on what your visualization shows.\n\n\n\n\n\nArticles 1. Getting Started with Tableau\n2. Chart Chooser for Tableau\nVideos 1. Introduction to Tableau for Beginners",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#what-is-tableau",
    "href": "materials/tableau/session1.html#what-is-tableau",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Tableau is a visual analytics platform that turns data into insight through interactive charts and dashboards.\nIn practice, Tableau bridges raw tables and business understanding, enabling you to explore, explain, and share findings quickly.\nThe typical Tableau workflow follows five stages:\n\nConnect ‚Äì link to files (Excel/CSV), databases, or cloud sources.\n\nPrepare ‚Äì profile fields, set data types, create relationships/joins, and fix quality issues.\n\nVisualize ‚Äì build views on shelves with Marks (color/size/label/shape).\n\nCompose ‚Äì assemble sheets in dashboards; add filters, actions, and layout.\n\nPublish & Share ‚Äì deliver to Tableau Public, Cloud, or Server for feedback and decision-making.\n\n\nThis flow is iterative: each visualization raises new questions ‚Äî adjust, test, and refine.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#business-applications-of-tableau",
    "href": "materials/tableau/session1.html#business-applications-of-tableau",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Tableau supports decision-making across multiple industries:\n\nTelecommunications: network KPIs, churn monitoring, device mix, and customer segmentation.\n\nRetail & E-commerce: category performance, retention cohorts, and funnel analysis.\n\nFinance: credit risk dashboards, revenue tracking, portfolio monitoring.\n\nMarketing: campaign attribution, A/B test results, and media mix performance.\n\nHealthcare & Public Sector: patient outcomes, resource management, and geographic analytics.\n\nUnifying theme: Tableau accelerates how analysts move from questions ‚Üí visuals ‚Üí decisions.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#tableau-products",
    "href": "materials/tableau/session1.html#tableau-products",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Product\nDescription\n\n\n\n\nTableau Desktop\nThe primary tool for creating visualizations and dashboards.\n\n\nTableau Prep\nA data preparation tool for cleaning and reshaping data before visualization.\n\n\nTableau Server\nOn-premises platform for sharing dashboards securely within organizations.\n\n\nTableau Cloud\nCloud-based alternative for publishing and managing dashboards without server setup.\n\n\nTableau Public\nFree, web-based platform for publishing dashboards publicly.\n\n\nTableau Next\nA new feature for automated insight generation and trend detection.\n\n\n\nDownload: Tableau Public\nSample Datasets: Sample Data\nDataset used: airbnb.xlsx\n\n\n\nConnect to Excel",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#tableau-file-types",
    "href": "materials/tableau/session1.html#tableau-file-types",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "File Type\nExtension\nDescription\n\n\n\n\nWorkbook\n.twb\nContains worksheets, dashboards, and connections (without data).\n\n\nPackaged Workbook\n.twbx\nWorkbook + embedded data and images (portable).\n\n\nData Source\n.tds\nContains metadata and field properties.\n\n\nPackaged Data Source\n.tdsx\nData source + data extract (portable).\n\n\nExtract\n.hyper\nOptimized extract of data for faster performance.\n\n\nBookmark\n.tbm\nSaves a single worksheet for reuse.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#data-types-in-tableau",
    "href": "materials/tableau/session1.html#data-types-in-tableau",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Type\nExample\nDescription\n\n\n\n\nNumber (Decimal)\nPrice\nQuantitative data with decimals.\n\n\nNumber (Whole)\nHost ID\nInteger data without decimals.\n\n\nDate / Date & Time\nHost Since\nTemporal data.\n\n\nString\nProperty Type\nTextual or categorical values.\n\n\nGeographic Role\nZip Code\nFields that can be mapped geographically.\n\n\nBoolean\nTrue/False\nLogical values used in filters or calculations.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#dimensions-vs.-measures",
    "href": "materials/tableau/session1.html#dimensions-vs.-measures",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Dimensions contain qualitative fields (categories such as names, dates, or geographic data).\n‚Üí Used to group, filter, and categorize data.\n\nMeasures contain quantitative, numeric fields.\n‚Üí Used to aggregate and analyze data (SUM, AVG, COUNT).",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#discrete-vs.-continuous-fields",
    "href": "materials/tableau/session1.html#discrete-vs.-continuous-fields",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Concept\nDescription\nExample\n\n\n\n\nDiscrete\nDistinct categories, shown as separate labels or headers.\nCustomer Name, Region\n\n\nContinuous\nNumeric or time fields forming an unbroken scale.\nSUM(Sales), Order Date\n\n\n\nTip: You can convert measures or dimensions between discrete and continuous using the field dropdown.\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous\nDiscrete",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#tableau-interface-overview",
    "href": "materials/tableau/session1.html#tableau-interface-overview",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Section\nFunction\n\n\n\n\nA. Data Pane\nContains all fields (dimensions, measures, parameters).\n\n\nB. Shelves\nControl how fields appear in the view (Rows, Columns, Filters).\n\n\nC. Marks Card\nControls visual elements (Color, Size, Label, Shape, Tooltip).\n\n\nD. Filters Shelf\nLimits the data shown in the visualization.\n\n\nE. Dashboard Pane\nCombines multiple sheets into a single interactive view.\n\n\nF. Toolbar & Buttons\nQuick access to tools (undo, redo, sort, swap, etc.).\n\n\n\n\n\n\nWorksheet",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#creating-basic-charts",
    "href": "materials/tableau/session1.html#creating-basic-charts",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Bar Chart\nLine Chart\nPie Chart\nScatter Plot",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#filters-groups-sets-and-sorting",
    "href": "materials/tableau/session1.html#filters-groups-sets-and-sorting",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Filters\nGroups\nSets\nSorting",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#assignment",
    "href": "materials/tableau/session1.html#assignment",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Create a simple Tableau dashboard using the Airbnb dataset.\n\nConnect to airbnb.xlsx\n\nBuild at least three visualizations (bar, line, scatter, or pie)\n\nAdd a filter or group for interactivity\n\nPublish your dashboard to Tableau Public\n\nInclude a short reflection (2‚Äì3 sentences) on what your visualization shows.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session1.html#recommended-resources",
    "href": "materials/tableau/session1.html#recommended-resources",
    "title": "Tableau Session 01: Introduction To Tableau",
    "section": "",
    "text": "Articles 1. Getting Started with Tableau\n2. Chart Chooser for Tableau\nVideos 1. Introduction to Tableau for Beginners",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 01: Introduction To Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html",
    "href": "materials/tableau/session4.html",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "",
    "text": "This session focuses on how to design dashboards that are clear, beautiful, and performant.\nStudents learn the foundations of visual storytelling, color theory, UX principles, dashboard layout, and types of dashboards used in organizations.\n\nBy the end of the class, you should be able to design dashboards that tell a clear story, apply color and layout intentionally, choose the right dashboard type for your audience, and optimize performance for a smooth user experience.\n\nThis week covers:\n\nVisual storytelling principles\n\nColor theory fundamentals\n\nUX principles for dashboard design\n\nDashboard layout best practices\n\nTypes of dashboards (Operational, Tactical, Analytical, Strategic, Multifunctional / Self-Service)\n\nInteractive dashboards (Filters, Parameters, Highlights, Actions)\n\nPerformance optimization (reduce extract size, optimize calculations, minimize load time)\n\nTask: Build an end-to-end business dashboard that:\n\nUses consistent layout and color\n\nIncludes filters, parameters, and highlights\n\nUses actions (filter, highlight, URL, sheet swapping) to guide the story",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#visual-storytelling-principles",
    "href": "materials/tableau/session4.html#visual-storytelling-principles",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Visual Storytelling Principles",
    "text": "Visual Storytelling Principles\n\nThe Role of Edward Tufte in Visual Storytelling\nIn the field of visual storytelling, Edward R. Tufte plays a foundational role.\nHis work connects data, design, and narrative, showing how information can be communicated not only clearly but also elegantly.\nTufte‚Äôs philosophy encourages designers and analysts to transform raw data into visual stories that inform and move audiences.\n\n‚ÄúAbove all else, show the data.‚Äù\n\n\n\nWho is Edward Tufte?\nEdward R. Tufte is an American statistician, artist, and former Yale professor, often called the father of data visualization.\nHe argues that visual storytelling should combine:\n\nTruth ‚Äì no distortion of data\n\nClarity ‚Äì no unnecessary complexity\n\nDesign integrity ‚Äì every element has a purpose\n\nHis most influential books include:\n\nThe Visual Display of Quantitative Information\n\nEnvisioning Information\n\nBeautiful Evidence\n\nTufte‚Äôs principles are not about decoration but purposeful simplicity ‚Äî every mark, color, or shape should tell part of the story.\n\n\n\nStorytelling with Data: Minard‚Äôs Map of the 1812 Russian Campaign\n\n\n\nMinard‚Äôs Map\n\n\nCharles Joseph Minard‚Äôs famous visualization tells the tragic story of Napoleon‚Äôs march to Moscow:\n\nThe width of the band represents the army size at each stage.\n\nThe path shows geographic direction.\n\nThe line below shows temperatures during the retreat.\n\nTogether, these layers merge six variables into one coherent narrative.\nWhy it matters:\n\nShows that data can convey emotion and consequence, not just numbers.\n\nEmbodies Tufte‚Äôs core message: ‚ÄúAbove all else, show the data.‚Äù\n\nDemonstrates clarity, precision, and storytelling depth.\n\nStorytelling takeaway:\nA single visual can tell a complete story ‚Äî if every element is meaningful.\n\n\n\nAvoid ‚ÄúChartjunk‚Äù ‚Äî Keep the Story Clear\n\n\n\nChartjunk Example\n\n\nChartjunk refers to unnecessary decorative elements (3D effects, clip art, heavy borders, textures) that do not add information.\nWhy it matters:\n\nVisual noise hides the message; simplicity reveals it.\n\nGuideline:\nEvery pixel, line, and color must serve the story, not style.\n\n\n\nMaximize the Data-Ink Ratio ‚Äî Tell More with Less\n\n\n\nData-Ink Ratio Example\n\n\nThe data-ink ratio describes how much of the ink in a chart actually represents data (instead of decoration).\nStorytelling principle:\n\nWhen visuals are simplified, the message becomes more powerful.\n\nPractical rules:\n\nRemove redundant labels and borders.\n\nUse subtle gridlines or remove them if they don‚Äôt help.\n\nHighlight only what matters (e.g., key lines, outliers).\n\n\n\n\nSmall Multiples ‚Äî Comparing Stories Side by Side\n\n\n\nSmall Multiples Example\n\n\nSmall multiples are sets of similar charts that share the same scale and layout.\nWhy they work:\n\nMake it easy to compare patterns across time, regions, or segments.\n\nEncourage exploration without overwhelming the viewer.\n\nStorytelling takeaway:\nUse repetition and consistent design to show change and contrast across multiple views.\n\n\n\nGraphical Integrity ‚Äî Be Honest in Your Story\n\n\n\nLie Factor Example\n\n\nA visualization must not distort the data.\nKey practices:\n\nKeep proportions true to values (no misleading scaling).\n\nAvoid truncated axes that exaggerate small differences (especially on bar charts).\n\nRepresent differences accurately in height, length, and area.\n\nStorytelling takeaway:\nIntegrity builds credibility ‚Äî the story must be as honest as the data.\n\n\n\nTufte‚Äôs Mindset in Dashboard Design\n\nFocus on the narrative: Highlight insights, not decoration.\n\nSimplify: Remove any element that doesn‚Äôt support the story.\n\nUse comparison wisely: Small multiples are powerful for showing change.\n\nBe truthful: Avoid distortion in axes, scales, and annotations.\n\nLayer context: Use time, geography, and value together when they clarify the story (like Minard‚Äôs map).",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#color-theory-fundamentals",
    "href": "materials/tableau/session4.html#color-theory-fundamentals",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Color Theory Fundamentals",
    "text": "Color Theory Fundamentals\n\nColor is Deceptive\n\n\n\nColor is Deceptive\n\n\nColor perception is context-dependent:\n\nThe same color may appear lighter/darker depending on its background.\n\nLighting, screen quality, and surrounding colors influence perception.\n\nWhen designing dashboard palettes, always consider:\n\nContext (background, other visuals)\n\nAudience (accessibility, color vision)\n\nGoal of the visualization (highlight vs neutral tone)\n\n\n\n\nUnderstanding the Color Wheel\nThe color wheel is a tool for understanding relationships between colors and building harmonious palettes.\nIt organizes:\n\nPrimary colors ‚Äì red, blue, yellow\n\nSecondary colors ‚Äì green, orange, purple\n\nTertiary colors ‚Äì combinations of primary + neighboring secondary (e.g., blue-green)\n\n\n\n\nColor wheel\n\n\nYou can experiment with palettes using tools like the Adobe Color Wheel.\n\n\n\nPrimary, Secondary, and Tertiary Colors\n\nPrimary colors: Basic building blocks; cannot be created by mixing (red, blue, yellow).\n\nSecondary colors: Mixes of two primaries (green, orange, purple).\n\nTertiary colors: Primary + neighboring secondary (yellow-orange, blue-green, red-purple).\n\nTertiary colors give more nuance and flexibility in dashboard palettes.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#color-harmony-schemes-for-dashboards",
    "href": "materials/tableau/session4.html#color-harmony-schemes-for-dashboards",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Color Harmony Schemes for Dashboards",
    "text": "Color Harmony Schemes for Dashboards\nUnderstanding color relationships helps you create balanced, readable dashboards.\n\nMonochromatic Scheme\n\n\n\nMonochromiatic scheme\n\n\n\nVariations of a single hue (tints, shades, tones).\n\nVery cohesive, calm, and professional.\n\nIdeal for minimalist dashboards and background colors.\n\n\n\nAnalogous Scheme\n\n\n\nAnalogous scheme\n\n\n\nUses colors next to each other on the wheel.\n\nCreates smooth, natural transitions.\n\nWorks well for gradients or multi-series charts with subtle differences.\n\n\n\nComplementary Scheme\n\n\n\nComplementary scheme\n\n\n\nColors opposite each other on the wheel.\n\nHigh contrast and strong visual energy.\n\nGreat for highlighting key metrics or ‚Äúgood vs bad‚Äù signals.\n\n\n\nCompound Scheme\n\n\n\nCompound scheme\n\n\n\nMix of two or more non-adjacent colors.\n\nOften forms a rectangle or square on the color wheel.\n\nBalances variety and harmony.\n\n\n\nTetradic Scheme\n\n\n\nTetradic scheme\n\n\n\nUses four colors evenly spaced on the wheel.\n\nVery vibrant and expressive.\n\nShould be used carefully to avoid visual overload.\n\n\n\n\nAdditional Useful Schemes\n\nSplit-Complementary Scheme\n\nOne base color + the two neighbors of its complement.\n\nKeeps high contrast but is less aggressive than a pure complementary scheme.\n\n\n\nNeutral Scheme\n\nFocuses on grays, whites, blacks, and muted tones.\n\nPerfect for analytic dashboards where data, not color, is the hero.\n\n\n\nWarm vs Cool Palettes\n\nWarm colors (red, orange, yellow) ‚Üí energy, urgency, attention.\n\nCool colors (blue, green, purple) ‚Üí calm, trust, stability.\n\nUse warm colors to highlight, cool colors as the baseline.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#design-principles-for-color-in-dashboards",
    "href": "materials/tableau/session4.html#design-principles-for-color-in-dashboards",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Design Principles for Color in Dashboards",
    "text": "Design Principles for Color in Dashboards\nEffective dashboards combine color harmony, functionality, readability, consistency, and accessibility.\n\nColor Harmony\nChoose colors that work well together and reflect the brand or context.\nLimit the number of hues; vary intensity instead.\n\n\nFunctionality\nColor should:\n\nGuide attention\n\nSignal status (e.g., red vs green)\n\nHelp group related elements\n\nEvery color should have a clear role.\n\n\nReadability\nUse color to support comprehension, not to decorate.\nSemantic example:\n\nGreen ‚Üí growth, success, ‚Äúgood‚Äù\n\nRed ‚Üí decline, risk, ‚Äúbad‚Äù\n\n\n\nSimplicity Through Consistency\n\nReuse the same colors for the same concepts across dashboards.\n\nToo many colors reduce clarity and create cognitive overload.\n\n\n\nAccessibility\nDesign for users with color vision deficiencies:\n\nDo not rely on color alone to encode information.\n\nEnsure strong contrast between foreground and background.\n\nTest palettes with tools like Coblis.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#ux-principles-for-dashboard-design",
    "href": "materials/tableau/session4.html#ux-principles-for-dashboard-design",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "UX Principles for Dashboard Design",
    "text": "UX Principles for Dashboard Design\nThis section summarizes 7 key UX principles for effective dashboards.\n\n1. User-Centricity\nDesign for specific users and use cases, not for ‚Äúeveryone‚Äù.\nAsk:\n\nWho is my audience? (role, data literacy, time)\n\nWhat decisions will they make with this dashboard?\n\nDo they need a quick overview, detailed analysis, or both?\n\nAnalysts may want detail and complexity; executives prefer clarity and summaries.\n\n\n\n2. Usefulness\nA dashboard is only valuable if it supports real decisions or tasks.\nAsk:\n\nIs this dashboard truly needed?\n\nDoes it replace or improve existing workflows (Excel decks, manual reports)?\n\nWhich questions does it answer?\n\nAvoid ‚Äúdashboard for the sake of dashboard‚Äù.\n\n\n\n3. Credibility\nUsers must trust your dashboard.\n\nValidate data sources.\n\nShow time stamps or update frequency.\n\nAvoid misleading charts and titles.\n\nUse zero baselines on bar charts\nLabel key values directly\nAvoid showing only cumulative numbers when rates matter\n\n\n\nCredibility Example\n\n\n\n\n\n4. Consistency\nConsistency makes dashboards predictable and intuitive.\nTypes:\n\nVisual: Same colors, fonts, line styles.\n\nFunctional: Interactions (clicks, hovers) behave similarly.\n\nNaming: Metrics and categories are named consistently.\n\nContextual: Align with brand guidelines.\n\n\n\n\nConsistency Example\n\n\n\n\n\n5. Clarity\nLess is more.\nUse Tufte‚Äôs data-ink ratio:\n\nRemove decorative elements and clutter.\n\nKeep text short and direct.\n\nEmphasize the most important metrics first.\n\n\n\n\nClarity Example\n\n\n\n\n\n6. Hierarchy\nVisual hierarchy guides the eye through the dashboard.\nUse:\n\nPosition (top-left is most prominent)\n\nSize (big = important)\n\nContrast (bold vs subtle)\n\nGrouping (containers, white space)\n\nSquint test: If you squint and still see the main message, your hierarchy works.\n\n\n\n7. Accessibility\nMake dashboards usable for everyone:\n\nUse clear fonts (10‚Äì12 pt+).\n\nKeep layouts simple.\n\nDon‚Äôt rely solely on tooltips or hover for important info.\n\nTest contrast and color palettes.\n\nResource: Dataviz Design Checklist",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#dashboard-layout-best-practices",
    "href": "materials/tableau/session4.html#dashboard-layout-best-practices",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Dashboard Layout Best Practices",
    "text": "Dashboard Layout Best Practices\n\nUsing Layout Containers in Tableau\nContainers help you organize and align content.\nTypes:\n\nHorizontal containers ‚Äì arrange elements side by side.\n\nVertical containers ‚Äì stack elements top to bottom.\n\n\n\n\n1. Adding a Layout Container\n\nDrag a Vertical Container from the Dashboard pane onto the canvas.\n\nYou may hold Shift to make it floating and position it manually.\n\nUse a light background color to see boundaries while designing.\n\n\n\n\n2. Container Structure\n\nSingle-level containers hold several elements directly.\n\nNested containers allow complex layouts (e.g., two charts on the left, one tall chart on the right).\n\n\n\n\n3. Adding Elements\n\nStart with Blank objects to design the grid.\n\nThen drag worksheets, text, and images into those positions.\n\n\n\n\n4. Orientation\n\nDropping items on the left/right edge ‚Üí horizontal arrangement.\n\nDropping items on the top/bottom edge ‚Üí vertical arrangement.\n\n\n\n\n5. Fitting Visualizations\n\nUse Fit Entire View to ensure charts adapt to container size.\n\nUse Fixed Width/Height for consistent sizing.\n\nDistribute content evenly when you want equal column/row sizes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit Entire View\nFixed Width/Height\nDistribute Content\n\n\n\n\n\n\n6. Padding\n\nInner padding: space between the container edge and content (breathing room).\n\nOuter padding: space between the container and other objects (spacing and hierarchy).\n\n\n\n\n7. Scaling Options\n\nFixed size: same size everywhere (best for presentations).\n\nRange: flexible within min‚Äìmax bounds (for laptops/desktops).\n\nAutomatic: fills available space (great for responsive dashboards, but test carefully).\n\n\n\n\n8. Scrollable Dashboards\nScrollable dashboards are especially helpful for multifunctional or analytic views with many sections.\nTo create:\n\nUse a fixed height smaller than the content height (e.g., dashboard 900px tall, content 1600px).\n\nPlace content inside a vertical container ‚Äî Tableau will add a vertical scroll bar.\n\nKeep key KPIs near the top; use section titles as anchors.\n\nAvoid multiple nested scroll areas.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#types-of-dashboards",
    "href": "materials/tableau/session4.html#types-of-dashboards",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Types of Dashboards",
    "text": "Types of Dashboards\nFollowing Stephen Few, we classify dashboards by function:\n\nOperational\n\nTactical\n\nAnalytical\n\nStrategic\n\nMultifunctional / Self-Service\n\nEach serves a different decision-making level.\n\n\n1. Operational Dashboards\n\nQuestion: ‚ÄúWhat is happening right now?‚Äù\n\n\nPurpose: Monitor day-to-day operations and detect issues as they occur.\n\nAudience: Frontline staff, operations teams, call center supervisors.\n\nExample: Telecom operations dashboard with uptime, call drop rate, open tickets.\n\nCommon visuals:\n\nKPI cards with thresholds\n\nGauges / bullet charts\n\nReal-time tables and alerts\n\n\n\n\nOperational Dashboard\n\n\n\n\n\n2. Tactical Dashboards\n\nQuestion: ‚ÄúHow are we performing against our goals?‚Äù\n\n\nPurpose: Track short- and mid-term performance vs targets.\n\nAudience: Team leads, department heads, project managers.\n\nExample: Sales dashboard tracking regional results vs monthly targets.\n\nCommon visuals:\n\nVariance-to-target bars\n\nTrend lines by month/quarter\n\nProgress bars by region/team\n\n\n\n\nTactical Dashboard\n\n\n\n\n\n3. Analytical Dashboards\n\nQuestion: ‚ÄúWhy is this happening?‚Äù\n\n\nPurpose: Explore data deeply, detect patterns, and understand root causes.\n\nAudience: Data analysts, BI teams, advanced users.\n\nExample: Churn dashboard showing which segments are at higher risk and why.\n\nCommon visuals:\n\nHeatmaps\n\nScatter plots\n\nCohort charts\n\nDrillable tables\n\n\n\n\nAnalytical Dashboard\n\n\n\n\n\n4. Strategic Dashboards\n\nQuestion: ‚ÄúWhere are we heading?‚Äù\n\n\nPurpose: Provide a high-level view of organizational performance and strategy.\n\nAudience: Executives, directors, board members.\n\nExample: Executive dashboard with revenue, profitability, market share, and satisfaction over time.\n\nCommon visuals:\n\nKPI scorecards\n\nTrend lines & forecasts\n\nHigh-level maps and summaries\n\n\n\n\nStrategic Dashboard\n\n\n\n\n\n5. Multifunctional Dashboards\nSome dashboards combine elements from several types.\nPurpose: Allow users to access multiple levels of insight in one place.\nExample:\n\nTop section ‚Üí Overview (strategic KPIs + operational health).\n\nLower sections ‚Üí Detailed analysis (trends, breakdowns, drill-downs).\n\nüí° Best practice: design these as scrollable dashboards with clear sections.\n\n\n\nMultifunctional Dashboard\n\n\n\n\n\nDashboard Type Summary\n\n\n\n\n\n\n\n\n\n\n\nType\nQuestion\nAudience\nUpdate Frequency\nFocus\nExample KPI\n\n\n\n\nOperational\nWhat‚Äôs happening now?\nFrontline teams\nReal-time\nEfficiency\nAverage handling time\n\n\nTactical\nAre we meeting our goals?\nManagers\nWeekly / Monthly\nPerformance\nSales vs target\n\n\nAnalytical\nWhy is this happening?\nAnalysts\nOn-demand\nInsight & causes\nChurn drivers\n\n\nStrategic\nWhere are we heading?\nExecutives\nMonthly / Quarterly\nOutcomes\nRevenue growth",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#reflection-and-discussion",
    "href": "materials/tableau/session4.html#reflection-and-discussion",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Reflection and Discussion",
    "text": "Reflection and Discussion\n\nWhich dashboard type do you use most often in your current work or study?\n\nWhere do you see chartjunk or poor color use in real dashboards around you?\n\nHow could you redesign one of your own dashboards using Tufte‚Äôs principles and color harmony?\n\nTake notes or share examples in class.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/tableau/session4.html#week-4-assignment-end-to-end-business-dashboard",
    "href": "materials/tableau/session4.html#week-4-assignment-end-to-end-business-dashboard",
    "title": "Tableau Session 04: Dashboard Design & Performance",
    "section": "Week 4 Assignment ‚Äì End-to-End Business Dashboard",
    "text": "Week 4 Assignment ‚Äì End-to-End Business Dashboard\nBuild a business dashboard in Tableau that:\n\nUses one clear narrative (e.g., ‚ÄúSales performance‚Äù, ‚ÄúNetwork health‚Äù, ‚ÄúCustomer churn‚Äù).\n\nIncludes at least three sheets (e.g., KPI view, trend view, breakdown view).\n\nApplies color theory (choose a scheme and document it briefly).\n\nImplements at least two interactive elements:\n\nFilters / quick filters\n\nParameters\n\nHighlight or filter actions\n\nURL or sheet-swap actions\n\n\nDemonstrates good layout and hierarchy using containers and padding.\n\nIncludes a short story caption (2‚Äì3 sentences) summarizing the main insight.\n\nOptional: publish to Tableau Public and share the link in the course chat..",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 04: Dashboard Design & Performance"
    ]
  },
  {
    "objectID": "materials/lab/tableau.html",
    "href": "materials/lab/tableau.html",
    "title": "Tableau",
    "section": "",
    "text": "You must create a Tableau Public account before working with the tool.\n\n\n\nVisit the Tableau Public website:\nhttps://public.tableau.com/ \nClick Sign In (top right). \nClick Create one now. \nEnter your:\n\nEmail\nName\nPassword\n\n\nAccept the terms and click Create account.\nCheck your email and verify your Tableau account.\nVerification is required before you can publish dashboards.\n\n\n\n\n\n\n\nDownload Tableau Public from the official website:\nhttps://public.tableau.com/app/discover\n\nChoose the installer for your operating system:\n\nWindows (Tableau Public for Windows)\nmacOS (Tableau Public for Mac)\n\n\n\n\n\n\n\n\nDownload the installer (.exe). \nRun the installer. \nAccept the license agreement.\nKeep the default installation options. \nLaunch Tableau Public after installation.\n\n\n\n\nOpen Tableau Public from the Start Menu.\nIf the application launches without errors, the setup is complete.\n&lt;!‚Äì TODO: screenshot - Tableau Public first launch (Windows"
  },
  {
    "objectID": "materials/lab/tableau.html#sign-up-for-tableau-public",
    "href": "materials/lab/tableau.html#sign-up-for-tableau-public",
    "title": "Tableau",
    "section": "",
    "text": "You must create a Tableau Public account before working with the tool.\n\n\n\nVisit the Tableau Public website:\nhttps://public.tableau.com/ \nClick Sign In (top right). \nClick Create one now. \nEnter your:\n\nEmail\nName\nPassword\n\n\nAccept the terms and click Create account.\nCheck your email and verify your Tableau account.\nVerification is required before you can publish dashboards."
  },
  {
    "objectID": "materials/lab/tableau.html#download",
    "href": "materials/lab/tableau.html#download",
    "title": "Tableau",
    "section": "",
    "text": "Download Tableau Public from the official website:\nhttps://public.tableau.com/app/discover\n\nChoose the installer for your operating system:\n\nWindows (Tableau Public for Windows)\nmacOS (Tableau Public for Mac)"
  },
  {
    "objectID": "materials/lab/tableau.html#windows-installation",
    "href": "materials/lab/tableau.html#windows-installation",
    "title": "Tableau",
    "section": "",
    "text": "Download the installer (.exe). \nRun the installer. \nAccept the license agreement.\nKeep the default installation options. \nLaunch Tableau Public after installation.\n\n\n\n\nOpen Tableau Public from the Start Menu.\nIf the application launches without errors, the setup is complete.\n&lt;!‚Äì TODO: screenshot - Tableau Public first launch (Windows"
  },
  {
    "objectID": "materials/lab/python.html",
    "href": "materials/lab/python.html",
    "title": "Python",
    "section": "",
    "text": "Download Miniconda from the official website:\ndownload link\n\nChoose the installer for your operating system:\n\nWindows (Miniconda3 Windows 64-bit)\nmacOS (Miniconda3 macOS Intel or macOS Apple Silicon)\n\nSelect Python 3.13.\n\n\n\n\n\n\n\n\nDownload the Miniconda installer (.exe). \nRun the installer. \nDuring setup, enable:\n\nAdd Miniconda to my PATH environment variable\nRegister Miniconda as the system Python \n\nComplete the installation and restart your terminal.\n\n\n\n\nOpen PowerShell or Command Prompt and run:\nconda --version\n\nThen test the Python installation:\npython --version\n\nCheck whether pip is installed correctly:\npip --version\n\n\n\n\n\n\n\n\n\nDownload the macOS installer (.pkg for Intel, .sh for Apple Silicon). \nIf using the .pkg file, run it normally. \nIf using the .sh installer:\n\nbash Miniconda3-py313_*-MacOSX-arm64.sh\n\n\nAccept the license terms.\nAllow Miniconda to initialize Conda in your shell.\nRestart Terminal.\n\n\n\n\nRun:\nconda --version\n\nThen verify Python:\npython --version\n\n\n\n\n\n\n\n\nExample environment with Python 3.13:\nconda create -n myenv python=3.13\n\n\n\n\nconda activate myenv\n\n\n\n\nconda deactivate\n\n\n\nconda env list\n\n\n\n\n\n\n\n\nconda install numpy\n\n\n\nconda install pandas matplotlib seaborn\n\n\n\nInside an activated environment:\npip install requests\n\n\n\n\n\n\nconda update conda\n\n\n\n\n\n\npandas\n\nnumpy\n\nmatplotlib\n\nseaborn\n\nscikit-learn\n\njupyter\n\nplotly\n\nfolium\n\nipykernel (for VS Code integration)\n\nExample install:\nconda install pandas numpy matplotlib seaborn scikit-learn jupyter ipykernel\n\nWe will explore these during class.\n\n\n\n\nAfter activating the environment:\npython -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\n\nThen in VS Code:\n\nOpen Command Palette (Ctrl+Shift+P or Cmd+Shift+P)\nSelect Python: Select Interpreter\nChoose Python (myenv)"
  },
  {
    "objectID": "materials/lab/python.html#download",
    "href": "materials/lab/python.html#download",
    "title": "Python",
    "section": "",
    "text": "Download Miniconda from the official website:\ndownload link\n\nChoose the installer for your operating system:\n\nWindows (Miniconda3 Windows 64-bit)\nmacOS (Miniconda3 macOS Intel or macOS Apple Silicon)\n\nSelect Python 3.13."
  },
  {
    "objectID": "materials/lab/python.html#windows-installation",
    "href": "materials/lab/python.html#windows-installation",
    "title": "Python",
    "section": "",
    "text": "Download the Miniconda installer (.exe). \nRun the installer. \nDuring setup, enable:\n\nAdd Miniconda to my PATH environment variable\nRegister Miniconda as the system Python \n\nComplete the installation and restart your terminal.\n\n\n\n\nOpen PowerShell or Command Prompt and run:\nconda --version\n\nThen test the Python installation:\npython --version\n\nCheck whether pip is installed correctly:\npip --version"
  },
  {
    "objectID": "materials/lab/python.html#macos-installation",
    "href": "materials/lab/python.html#macos-installation",
    "title": "Python",
    "section": "",
    "text": "Download the macOS installer (.pkg for Intel, .sh for Apple Silicon). \nIf using the .pkg file, run it normally. \nIf using the .sh installer:\n\nbash Miniconda3-py313_*-MacOSX-arm64.sh\n\n\nAccept the license terms.\nAllow Miniconda to initialize Conda in your shell.\nRestart Terminal.\n\n\n\n\nRun:\nconda --version\n\nThen verify Python:\npython --version"
  },
  {
    "objectID": "materials/lab/python.html#creating-and-managing-environments",
    "href": "materials/lab/python.html#creating-and-managing-environments",
    "title": "Python",
    "section": "",
    "text": "Example environment with Python 3.13:\nconda create -n myenv python=3.13\n\n\n\n\nconda activate myenv\n\n\n\n\nconda deactivate\n\n\n\nconda env list"
  },
  {
    "objectID": "materials/lab/python.html#installing-packages",
    "href": "materials/lab/python.html#installing-packages",
    "title": "Python",
    "section": "",
    "text": "conda install numpy\n\n\n\nconda install pandas matplotlib seaborn\n\n\n\nInside an activated environment:\npip install requests"
  },
  {
    "objectID": "materials/lab/python.html#updating-conda",
    "href": "materials/lab/python.html#updating-conda",
    "title": "Python",
    "section": "",
    "text": "conda update conda"
  },
  {
    "objectID": "materials/lab/python.html#recommended-packages-for-the-course",
    "href": "materials/lab/python.html#recommended-packages-for-the-course",
    "title": "Python",
    "section": "",
    "text": "pandas\n\nnumpy\n\nmatplotlib\n\nseaborn\n\nscikit-learn\n\njupyter\n\nplotly\n\nfolium\n\nipykernel (for VS Code integration)\n\nExample install:\nconda install pandas numpy matplotlib seaborn scikit-learn jupyter ipykernel\n\nWe will explore these during class."
  },
  {
    "objectID": "materials/lab/python.html#adding-environment-to-vs-code",
    "href": "materials/lab/python.html#adding-environment-to-vs-code",
    "title": "Python",
    "section": "",
    "text": "After activating the environment:\npython -m ipykernel install --user --name myenv --display-name \"Python (myenv)\"\n\nThen in VS Code:\n\nOpen Command Palette (Ctrl+Shift+P or Cmd+Shift+P)\nSelect Python: Select Interpreter\nChoose Python (myenv)"
  },
  {
    "objectID": "materials/lab/index.html",
    "href": "materials/lab/index.html",
    "title": "Lab Sessions",
    "section": "",
    "text": "Vs Code\n\n\nLab Session\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nLab Sessions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit and GitHub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nLab Sessions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableau\n\n\nLab Sessions\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "materials/sql/session3.html",
    "href": "materials/sql/session3.html",
    "title": "Session 03: Data Analysis with SQL | Part I",
    "section": "",
    "text": "Before diving into more SQL syntax, let‚Äôs pause for a moment to consider what happens behind the scenes when a SQL query is executed.\nTo understand why this matters, think about an e-commerce company that sells products online (similar to our case study). If management wants to reduce operational costs or improve customer experience, they must analyze and optimize many background processes‚Äîinventory management, order fulfillment, payment processing, and delivery.\nThe same idea applies when running SQL queries.\n\n\n\n\n\n\nImportant\n\n\n\nWhat you write is only part of the story; how the database executes it matters just as much.\n\n\n\n\nThe first key concept to understand is that SQL is a declarative programming language.\nThis means:\n\nYou tell the database what result you want\nYou do not tell it how to get that result\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen you write a SELECT statement, you describe the desired output, and the database engine (RDBMS) decides the most efficient way to retrieve the data.\n\n\nConsider the following Declarative Query:\nSELECT product_name\nFROM products;\nThis statement tells the database:\n\nreturn all values from the product_name column\nread the data from the products table\n\nWhat it does not specify:\n\nwhich indexes to use\nhow the data should be scanned\nwhether to read data sequentially or via an index\n\nAll of these decisions are handled internally by the database engine.\n\n\n\nWhen a query is submitted:\n\nThe database parses the SQL statement\nIt checks syntax and permissions\nThe query planner evaluates multiple execution strategies\nThe optimizer selects the most efficient execution plan\nThe query is executed and results are returned\n\nBecause of this process, part of writing efficient SQL queries is trusting the database to do its job‚Äîwhile still writing clear, well-structured queries that allow the optimizer to work effectively.\n\n\n\nAs a data analyst, this means:\n\nYou focus on business logic and correctness\nThe database focuses on performance and execution\nClean SQL enables better optimization\nProper indexes and constraints support faster queries\n\nUnderstanding what happens behind the scenes will help you:\n\ninterpret slow queries\nwrite more efficient SQL\ncollaborate more effectively with data engineers and database admistrators\n\nThis foundation becomes especially important as datasets grow larger and queries become more complex.\n\nIf this query plan looks confusing right now. You don‚Äôt need to understand every detail. In fact, the only element you need to take note of is the cost. Now, this cost doesn‚Äôt refer to the actual cost of running the query, rather it‚Äôs an arbitrary unit that‚Äôs used to indicate the estimated run time of the query. The cost, or time, of returning the first row is 0, but the cost of returning all the rows is 64. Since the cost unit doesn‚Äôt refer to a specific second or minute, all this tells us is that a query with a cost of 64 will take longer than a query with a cost of 30.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 03: Data Analysis with SQL | Part I"
    ]
  },
  {
    "objectID": "materials/sql/session3.html#behind-the-scenes",
    "href": "materials/sql/session3.html#behind-the-scenes",
    "title": "Session 03: Data Analysis with SQL | Part I",
    "section": "",
    "text": "Before diving into more SQL syntax, let‚Äôs pause for a moment to consider what happens behind the scenes when a SQL query is executed.\nTo understand why this matters, think about an e-commerce company that sells products online (similar to our case study). If management wants to reduce operational costs or improve customer experience, they must analyze and optimize many background processes‚Äîinventory management, order fulfillment, payment processing, and delivery.\nThe same idea applies when running SQL queries.\n\n\n\n\n\n\nImportant\n\n\n\nWhat you write is only part of the story; how the database executes it matters just as much.\n\n\n\n\nThe first key concept to understand is that SQL is a declarative programming language.\nThis means:\n\nYou tell the database what result you want\nYou do not tell it how to get that result\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen you write a SELECT statement, you describe the desired output, and the database engine (RDBMS) decides the most efficient way to retrieve the data.\n\n\nConsider the following Declarative Query:\nSELECT product_name\nFROM products;\nThis statement tells the database:\n\nreturn all values from the product_name column\nread the data from the products table\n\nWhat it does not specify:\n\nwhich indexes to use\nhow the data should be scanned\nwhether to read data sequentially or via an index\n\nAll of these decisions are handled internally by the database engine.\n\n\n\nWhen a query is submitted:\n\nThe database parses the SQL statement\nIt checks syntax and permissions\nThe query planner evaluates multiple execution strategies\nThe optimizer selects the most efficient execution plan\nThe query is executed and results are returned\n\nBecause of this process, part of writing efficient SQL queries is trusting the database to do its job‚Äîwhile still writing clear, well-structured queries that allow the optimizer to work effectively.\n\n\n\nAs a data analyst, this means:\n\nYou focus on business logic and correctness\nThe database focuses on performance and execution\nClean SQL enables better optimization\nProper indexes and constraints support faster queries\n\nUnderstanding what happens behind the scenes will help you:\n\ninterpret slow queries\nwrite more efficient SQL\ncollaborate more effectively with data engineers and database admistrators\n\nThis foundation becomes especially important as datasets grow larger and queries become more complex.\n\nIf this query plan looks confusing right now. You don‚Äôt need to understand every detail. In fact, the only element you need to take note of is the cost. Now, this cost doesn‚Äôt refer to the actual cost of running the query, rather it‚Äôs an arbitrary unit that‚Äôs used to indicate the estimated run time of the query. The cost, or time, of returning the first row is 0, but the cost of returning all the rows is 64. Since the cost unit doesn‚Äôt refer to a specific second or minute, all this tells us is that a query with a cost of 64 will take longer than a query with a cost of 30.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 03: Data Analysis with SQL | Part I"
    ]
  },
  {
    "objectID": "materials/sql/session3.html#query-plan",
    "href": "materials/sql/session3.html#query-plan",
    "title": "Session 03: Data Analysis with SQL | Part I",
    "section": "Query Plan",
    "text": "Query Plan\nAs you become more familiar with SQL, you will start combining multiple SQL commands into scripts.\nWhen working with large databases‚Äîoften containing millions or billions of records‚Äîit becomes essential to consider the time and cost of running those scripts.\nFor example, imagine working with a large transactional system that stores sales and product data.\nRunning a poorly optimized query on a frequently accessed table could significantly slow down reports and increase infrastructure costs. That‚Äôs why queries that run often should be optimized before being deployed.\nFaster queries mean:\n\nlower computational cost\nbetter system performance\nquicker insights for decision-makers\n\n\nWhat Is a Query Plan?\nA query plan shows how the database intends to execute a SQL query.\nMost relational database management systems (RDBMS), including PostgreSQL, can estimate:\n\nhow long a query will take\nhow many rows will be processed\nwhich operations will be performed (scans, joins, filters)\n\nTo generate a query plan, you simply add the keyword EXPLAIN before your SQL query.\nSuppose you want to retrieve all records from a sales table and understand how expensive this operation might be.\nEXPLAIN\nSELECT *\nFROM sales;\nWhen this query is executed, PostgreSQL does not return the data itself.\nInstead, it returns a query plan describing how the data would be retrieved.\nA basic query plan might look like this:\n\"Seq Scan on sales  (cost=0.00..92.00 rows=5000 width=34)\"\nThis output tells us:\n\nSeq Scan: PostgreSQL is performing a sequential scan, reading every row in the table\nsales: the table being scanned\ncost: an internal estimate of startup and total execution cost\nrows: estimated number of rows returned\nwidth: average size of each row in bytes\n\nA sequential scan is common when:\n\nthe table is relatively small\nno suitable index exists\nmost rows are expected to be returned\n\n\n\n\n\n\n\nTip\n\n\n\nTry yourself\n\n\nUnderstanding query plans helps you:\n\ndiagnose slow queries\ndecide when indexes are needed\nwrite more efficient SQL\ncommunicate performance issues to data engineers or DBAs\n\nAs datasets grow, even simple queries can become expensive.\nLearning to read and reason about query plans is a key step toward writing production-ready SQL.\n\n\n\n\n\n\nTip\n\n\n\nQuery optimization is a huge topic, most of which goes beyond the scope of the bootcamp and the role of a data analyst. Just know that writing optimized queries is a good habit to get into, especially queries that will be executed frequently.\nIf you‚Äôd like to read more about query optimization:\n\nthis articale is a good place to start: SQL Tuning or Query Optimization\nPostgreSQL tuning\nExecution order",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 03: Data Analysis with SQL | Part I"
    ]
  },
  {
    "objectID": "materials/sql/session3.html#refining-your-sql-queries",
    "href": "materials/sql/session3.html#refining-your-sql-queries",
    "title": "Session 03: Data Analysis with SQL | Part I",
    "section": "Refining Your SQL Queries",
    "text": "Refining Your SQL Queries\nWe have learned that reducing the cost of your queries is important.\nBut how do you actually optimize SQL queries in practice?\nOne of the simplest and most effective ways is to be precise about what data you request.\nBefore introducing additional commands, it‚Äôs worth revisiting SELECT, because it is the foundation of almost every analytical query.\n\nSELECT\nAs you already know, SELECT allows you to retrieve data from a database.\nA SELECT statement usually follows this structure:\nSELECT column_name_1,\n       column_name_2\nFROM table_name;\nIf you want to retrieve multiple columns, you list them separated by commas.\nConsider the products table:\nSELECT product_name,\n       price,\n       category\nFROM products;\nThis query tells the database to:\n\nreturn the product_name\nreturn the price\nreturn the category\nread the data from the products table\n\nOnly these three columns will be included in the result set.\n\n\nWhy Column Selection Matters?\nSelecting only the columns you actually need:\n\nreduces the amount of data transferred\nimproves query performance\nmakes results easier to read and interpret\n\n\n\n\n\n\n\nImportantTry yourself\n\n\n\nCompare the following two queries:\nEXPLAIN\nSELECT *\nFROM products;\nEXPLAIN\nSELECT product_name, price\nFROM products;\nThe second query is usually preferable when you only need product names and prices.\n\n\n\n\nORDER BY\nThe ORDER BY clause sorts query results in ascending or descending order.\nIt is commonly used when you want to view:\n\nthe first or last records\nhighest or lowest values\nalphabetically ordered text data\n\nBy default, ORDER BY sorts results in ascending order.\nSELECT column_name\nFROM table_name\nORDER BY column_name;\nYou can explicitly control the sorting direction using:\n\nASC for ascending order\nDESC for descending order\n\nSuppose you want to view products sorted by name in descending alphabetical order (Z \\(\\rightarrow\\) A).\nSELECT\n  product_name,\n  category\nFROM products\nORDER BY product_name DESC;\nThis query returns:\n\nproduct_name\ncategory\n\nsorted from Z to A, because product_name is a text (character) column.\nIf you sort a numeric column in descending order, results are ordered from largest to smallest.\nSELECT\n  product_name,\n  price\nFROM products\nORDER BY price DESC;\nThis query returns the most expensive products first.\nYou can sort by more than one column.  The database applies sorting from left to right.\nSELECT\n  product_name,\n  category,\n  price\nFROM products\nORDER BY category ASC, price DESC;\nThis means:\n\nProducts are grouped by category (A \\(\\rightarrow\\) Z)\nWithin each category, products are sorted by price (high \\(\\rightarrow\\) low)\n\nTo sum up:\n\nORDER BY is applied after SELECT and FROM\nDefault sort order is ascending\nUse DESC for reverse ordering\nText columns are sorted alphabetically\nNumeric columns are sorted by value\nMultiple columns can be combined for fine-grained control\n\n\n\n\n\n\n\nTip\n\n\n\nIn the ORDER BY statment, instead of writing column names , we can simply provide its index.\nFor example:\nSELECT\n  product_name,\n  category,\n  price\nFROM products\nORDER BY 2 ASC, 3 DESC;\n\n\n\n\nLIMIT\nThe LIMIT clause restricts the number of rows returned by a query.\nIt is useful when:\n\nyou only need a small sample of the data\nyou want to preview results\nyou want to reduce query cost and execution time\n\nBasic Syntax:\nSELECT column_name\nFROM table_name\nLIMIT number_of_rows;\nThe value provided to LIMIT must always be a number.\nExample:\nSuppose you want to retrieve product names and prices, sorted by price from highest to lowest, but you only care about the top 10 most expensive products.\nSELECT\n  product_name,\n  price\nFROM products\nORDER BY price DESC\nLIMIT 10;\nThis query:\n\nsorts products by price in descending order\nreturns only the first 10 rows\n\nUsing LIMIT in this way saves time and computational resources, especially when working with large tables.\n\n\n\n\n\n\nImportant\n\n\n\n\nLIMIT is always written at the end of a query\nit must be placed after ORDER BY (if present)\nthe value must be numeric\n\nIf LIMIT is placed elsewhere in the query, PostgreSQL will return an error.\n\n\n\n\n\n\n\n\n\nNoteDatabase Compatibility Note\n\n\n\nNot all SQL databases use LIMIT.\n\nPostgreSQL, MySQL, SQLite \\(\\rightarrow\\) LIMIT\nSQL Server \\(\\rightarrow\\) TOP\n\n\n\n\n\nGROUP BY\nThe GROUP BY clause is very popular among data analysts :)\nThe GROUP BY clause allows you to group rows that share the same values in one or more columns.\nIt is commonly used together with aggregate functions to summarize data.\nSimilar to Excel, SQL provides several aggregate functions that allow you to perform calculations across multiple rows of data.\nBelow is a list of commonly used aggregate functions.\nNote that SUM and AVG can only be applied to numeric columns.\n\nCOUNT() counts the number of rows\nMAX() finds the maximum value in a column\nMIN() finds the minimum value in a column\nSUM() adds up all numeric values in a column\nAVG() calculates the average of numeric values in a column\n\n\n\n\n\n\n\nNote\n\n\n\nWe are going to learn more functions later.\n\n\n\nExample | Grouping Sales by Product\nSuppose you want to calculate total sales per product.\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id;\nThis query:\n\ngroups rows by product_id\ncalculates total revenue per product\nreturns one row per product\n\n\n\nKey Rules for GROUP BY\n\nEvery column in SELECT must either:\n\nappear in the GROUP BY clause, or\nbe wrapped in an aggregate function\n\nGROUP BY is evaluated after WHERE and before ORDER BY\n\n\n\nExample | Grouping with Sorting and Limiting\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nORDER BY total_revenue DESC\nLIMIT 5;\nThis query returns the top 5 products by total revenue.\n\n\n\n\n\n\nTip\n\n\n\n\nLIMIT controls how many rows are returned\nGROUP BY controls how rows are aggregated\nBoth are essential tools for efficient, scalable data analysis\nThey are often used together in real-world analytical queries\n\n\n\n\n\nExample | Discovering Distinct Values\nGROUP BY can also be used to identify distinct values in a column.\nThis is useful when you want to understand how data is organized without seeing every individual row.\nTo see all product categories in the database, you could start with:\nSELECT \n  category\nFROM products;\nThis query returns the category for every product, which may include many duplicates. If you only want to know which categories exist, GROUP BY is the right tool.\nSELECT \n  category,\n  COUNT(*)\nFROM products\nGROUP BY category;\nThis query:\n\ngroups rows by category\nreturns one row per unique category\nremoves duplicate category values from the output\n\nThe result is a concise summary of the different product categories stored in the database.\nUsing GROUP BY in this way helps you:\n\nexplore the structure of a dataset\nvalidate categorical columns\nprepare data for aggregation and reporting\n\nIn practice, GROUP BY becomes even more powerful when combined with aggregate functions, which you‚Äôll explore next.\n\n\nExample | Sales per Pruduct\nAggregate functions are most powerful when combined with GROUP BY, which allows you to summarize data across categories. Suppose you are working with a sales department and want to understand how many sales transactions exist per product.\nTo begin, recall that the sales table may contain many rows per product.\nSELECT\n  product_id,\n  COUNT(transaction_id) AS number_of_transactions\nFROM sales\nGROUP BY product_id;\nThis query:\n\ngroups rows by product_id\ncounts how many transactions exist for each product\nreturns one row per product\n\n\n\nExample | Total Revenue per Product\nYou can also summarize numeric columns such as total revenue.\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id;\nHere:\n\nSUM(total_sales) adds up all sales values per product\nthe result shows total revenue generated by each product\n\n\n\nExample | Average Price by Category\nAggregate functions are often used for higher-level analysis.\nSELECT\n  category,\n  AVG(price) AS average_price\nFROM products\nGROUP BY category;\nThis query:\n\ngroups products by category\ncalculates the average price within each category\n\n\n\nGROUP BY with Multiple Columns\nYou can also group data by multiple columns.\nTo do this, simply list all grouping columns after GROUP BY, separated by commas.\nThis allows you to create more granular summaries by combining multiple dimensions.\n\nExample | Grouping by Multiple Attributes\nSuppose you want to understand how many sales transactions exist for each product and employee_id (later we will get the employee name) combination.\nSELECT\n  product_id,\n  COUNT(transaction_id) AS transaction_count\nFROM sales\nGROUP BY product_id\nORDER BY COUNT(transaction_id) DESC;\nThis query:\n\ngroups records by both product_id and employee_id\ncounts the number of transactions in each combination\nreturns one row per unique (product_id, employee_id) pair\n\nThe output represents:\n\none row per product and employee combination\nthe total number of transactions for that specific combination\n\nGrouping by multiple columns helps you:\n\nanalyze performance across multiple dimensions\nbreak down metrics by category, region, channel, or time\nbuild detailed summaries for dashboards and reports\n\nYou will get more practice using multi-column GROUP BY as you move into data summarization, aggregation, and data cleaning tasks later in the course.\n\n\n\n\nDISTINCT\nThe DISTINCT keyword is used to return unique values from a column or a combination of columns.\nIt is especially useful when a dataset contains duplicate values and you want to understand the unique categories or combinations present in the data.\nSyntax\nSELECT DISTINCT column_name\nFROM table_name;\n\nExample | Finding Unique Values\nSuppose you want to see all unique product categories in the products table.\nSELECT DISTINCT category\nFROM products;\nThis query returns one row per unique category, even if multiple products belong to the same category.\n\n\nDISTINCT with Multiple Columns\nDISTINCT can also be applied to multiple columns to find unique combinations.\nFor example, to find unique combinations of product category and price:\nSELECT DISTINCT\n  category,\n  price\nFROM products;\nThis returns each unique (category, price) pair found in the table.\n\n\nDISTINCT with COUNT\nSELECT DISTINCT\n  COUNT(category),\n  COUNT(DISTINCT category)\nFROM products;\n\n\nDISTINCT vs GROUP BY\nIn many cases, DISTINCT and GROUP BY can produce the same result when no aggregate functions are used.\nUsing DISTINCT:\nSELECT DISTINCT\n  category,\n  price\nFROM products;\nUsing GROUP BY:\nSELECT\n  category,\n  price\nFROM products\nGROUP BY category, price;\nBoth queries return the unique combinations of category and price.\n\n\nWhen to Use DISTINCT\nUse DISTINCT when:\n\nyou only need unique values\nno aggregation is required\nyou want simpler, more readable SQL\n\nUse GROUP BY when:\n\nyou need aggregate functions\nyou want more control over grouping logic\n\n\n\n\nGROUP BY vs DISTINCT\nWhich command do you think is faster: GROUP BY or DISTINCT?\nUse EXPLAIN to generate query plans for both queries and compare their estimated costs to see which one runs more efficiently in PostgreSQL.\n\n\nHAVING\n\nWhy HAVING Exists\nThe HAVING clause is designed to filter aggregated results.\nIn analytical queries, data is often grouped and summarized before it becomes meaningful.\nAt that stage, row-level filtering is no longer sufficient.\nHAVING fills this gap.\n\nWHERE \\(\\rightarrow\\) filters rows before aggregation\nHAVING \\(\\rightarrow\\) filters groups after aggregation\n\nUnderstanding this distinction is essential for correct and efficient SQL.\n\n\nWHERE vs HAVING\nThink of SQL execution as a pipeline.\n\nraw rows are first filtered\nrows are then grouped\naggregates are calculated\ngroups are filtered again if needed\n\nThis leads to a clear separation of responsibilities:\n\nWHERE \\(\\rightarrow\\) row-level filtering\nGROUP BY \\(\\rightarrow\\) aggregation\nHAVING \\(\\rightarrow\\) group-level filtering\n\n\n\nLogical Query Execution Order\nAlthough SQL is written top-down, it is executed in a different logical order.\nAt a high level:\n\nFROM\nWHERE\nGROUP BY\nHAVING\nSELECT\nORDER BY\nLIMIT\n\nThis order explains why aggregate functions cannot appear in WHERE, but are allowed in HAVING.\n\n\nBasic HAVING Syntax\nA typical query using HAVING looks like this:\nSELECT\n  group_column,\n  AGGREGATE_FUNCTION(column)\nFROM table_name\nGROUP BY group_column\nHAVING AGGREGATE_FUNCTION(column) condition;\nImportant points:\n\nHAVING is evaluated after aggregation\nit almost always appears together with GROUP BY\nit works on aggregated values, not raw rows\n\n\n\n\nHAVING Example | Filtering by Total Revenue\nSuppose you want to identify products that generated more than 10,000 in total revenue.\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nHAVING SUM(total_sales) &gt; 10000;\nThis query:\n\ngroups sales by product\ncalculates total revenue per product\nkeeps only products above the revenue threshold\n\n\n\nHAVING vs WHERE ‚Äî Common Mistake\nUsing WHERE with aggregate functions is not allowed.\nIncorrect example:\nSELECT\n  product_id,\n  SUM(total_sales)\nFROM sales\nWHERE SUM(total_sales) &gt; 10000\nGROUP BY product_id;\nThis fails because SUM(total_sales) does not exist at the WHERE stage of execution.\n\n\n\nCombining WHERE and HAVING\nIn real-world analytical queries, WHERE and HAVING are often used together.\nTheir roles are complementary:\n\nWHERE removes unnecessary rows early\nHAVING applies business rules after aggregation\n\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nWHERE total_sales &gt; 0\nGROUP BY product_id\nHAVING SUM(total_sales) &gt; 10000;\nThis approach is usually more efficient than relying on HAVING alone.\n\n\n\nHAVING with COUNT\nHAVING is frequently used with COUNT().\nExample: find products with at least 50 transactions.\nSELECT\n  product_id,\n  COUNT(transaction_id) AS transaction_count\nFROM sales\nGROUP BY product_id\nHAVING COUNT(transaction_id) &gt;= 50;\nHere:\n\ntransactions are grouped by product\nonly frequently sold products are retained\n\n\n\n\nHAVING with Multiple Conditions\nMultiple aggregate conditions can be combined using logical operators.\nSELECT\n  product_id,\n  COUNT(transaction_id) AS transaction_count,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nHAVING\n  COUNT(transaction_id) &gt;= 50\n  AND SUM(total_sales) &gt; 10000;\nThis allows you to enforce compound business rules at the group level.\n\n\n\nWhen to Use HAVING\nUse HAVING when:\n\nfiltering aggregated results\napplying thresholds to groups\nworking with SUM, COUNT, AVG, MIN, or MAX\n\nDo not use HAVING when row-level filtering with WHERE is sufficient.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 03: Data Analysis with SQL | Part I"
    ]
  },
  {
    "objectID": "materials/sql/session3.html#practice-2",
    "href": "materials/sql/session3.html#practice-2",
    "title": "Session 03: Data Analysis with SQL | Part I",
    "section": "Practice 2",
    "text": "Practice 2\nAssume the following:\n\nTables customers, products, and sales already exist\nTheir schemas and columns are already documented\nYour task is not discovery, but improvement, enforcement, and optimization\n\nYou must not recreate tables or re-document schemas.\n\nTask 1 | Enforce Missing Business Rules with ALTER TABLE\nEven with a documented schema, databases often lack enforced rules.\nApply the following constraints using ALTER TABLE:\nBusiness rules:\n\nEmployees emails must be unique\nEmoployee phone numbers must be mandatory\nProduct prices must be non-negative\nSales totals must be non-negative\n\nALTER TABLE employees\nADD CONSTRAINT uq_employees_email UNIQUE (email);\nALTER TABLE employees\nALTER COLUMN phone_number SET NOT NULL;\nALTER TABLE products\nADD CONSTRAINT chk_products_price CHECK (price &gt;= 0);\nALTER TABLE sales\nADD CONSTRAINT chk_sales_total CHECK (total_sales &gt;= 0);\n\n\nTask 2 | Add a New Analytical Attribute\nThe business now wants to analyze sales performance by channel.\nAdd a new column to the sales table:\n\nColumn name: sales_channel\nAllowed values: 'online', 'store'\n\nALTER TABLE sales\nADD COLUMN sales_channel TEXT;\nAdd a constraint to enforce valid values.\nALTER TABLE sales\nADD CONSTRAINT chk_sales_channel\nCHECK (sales_channel IN ('online', 'store'));\nPopulate the column with sample values.\nUPDATE sales\nSET sales_channel = 'online'\nWHERE transaction_id % 2 = 0;\n\n\n\n\n\n\nImportant\n\n\n\nwrite an explanation\n\n\n\n\nTask 3 | Add Indexes for Query Performance\nBased on common analytical queries, create indexes on columns that are frequently used for (according to you):  - joins - grouping - filtering\nCreate the following indexes:\nCREATE INDEX idx_sales_product_id\nON sales (product_id);\nCREATE INDEX idx_sales_customer_id\nON sales (customer_id);\nCREATE INDEX idx_products_category\nON products (category);\n\n\nTask 4 | Validate Index Usage with EXPLAIN\nRun EXPLAIN on an aggregation query that is commonly used in reporting.\nEXPLAIN\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id;\nWrite a short interpretation:\n\nIs a sequential scan used?\nDoes PostgreSQL leverage the index?\nWhy might the planner choose this plan?\n\n\n\nTask 5 | Reduce Query Cost by Refining SELECT\nRewrite a reporting query to avoid unnecessary data retrieval.\nOriginal query:\nSELECT *\nFROM sales;\nRefined query:\nSELECT\n  transaction_id,\n  product_id,\n  total_sales\nFROM sales;\nExplain in 2‚Äì3 sentences:\n\nwhy this reduces cost\nwhen SELECT * might still be acceptable\n\n\n\nTask 6 | ORDER BY and LIMIT for Business Questions\nMarketing wants to identify the top 5 products by total revenue.\nWrite a query that:\n\naggregates sales\nsorts by revenue\nlimits the output\n\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nORDER BY total_revenue DESC\nLIMIT 5;\nRun EXPLAIN and comment on:\n\nsorting cost\nwhether indexes help in this case\n\n\n\n\nTask 7 | DISTINCT vs GROUP BY (Efficiency Comparison)\nRetrieve unique combinations of category and price using both approaches.\nUsing DISTINCT:\nEXPLAIN\nSELECT DISTINCT\n  category,\n  price\nFROM products;\nUsing GROUP BY:\nEXPLAIN\nSELECT\n  category,\n  price\nFROM products\nGROUP BY category, price;\nAnswer briefly:\n\nAre the query plans similar?\nWhich has lower estimated cost?\nWhy might PostgreSQL optimize them the same way?\n\n\n\nTask 8 | Constraint Enforcement Test\nAttempt to violate at least two constraints you added earlier and observe the errors.\nExamples:\nUPDATE products\nSET price = -5\nWHERE product_id = 101;\nINSERT INTO customers (customer_id, email, phone_number)\nVALUES (999, 'anna@example.com', '091000999');\nExplain:\n\nwhich constraint was triggered\nwhy this protects data quality\n\n\n\n\nTask 9 | Reflection (Short Answer)\nAnswer briefly (3‚Äì5 sentences total):\n\nWhich constraints provide the highest business value?\nWhich index would you prioritize in a production environment?\nWhat signals tell you a query needs optimization?\n\n\n\nSubmission Rules\n\nNo table creation\nNo schema documentation\nOnly ALTER, CREATE INDEX, UPDATE, EXPLAIN, SELECT, WHERE, GROUP BY, HAVING, ORDER BY, LIMI\nInclude SQL and short written interpretations\nRemember you need to put the code as .sql file\nAdd to staging by git add .\nCommit the changes by git comit -m \"meaningfull message\"\nPush to GitHub! git push",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 03: Data Analysis with SQL | Part I"
    ]
  },
  {
    "objectID": "materials/sql/session5.html",
    "href": "materials/sql/session5.html",
    "title": "Session 05: Data Analysis with SQL | Functions",
    "section": "",
    "text": "Before jumping to the built-in SQL Funcions let‚Äôs ensure that we have sales_analysis table.\n\n\nLets run the Docker in detached mode:\ndocker compose up -d\n\n\n\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;\n\n\n\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);\n\n\n\n\n\n\nNote\n\n\n\nNote, that we are going to take closer look on JOINs during the next session.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 05: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session5.html#from-session-4",
    "href": "materials/sql/session5.html#from-session-4",
    "title": "Session 05: Data Analysis with SQL | Functions",
    "section": "",
    "text": "Before jumping to the built-in SQL Funcions let‚Äôs ensure that we have sales_analysis table.\n\n\nLets run the Docker in detached mode:\ndocker compose up -d\n\n\n\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;\n\n\n\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);\n\n\n\n\n\n\nNote\n\n\n\nNote, that we are going to take closer look on JOINs during the next session.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 05: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session5.html#numeric-functions",
    "href": "materials/sql/session5.html#numeric-functions",
    "title": "Session 05: Data Analysis with SQL | Functions",
    "section": "Numeric Functions",
    "text": "Numeric Functions\nThis section provides a clean, end-to-end coverage of numeric functions, organized from basic to advanced, with a strong analytical mindset.\nFor each function, you will see:\n\na conceptual input table\nthe SQL query\nthe output table\ncommon analytical use cases\nanalytical interpretation\n\nAll examples assume the sales_analysis table.\n\nSUM()\nSUM() is one of the most frequently used SQL functions.\n\nSUM() Basic Aggregation\nSUM() adds numeric values across multiple rows.\nInput\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\n80.00\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntotal_revenue\n\n\n\n\n300.00\n\n\n\n\n\nSUM() and NULL Values | Default Behavior\nBy default, SUM() ignores NULL values.\nInput\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\nNULL\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntotal_revenue\n\n\n\n\n220.00\n\n\n\nKey points\n\nNULL values do not contribute to the sum\n\nNULL does not mean zero\n\nif all values are NULL, the result of SUM() is NULL\n\nHandling missing values explicitly will be covered later in a dedicated chapter. \n\n\n\nSUM() with GROUP BY | Aggregation by Category\nGrouping allows SUM() to operate per dimension.\nInput\n\n\n\ncategory\ntotal_sales\n\n\n\n\nAccessories\n50.00\n\n\nAccessories\n70.00\n\n\nElectronics\n120.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  category,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY category;\n\\[\\downarrow\\]\n\n\n\ncategory\ntotal_revenue\n\n\n\n\nAccessories\n120.00\n\n\nElectronics\n120.00\n\n\n\n\n\nAnalytical Use Cases\n\ntotal revenue calculation\n\ntotal cost or volume\n\nhigh-level KPI reporting\n\n\n\nrevenue by product category\n\ncost by department\n\nKPI breakdowns for dashboards\n\n\n\n\n\n\n\nWarningAnalytical Warning | Always Check the Grain\n\n\n\nBefore using SUM(), always verify the grain of the table.\n\nsales_analysis is at the transaction level\n\nsumming values duplicated per transaction (e.g.¬†salaries) produces incorrect results\n\naggregation must match the entity you want to measure\n\nCorrect use of SUM() depends as much on data structure as on SQL syntax.\n\n\n\n\n\n\nAVG()\nAVG() is a numeric aggregate function used to calculate the arithmetic mean of a set of values.\nIn analytics, averages are commonly used to understand typical behavior, but they must be interpreted carefully‚Äîespecially in the presence of NULL values, outliers, and skewed distributions.\n\n\nAVG() Basic Aggregation\nAVG() calculates the sum of values divided by the number of non-NULL observations.\nInput\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\n0.00\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  AVG(discount) AS avg_discount\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\navg_discount\n\n\n\n\n0.10\n\n\n\nThe average discount is calculated as:\n\\[\n\\frac{0.10 + 0.00 + 0.20}{3} = 0.10\n\\]\n\n\n\nAVG() and NULL Values | Default Behavior\nBy default, AVG() ignores NULL values in both the numerator and the denominator.\nInput\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  AVG(discount) AS avg_discount\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\navg_discount\n\n\n\n\n0.15\n\n\n\nExplanation\n\nonly non-NULL values are used\n\ndenominator is 2, not 3\n\n\\[\n\\frac{0.10 + 0.20}{2} = 0.15\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nThis behavior is often unexpected for beginners and must be understood clearly.\n\n\n\n\n\nAVG() with GROUP BY | Average per Category\nGrouping allows AVG() to compute averages per dimension.\nInput\n\n\n\ncategory\nprice\n\n\n\n\nAccessories\n25.00\n\n\nAccessories\n35.00\n\n\nElectronics\n120.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  category,\n  AVG(price) AS avg_price\nFROM sales_analysis\nGROUP BY category;\n\\[\\downarrow\\]\n\n\n\ncategory\navg_price\n\n\n\n\nAccessories\n30.00\n\n\nElectronics\n120.00\n\n\n\n\n\n\nAVG() with GROUP BY | Average Sales per Customer\nA common analytical use case is average behavior per entity.\nSELECT\n  customer_name,\n  AVG(total_sales) AS avg_transaction_value\nFROM sales_analysis\nGROUP BY customer_name;\nThis answers questions such as:\n\nwhat is the typical transaction value per customer?\nwho are high-value vs low-value customers?\n\n\n\n\nAnalytical Interpretation | When AVG() Is Appropriate\nUse AVG() when:\n\ndata is symmetrically distributed\nextreme values are rare\nyou want a representative ‚Äútypical‚Äù value\n\nTypical examples:\n\naverage order value\n\naverage discount rate\n\naverage price per category\n\n\n\n\nAnalytical Limitation | Sensitivity to Outliers\nAVG() is highly sensitive to outliers.\nExample scenario\n\n\n\ntotal_sales\n\n\n\n\n20\n\n\n25\n\n\n30\n\n\n500\n\n\n\nThe average is pulled upward by a single extreme value.\nIn such cases, the average may misrepresent typical behavior.\n\n\n\nAVG() vs Median | Conceptual Note\n\nAVG() reflects the mean\nthe median reflects the middle value\n\nUse:\n\nAVG ‚Üí for stable, normally distributed data\n\nMedian ‚Üí for skewed data or income-like variables\n\nMedian calculations will be covered later in the course.\n\n\n\n\n\n\nWarningAnalytical Warning | Always Check the Distribution\n\n\n\nBefore relying on AVG(), consider:\n\npresence of outliers\n\nskewness of the data\n\nwhether ‚Äútypical‚Äù behavior is meaningful\n\nBlindly using averages is one of the most common analytical mistakes.\n\n\n\n\nAnalytical Use Cases\n\naverage order value\n\naverage discount per transaction\n\naverage price by category\n\nbenchmarking typical performance\n\n\n\nAVG() computes the arithmetic mean\n\nNULL values are ignored by default\n\ngrouping enables per-dimension averages\n\naverages are sensitive to outliers\n\ndistribution shape matters for interpretation\n\n\n\n\n\n\n\nImportant\n\n\n\nUnderstanding when not to use AVG() is just as important as knowing how to write it.\n\n\n\n\n\nMIN() and MAX()\nMIN() and MAX() are aggregate functions used to identify extreme values in a dataset.\nThey answer the questions:\n\nWhat is the smallest value?\nWhat is the largest value?\n\nThese functions are widely used in exploratory analysis, data validation, and boundary checks.\n\n\nMIN() and MAX() | Basic Aggregation\nMIN() returns the lowest value, and MAX() returns the highest value in a column.\nInput\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\n80.00\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  MIN(total_sales) AS min_sale,\n  MAX(total_sales) AS max_sale\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nmin_sale\nmax_sale\n\n\n\n\n80.00\n120.50\n\n\n\n\n\n\nMIN() and MAX() and NULL Values | Default Behavior\nBoth MIN() and MAX() ignore NULL values by default.\nInput\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\nNULL\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  MIN(total_sales) AS min_sale,\n  MAX(total_sales) AS max_sale\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nmin_sale\nmax_sale\n\n\n\n\n99.50\n120.50\n\n\n\nKey points\n\nNULL values do not participate in comparisons\n\nif all values are NULL, the result is NULL\n\n\n\n\nMIN() and MAX() with GROUP BY | Extremes per Category\nGrouping allows you to find minimum and maximum values per dimension.\nInput\n\n\n\ncategory\nprice\n\n\n\n\nAccessories\n25.00\n\n\nAccessories\n35.00\n\n\nElectronics\n120.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  category,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price\nFROM sales_analysis\nGROUP BY category;\n\\[\\downarrow\\]\n\n\n\ncategory\nmin_price\nmax_price\n\n\n\n\nAccessories\n25.00\n35.00\n\n\nElectronics\n120.00\n120.00\n\n\n\n\n\n\nMIN() and MAX() on Text Columns\nMIN() and MAX() also work on text (string) columns.\nFor text values:\n\nMIN() returns the alphabetically first value\n\nMAX() returns the alphabetically last value\n\nInput\n\n\n\nproduct_name\n\n\n\n\nCable\n\n\nMouse\n\n\nKeyboard\n\n\n\n\\[\\downarrow\\]\nSELECT\n  MIN(product_name) AS first_product,\n  MAX(product_name) AS last_product\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nfirst_product\nlast_product\n\n\n\n\nCable\nMouse\n\n\n\n\nThis behavior is based on lexicographical (alphabetical) ordering.\n\n\n\n\nAnalytical Use Cases\n\nidentifying cheapest and most expensive products\n\nfinding first and last dates in time-based data\n\nvalidating numeric ranges\n\ndetecting unexpected extreme values\n\nalphabetical boundary checks for text data\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Extremes Are Not Typical\n\n\n\nMIN() and MAX() show boundaries, not typical behavior.\n\nthey are highly sensitive to outliers\n\na single abnormal value can dominate the result\n\nAlways pair extremes with other statistics such as:\n\nAVG()\n\nCOUNT()\n\ndistribution analysis\n\nto get a complete picture.\n\nMIN() and MAX() are essential tools for understanding the limits of your data, but they should never be used in isolation.\n\n\n\n\n\nCOUNT()\nCOUNT() is one of the most fundamental SQL aggregate functions.\nIt is used to measure volume, cardinality, and data completeness.\nUnlike SUM() or AVG(), COUNT() does not depend on numeric magnitude.\nIt answers a simpler‚Äîbut critically important‚Äîquestion:\n\nHow many rows or values exist?\n\nIn analytics, incorrect use of COUNT() is a common source of silent errors, especially when NULLs or duplicates are involved.\n\n\nCOUNT() | Basic Row Counting\nCOUNT(*) counts the number of rows returned by a query.\nInput\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  COUNT(*) AS total_rows\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntotal_rows\n\n\n\n\n3\n\n\n\nCOUNT(*) includes all rows, regardless of NULL values.\n\n\n\nCOUNT(column) | Counting Non-NULL Values\nCOUNT(column) counts only non-NULL values in a specific column.\nInput\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  COUNT(discount) AS non_null_discounts\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nnon_null_discounts\n\n\n\n\n2\n\n\n\nKey distinction\n\nCOUNT(*) ‚Üí counts rows\nCOUNT(column) ‚Üí counts existing values\n\n\n\n\nCOUNT() with GROUP BY | Volume per Dimension\nGrouping allows COUNT() to measure volume by category.\nInput\n\n\n\nproduct_name\ntransaction_id\n\n\n\n\nMouse\n1001\n\n\nMouse\n1002\n\n\nKeyboard\n1003\n\n\n\n\\[\\downarrow\\]\nSELECT\n  product_name,\n  COUNT(*) AS transaction_count\nFROM sales_analysis\nGROUP BY product_name;\n\\[\\downarrow\\]\n\n\n\nproduct_name\ntransaction_count\n\n\n\n\nMouse\n2\n\n\nKeyboard\n1\n\n\n\nThis is one of the most common analytical patterns in SQL.\n\n\n\nCOUNT(DISTINCT) | Counting Unique Values\nCOUNT(DISTINCT column) counts the number of unique, non-NULL values.\nSELECT\n  COUNT(DISTINCT customer_name) AS unique_customers\nFROM sales_analysis;\nThis answers questions like:\n\nhow many unique customers exist?\nhow many unique products were sold?\n\n\n\n\nCOUNT(DISTINCT) with GROUP BY | Unique Entities per Group\nSELECT\n  city,\n  COUNT(DISTINCT customer_name) AS unique_customers\nFROM sales_analysis\nGROUP BY city;\nThis pattern is commonly used for:\n\nmarket reach analysis\ncustomer distribution by geography\npenetration metrics\n\n\n\n\nCOUNT() for Duplicate Detection\nDuplicates are detected using COUNT() together with GROUP BY and HAVING.\nExample | Duplicate Transactions\nSELECT\n  transaction_id,\n  COUNT(*) AS duplicate_count\nFROM sales_analysis\nGROUP BY transaction_id\nHAVING COUNT(*) &gt; 1;\nAny returned row indicates a duplicate transaction ID.\n\n\n\nCOUNT() and NULL Values | Key Rules\n\nCOUNT(*) counts rows, including those with NULLs\nCOUNT(column) ignores NULLs\nCOUNT(DISTINCT column) ignores NULLs and duplicates\nif all values are NULL, COUNT(column) returns 0\n\nUnderstanding these rules is essential for correct metrics.\n\n\n\n\n\n\n\nWarningAnalytical Warning | Define What You Are Counting\n\n\n\nBefore using COUNT(), always ask:\n\nam I counting rows or entities?\ndo I need DISTINCT?\nis the table grain aligned with my question?\n\nCounting the wrong thing leads to misleading KPIs even when SQL syntax is correct.\n\n\n\n\n\nAnalytical Use Cases\n\nnumber of transactions\nnumber of customers\ndata completeness checks\nduplicate detection\ndenominator for rates and averages\n\n\n\n\n\n\n\nImportant\n\n\n\nMastering COUNT() is a prerequisite for trustworthy analytical SQL.\n\n\n\n\n\nRow-Level Arithmetic\nRow-level arithmetic refers to calculations performed on each individual row of a table.\nUnlike aggregate functions (SUM, AVG, COUNT), these operations do not reduce rows, istead they create derived columns.\nRow-level arithmetic is foundational for:\n\nfeature engineering\n\nrevenue and cost calculations\n\nmetric normalization\n\npreparing data for aggregation\n\nIn practice, most analytical pipelines combine row-level calculations first, followed by aggregation.\n\n\nBasic Calculation\nA common example is calculating revenue at the transaction level.\nInput\n\n\n\ntransaction_id\nquantity\nprice\n\n\n\n\n1001\n2\n25.00\n\n\n1002\n1\n80.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  transaction_id,\n  quantity * price AS calculated_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntransaction_id\ncalculated_revenue\n\n\n\n\n1001\n50.00\n\n\n1002\n80.00\n\n\n\nEach row is processed independently, producing a new derived value.\n\n\nCombining Multiple Columns\nYou can combine more than two columns in a single expression.\nInput\n\n\n\ntransaction_id\nquantity\nprice\ndiscount\n\n\n\n\n1001\n2\n25.00\n0.10\n\n\n1002\n1\n80.00\n0.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  transaction_id,\n  quantity * price * (1 - discount) AS net_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntransaction_id\nnet_revenue\n\n\n\n\n1001\n45.00\n\n\n1002\n80.00\n\n\n\nThis pattern is common in pricing, discounting, and billing logic.\n\n\nOrder of Operations\nSQL follows standard arithmetic precedence:\n\nparentheses\n\nmultiplication and division\n\naddition and subtraction\n\nUse parentheses to make calculations explicit and readable.\nSELECT\n  transaction_id,\n  (quantity * price) - (quantity * price * discount) AS net_revenue\nFROM sales_analysis;\nThis is equivalent to the previous example but often clearer in business contexts.\n\n\n\nNumeric Transformations\nRow-level arithmetic is often combined with numeric functions such as ROUND, CEILING, or FLOOR.\nSELECT\n  transaction_id,\n  ROUND(quantity * price, 2) AS rounded_revenue\nFROM sales_analysis;\nThis is especially important for financial reporting.\n\n\n\nRow-Level Arithmetic vs Aggregation\nIt is critical to distinguish between:\n\nrow-level calculations ‚Üí preserve rows\n\naggregate calculations ‚Üí reduce rows\n\nSELECT\n  SUM(quantity * price) AS total_revenue\nFROM sales_analysis;\nHere:\n\nquantity * price is computed per row\nSUM() aggregates those results into a single value\n\nThis pattern‚Äîcalculate first, aggregate second‚Äîis a core analytical principle.\n\n\nCommon Analytical Use Cases\n\ntransaction-level revenue\n\nnet revenue after discounts\n\ncost per unit\n\nmargin calculations\n\nnormalized metrics\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Check NULL Propagation\n\n\n\nIn arithmetic expressions:\n\nif any operand is NULL, the result is NULL\nthis can silently remove rows from later aggregations\n\nUnderstanding NULL behavior is essential and will be revisited in a dedicated NULL-handling chapter.\n\nRow-level arithmetic is the bridge between raw data and meaningful analytical metrics.\n\n\n\n\n\nCEILING() and FLOOR()\nCEILING() and FLOOR() are numeric functions used to round values to integer boundaries.\n\nCEILING() rounds a value up to the nearest integer\n\nFLOOR() rounds a value down to the nearest integer\n\nThese functions are commonly used for:\n\nthreshold logic\nrange construction\ndistribution analysis\n\n\nThresholds and Ranges\nInput\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.10\n\n\n1002\n80.90\n\n\n\n\\[\\downarrow\\]\nSELECT\n  transaction_id,\n  CEILING(total_sales) AS rounded_up,\n  FLOOR(total_sales)   AS rounded_down\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntransaction_id\nrounded_up\nrounded_down\n\n\n\n\n1001\n121\n120\n\n\n1002\n81\n80\n\n\n\nHere:\n\nCEILING() is useful when values must meet a minimum threshold\nFLOOR() is useful when values must not exceed a maximum threshold\n\n\n\n\nAnalytical Use Cases\n\nbilling thresholds (minimum chargeable units)\n\nrounding prices or quantities for operational rules\n\nconstructing numeric ranges\n\ndiscretizing continuous values\n\n\n\nCEILING() with GROUP BY | Revenue Ranges\nCEILING() can be used to group continuous numeric values into fixed-width ranges, which is a common technique in exploratory and descriptive analytics.\nIn this example, transaction revenues are grouped into 50-unit revenue ranges.\n\nInput (sales_analysis ‚Äî simplified view)\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n12.40\n\n\n1002\n47.80\n\n\n1003\n52.10\n\n\n1004\n79.90\n\n\n1005\n101.25\n\n\n1006\n138.60\n\n\n\n\nStep 1 | Range Assignment Logic\nThe expression below assigns each row to a range:\n\\[\n\\text{revenue\\_range} = \\lceil \\frac{\\text{total\\_sales}}{50} \\rceil \\times 50\n\\]\nThis means:\n\nvalues from 0 &lt; x ‚â§ 50 ‚Üí range 50\nvalues from 50 &lt; x ‚â§ 100 ‚Üí range 100\nvalues from 100 &lt; x ‚â§ 150 ‚Üí range 150\n\n\nQuery\nSELECT\n  CEILING(total_sales / 50.0) * 50 AS revenue_range,\n  COUNT(*) AS transactions,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY CEILING(total_sales / 50.0) * 50\nORDER BY revenue_range;\n\nIntermediate Mapping (Conceptual)\n\n\n\ntransaction_id\ntotal_sales\nrevenue_range\n\n\n\n\n1001\n12.40\n50\n\n\n1002\n47.80\n50\n\n\n1003\n52.10\n100\n\n\n1004\n79.90\n100\n\n\n1005\n101.25\n150\n\n\n1006\n138.60\n150\n\n\n\n\nOutput\n\n\n\nrevenue_range\ntransactions\ntotal_revenue\n\n\n\n\n50\n2\n60.20\n\n\n100\n2\n132.00\n\n\n150\n2\n239.85\n\n\n\n\nHow to Interpret the Output\nEach row represents a revenue range, not individual transactions.\nFor example:\n\nrevenue_range = 100 represents all transactions where\n\\[50 &lt; \\text{total\\_sales} \\le 100\\]\ntransactions shows how many transactions fall into that range\ntotal_revenue shows the sum of sales within the range\n\n\n\n\nAnalytical Use Cases\n\ndistribution analysis\n\nrevenue concentration analysis\n\nidentifying dominant transaction ranges\n\npreparing binned metrics for dashboards\n\nexploratory analysis before modeling\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Ranges Are Design Choices\n\n\n\nRanges created with CEILING() or FLOOR() are analytical assumptions.\n\ndifferent range widths lead to different interpretations\n\noverly wide ranges hide structure\n\noverly narrow ranges introduce noise\n\nAlways justify:\n\nrange size\n\nboundary logic\n\nbusiness meaning\n\n\n\nCEILING() combined with GROUP BY is a powerful technique for turning continuous numeric data into interpretable analytical summaries, but its effectiveness depends on thoughtful range design.\n\n\n\nCOALESCE()\nCOALESCE() is a SQL function used for explicit NULL handling.\nIt replaces NULL values with a specified substitute, allowing you to control how missing data is treated before aggregation or analysis.\nHandling NULL values explicitly is a critical analytical step, because different substitution strategies lead to different interpretations and KPIs.\nCommon strategies include:\n\nreplacing NULL with 0\nreplacing NULL with the average\nreplacing NULL with the median\n\nEach choice has distinct analytical implications.\n\n\n\n\n\n\nNote\n\n\n\nBy default, most aggregate functions ignore NULL values.\nCOALESCE() lets you override this behavior intentionally.\n\n\n\nCOALESCE() with Zero | Treat Missing as Zero\nUse this approach only when business logic clearly defines NULL as zero,\nfor example:\n\nno discount applied\nno quantity sold\nno activity recorded\n\nInput\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  AVG(COALESCE(discount, 0)) AS avg_discount_with_zeros\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\navg_discount_with_zeros\n\n\n\n\n0.10\n\n\n\nInterpretation\n\\[\n(0.10 + 0 + 0.20) / 3 = 0.10\n\\]\nWhen to Use\n\nNULL truly means zero impact\nKPIs explicitly require zero inclusion\noperational metrics (counts, volumes)\n\n\n\n\nCOALESCE() with Average | Mean Imputation\nIn this approach, missing values are replaced with the overall average.\nThis preserves the mean level of the data but reduces variability, since all missing values become identical.\n\\[\\downarrow\\]\nSELECT\n  AVG(COALESCE(discount, avg_discount)) AS avg_discount_mean_imputed\nFROM sales_analysis,\nConceptual Effect:\n\nNULL values become the average\noverall mean remains unchanged\nvariance is reduced\n\nWhen to Use Average Imputation\n\ndistribution is approximately symmetric\nno strong outliers exist\nmissing values are random and rare\nreporting-level analysis\n\nWhen NOT to Use\n\nskewed distributions\npresence of outliers\nperformance or incentive analysis\n\n\n\n\nCOALESCE() with Median | Robust Imputation\nMedian imputation is more robust than average imputation\nbecause it is not affected by outliers.\nPostgreSQL supports median calculation using PERCENTILE_CONT(0.5).\n\\[\\downarrow\\]\nSELECT\n  AVG(COALESCE(discount, median_discount)) AS avg_discount_median_imputed\nFROM sales_analysis;\n\n\n\n\n\n\nWarning\n\n\n\nThe function below is a window function, which will be covered in upcoming sessions.\nPERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY discount)\nSELECT\n         PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY discount) AS median_discount\nFROM sales_analysis\n\n\nWhen to Use Median Imputation\n\nskewed distributions\npresence of outliers\nfinancial or behavioral metrics\nfairness-sensitive analysis\n\nWhen NOT to Use\n\nvery small datasets\nwhen business rules require exact values\noperational counting logic\n\n\n\n\nAverage vs Median | How to Choose\n\n\n\nScenario\nPrefer\n\n\n\n\nSymmetric distribution\nAverage\n\n\nSkewed distribution\nMedian\n\n\nPresence of outliers\nMedian\n\n\nKPI reporting (business averages)\nAverage\n\n\nRobust or fairness analysis\nMedian\n\n\nModeling or downstream ML\nMedian\n\n\n\n\n\n\nKey Analytical Warning\nImputation is not neutral.\n\nit changes distributions\nit affects KPIs\nit must be documented\n\nBefore using COALESCE(), always ask:\n\nwhat does NULL represent?\nam I preserving reality or convenience?\nwill this decision influence decisions or incentives?\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nCOALESCE() gives you control over missing data\nzero imputation is a business decision, not a default\naverage imputation preserves the mean\nmedian imputation preserves robustness\nchoosing the wrong strategy leads to misleading insights\n\nNULL handling is not just a technical detail ‚Äî it is a core analytical responsibility.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 05: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session5.html#case-study-assignment-sales-performance-data-quality-analysis",
    "href": "materials/sql/session5.html#case-study-assignment-sales-performance-data-quality-analysis",
    "title": "Session 05: Data Analysis with SQL | Functions",
    "section": "Case Study Assignment | Sales Performance & Data Quality Analysis",
    "text": "Case Study Assignment | Sales Performance & Data Quality Analysis\n\nContext\nYou are working as a data analyst for an e-commerce company.\nManagement relies on the sales_analysis table for reporting, performance tracking, and decision-making.\nRecently, stakeholders raised concerns about:\n\ninconsistent KPIs across dashboards\n\nunclear handling of missing values\n\nmisleading averages in reports\n\nlack of visibility into revenue distribution\n\nYour task is to analyze, validate, and redesign key metrics using SQL numeric functions.\nYou are not allowed to modify table structures or insert/update data.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 05: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session5.html#the-task",
    "href": "materials/sql/session5.html#the-task",
    "title": "Session 05: Data Analysis with SQL | Functions",
    "section": "The Task",
    "text": "The Task\nUsing only SQL queries on the sales_analysis table, produce a single analytical result set (or a small set of related result sets) that answers the following business questions.\nYou may create intermediate queries or CTEs if needed.\n\n\nBusiness Questions to Address\n\n1. Revenue Overview\n\nWhat is the total revenue of the company?\nHow is revenue distributed across product categories?\nWhich category contributes the largest share of revenue?\n\n\n\n\n2. Typical Transaction Value\nManagement wants to understand a ‚Äútypical‚Äù transaction.\n\nCalculate the average transaction value\nCalculate the median transaction value\nBased on the data, explain which metric is more appropriate and why\n\n(Hint: consider skewness)\n\n\n\n3. NULL Impact Assessment\nDiscounts are inconsistently recorded.\n\nHow many transactions have NULL discounts?\nCalculate average discount using:\n\ndefault behavior\nzero imputation\naverage imputation\nmedian imputation\n\nExplain how each approach changes interpretation\n\n\n\n\n4. Revenue Distribution Analysis\nTo improve pricing strategy, management wants to understand revenue ranges.\n\nGroup transactions into 50-unit revenue ranges\nFor each range, compute:\n\nnumber of transactions\ntotal revenue\n\nIdentify the dominant revenue range\n\n\n\n\n5. Data Quality Check\nBefore finalizing KPIs:\n\nCheck for duplicate transaction IDs\nExplain the risk of aggregating employee salary directly from this table\nIdentify one additional potential data quality risk in sales_analysis\n\n\n\n\n\nDeliverables\nSubmit one SQL-driven analysis that includes:\n\nSQL queries (PostgreSQL syntax)\nclear column naming\nlogically structured output\nbrief written interpretation (1‚Äì2 sentences per section)\n\nYou may submit:\n\none .qmd file\nor\n\none .sql file plus a short explanation document\n\n\n\nEvaluation Criteria\nYour work will be evaluated on:\n\ncorrectness of SQL logic\nappropriate use of numeric functions\ncorrect handling of NULL values\nawareness of table grain\nclarity of analytical reasoning\nalignment with business questions\n\n\n\n\nKey Expectation\nThis is not a syntax exercise.\nYou are expected to demonstrate:\n\nanalytical judgment\nmetric design thinking\nawareness of data limitations\n\n\n\n\nFinal Note\nA correct query can still produce a wrong insight.\nYour goal is to produce results that management can trust.\nThese numeric patterns form a critical foundation for data analysis with SQL.\n\n\nPractical Rule of Thumb\n\naverage ‚âà median ‚Üí distribution is likely symmetric\naverage &gt; median ‚Üí right-skewed\naverage &lt; median ‚Üí left-skewed\n\n\n\n\n\n\n\nTip\n\n\n\nRemember to review this part from Intro to Statistics",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 05: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session5.html#case-study-assignment-sales-performance-data-quality-analysis-1",
    "href": "materials/sql/session5.html#case-study-assignment-sales-performance-data-quality-analysis-1",
    "title": "Session 05: Data Analysis with SQL | Functions",
    "section": "Case Study Assignment | Sales Performance & Data Quality Analysis",
    "text": "Case Study Assignment | Sales Performance & Data Quality Analysis\n\nContext\nYou are working as a data analyst for an e-commerce company.\nManagement relies on the sales_analysis table for reporting, performance tracking, and decision-making.\nRecently, stakeholders raised concerns about:\n\ninconsistent KPIs across dashboards\n\nunclear handling of missing values\n\nmisleading averages in reports\n\nlack of visibility into revenue distribution\n\nYour task is to analyze, validate, and redesign key metrics using SQL numeric functions.\nYou are not allowed to modify table structures or insert/update data.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 05: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#learning-goals",
    "href": "materials/sql/slides/session2.html#learning-goals",
    "title": "Intro to PostgreSQL",
    "section": "Learning Goals",
    "text": "Learning Goals\nBy the end of this session, you will be able to:\n\nunderstand how data is structured in PostgreSQL\nidentify tables, rows, and columns\nexplain different types of keys and relationships\nunderstand how indexes affect performance\nrecognize common PostgreSQL data types\nget familiarized with basic SQL syntax",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#erd-simple",
    "href": "materials/sql/slides/session2.html#erd-simple",
    "title": "Intro to PostgreSQL",
    "section": "ERD Simple",
    "text": "ERD Simple\n\n!",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#sales-analytics-database",
    "href": "materials/sql/slides/session2.html#sales-analytics-database",
    "title": "Intro to PostgreSQL",
    "section": "Sales Analytics Database",
    "text": "Sales Analytics Database\nThe database captures information about:\n\ncustomers who place orders\nproducts that are sold\nemployees involved in the sales process\norders placed over time\nindividual sales transactions",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#questions-to-data-analyst",
    "href": "materials/sql/slides/session2.html#questions-to-data-analyst",
    "title": "Intro to PostgreSQL",
    "section": "Questions to Data Analyst",
    "text": "Questions to Data Analyst\nYour role as an analyst is to query this data to answer business questions related to:\n\nrevenue and sales performance\nproduct popularity\ncustomer behavior\nemployee contribution\ntime-based trends",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#erd-complete",
    "href": "materials/sql/slides/session2.html#erd-complete",
    "title": "Intro to PostgreSQL",
    "section": "ERD Complete",
    "text": "ERD Complete\n\n!",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#keys",
    "href": "materials/sql/slides/session2.html#keys",
    "title": "Intro to PostgreSQL",
    "section": "Keys",
    "text": "Keys\n\n\n\nPrimary Key\nCandidate Key\nComposite Key\nSurrogate Key\nForeign key\n\n\n\n\n\n!",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#primary-key",
    "href": "materials/sql/slides/session2.html#primary-key",
    "title": "Intro to PostgreSQL",
    "section": "Primary Key",
    "text": "Primary Key\nA primary key uniquely identifies each record in a table.\n\n\n\n\n\n\n\n\n\n\ntransaction_id\norder_id\nproduct_id\ncustomer_id\ntotal_sales\n\n\n\n\n1001\n501\n12\n3001\n49.99\n\n\n1002\n501\n18\n3001\n19.99\n\n\n1003\n502\n12\n3005\n49.99\n\n\n1004\n503\n25\n3008\n89.99",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#candidate-key",
    "href": "materials/sql/slides/session2.html#candidate-key",
    "title": "Intro to PostgreSQL",
    "section": "Candidate Key",
    "text": "Candidate Key\n\n\n\n\n\n\n\n\n\ncustomer_id\ncustomer_name\nemail\ncity\n\n\n\n\n3001\nAlice Johnson\nalice.johnson@email.com\nBerlin\n\n\n3002\nMark Thompson\nmark.thompson@email.com \nParis\n\n\n3003\nElena Petrova\nelena.p@email.com \nMadrid\n\n\n3004\nDavid Chen\ndavid.chen@email.com \nLondon",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#composite-key",
    "href": "materials/sql/slides/session2.html#composite-key",
    "title": "Intro to PostgreSQL",
    "section": "Composite Key",
    "text": "Composite Key\n\n\n\norder_id\nproduct_id\nproduct_name\nquantity\n\n\n\n\n501\n12\nWireless Mouse\n1\n\n\n501\n18\nUSB-C Cable\n2\n\n\n502\n12\nWireless Mouse\n1\n\n\n503\n25\nMechanical Keyboard\n1\n\n\n\n\n\n\n\n\n\n\nComposite Key\n\n\n(order_id, product_id) could uniquely identify rows in some transactional systems.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#surrogate-key",
    "href": "materials/sql/slides/session2.html#surrogate-key",
    "title": "Intro to PostgreSQL",
    "section": "Surrogate Key",
    "text": "Surrogate Key\n\n\n\ntransaction_id\norder_id\nproduct_id\nquantity\ntotal_sales\n\n\n\n\n1001\n501\n12\n1\n49.99\n\n\n1002\n501\n18\n2\n19.99\n\n\n1003\n502\n12\n1\n49.99\n\n\n1004\n503\n25\n1\n89.99\n\n\n\nSurrogate Key helps:\n\nsimplify joins\nreduce index size\nimprove query readability",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#foreign-key",
    "href": "materials/sql/slides/session2.html#foreign-key",
    "title": "Intro to PostgreSQL",
    "section": "Foreign Key",
    "text": "Foreign Key\n\n!",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#indexes",
    "href": "materials/sql/slides/session2.html#indexes",
    "title": "Intro to PostgreSQL",
    "section": "Indexes",
    "text": "Indexes\n\n!",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#trade-off",
    "href": "materials/sql/slides/session2.html#trade-off",
    "title": "Intro to PostgreSQL",
    "section": "Trade-Off",
    "text": "Trade-Off\n\n\n\n\n\n\n\nFull Table Scan\n\n\nWhen a query checks every row instead of using an index, this is called a full scan.\n\n\n\n\nIndexes improve read performance but come with trade-offs:\n\nextra storage\nslower inserts and updates\nmaintenance overhead",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#when-not-to-create",
    "href": "materials/sql/slides/session2.html#when-not-to-create",
    "title": "Intro to PostgreSQL",
    "section": "When not to Create!",
    "text": "When not to Create!\n\nVery small tables\nFrequently updated columns\nColumns with many NULL values\nColumns with very low cardinality",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#single-column-index",
    "href": "materials/sql/slides/session2.html#single-column-index",
    "title": "Intro to PostgreSQL",
    "section": "Single-Column Index",
    "text": "Single-Column Index\nA single-column index is built on one column.\n\n\n\nEmployee ID\nName\nContact Number\nAge\n\n\n\n\n1\nMax\n800692692\n24\n\n\n2\nJessica\n800123456\n35\n\n\n3\nMikeal\n800745547\n49",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#composite-index",
    "href": "materials/sql/slides/session2.html#composite-index",
    "title": "Intro to PostgreSQL",
    "section": "Composite Index",
    "text": "Composite Index\nConsider employees table, but now imagine that queries often filter by both name and age at the same time.\n\n\n\\[\\rightarrow\\]\n\n\n\nemployee_id\nname\nage\ndepartment\n\n\n\n\n1\nMax\n24\nSales\n\n\n2\nJessica\n35\nMarketing\n\n\n3\nMax\n35\nFinance\n\n\n\n\nWhen to use:\n\nEfficient for queries filtering on name\nEfficient for queries filtering on name and age\nNot efficient for queries filtering on age only",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#unique-index",
    "href": "materials/sql/slides/session2.html#unique-index",
    "title": "Intro to PostgreSQL",
    "section": "Unique Index",
    "text": "Unique Index\nBasically all the candidate keys :)\n\n\n\n\n\ncustomer_id\ncustomer_name\nemail\n\n\n\n\n3001\nAlice Johnson\nalice@email.com\n\n\n3002\nMark Thompson\nmark@email.com\n\n\n3003\nElena Petrova\nelena@email.com\n\n\n\n\nUse a unique index when:\n\nvalues must be unique across rows\nthe column is frequently used for lookup\nuniqueness is part of business logic",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#answer",
    "href": "materials/sql/slides/session2.html#answer",
    "title": "Intro to PostgreSQL",
    "section": "Answer",
    "text": "Answer\n\n\n\n\n\n\n\n\n\nConcept\nEnforces Uniqueness\nImproves Query Speed\nStored as Index\n\n\n\n\nPrimary Key\nYes\nYes\nYes (unique index)\n\n\nUnique Constraint\nYes\nYes\nYes (unique index)\n\n\nForeign Key\nNo\nSometimes\nNo (by default)\n\n\nRegular Index\nNo\nYes\nYes",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#data-types-in-postgresql",
    "href": "materials/sql/slides/session2.html#data-types-in-postgresql",
    "title": "Intro to PostgreSQL",
    "section": "Data Types in PostgreSQL",
    "text": "Data Types in PostgreSQL\nData types define:\n\nwhat kind of data a column can store\nhow data is validated\nhow much storage is used\nhow fast queries can run",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#why-data-types-matter",
    "href": "materials/sql/slides/session2.html#why-data-types-matter",
    "title": "Intro to PostgreSQL",
    "section": "Why Data Types Matter?",
    "text": "Why Data Types Matter?\nChoosing the wrong data type can:\n\nbreak calculations\nreduce performance\nwaste storage\nintroduce subtle errors",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#numeric-data-types",
    "href": "materials/sql/slides/session2.html#numeric-data-types",
    "title": "Intro to PostgreSQL",
    "section": "Numeric Data Types",
    "text": "Numeric Data Types\n\n\n\nData Type\nDescription\nExample\nUse Case\n\n\n\n\nINTEGER\nWhole numbers\n2500\nCounts\n\n\nBIGINT\nLarge integers\n9876543210\nIDs\n\n\nNUMERIC(p,s)\nExact decimals\n15432.75\nRevenue\n\n\nDOUBLE PRECISION\nApproximate\n0.123456\nKPIs\n\n\n\nThe Rule of Thumb:\n\nUse INTEGER for counts\nUse NUMERIC for money\nUse DOUBLE PRECISION for ratios\n\nIndex performance depends on data type size:\n\nsmaller types \\(\\rightarrow\\) faster indexes\nlarger types \\(\\rightarrow\\) more storage",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#main-character-data-types",
    "href": "materials/sql/slides/session2.html#main-character-data-types",
    "title": "Intro to PostgreSQL",
    "section": "Main Character Data Types",
    "text": "Main Character Data Types\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nTypical Use Case\n\n\n\n\nCHAR(n)\nFixed-length character string\nCHAR(5) ‚Üí ‚ÄòUS‚Äô\nCountry codes, fixed formats\n\n\nVARCHAR(n)\nVariable-length string with limit\nVARCHAR(50) ‚Üí ‚ÄòKaren Hovhannisyan‚Äô\nNames, emails\n\n\nTEXT\nVariable-length string, no limit\n‚ÄòThis product has been discontinued.‚Äô\nDescriptions, comments\n\n\nCHARACTER VARYING(n)\nSQL-standard name for VARCHAR\nCHARACTER VARYING(20) ‚Üí ‚ÄòA123XZ‚Äô\nCodes, identifiers",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#main-date-and-time-data-types",
    "href": "materials/sql/slides/session2.html#main-date-and-time-data-types",
    "title": "Intro to PostgreSQL",
    "section": "Main Date and Time Data Types",
    "text": "Main Date and Time Data Types\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nTypical Use Case\n\n\n\n\nDATE\nCalendar date (no time)\n2025-03-15\nBirth dates, order dates\n\n\nTIME\nTime of day (no date)\n14:30:00\nOpening hours\n\n\nTIME WITH TIME ZONE\nTime with time zone info\n14:30:00+04\nCross-region schedules\n\n\nTIMESTAMP\nDate and time (no timezone)\n2025-03-15 14:30:00\nLocal event logs\n\n\nTIMESTAMP WITH TIME ZONE (TIMESTAMPTZ)\nDate and time with timezone handling\n2025-03-15 10:30:00+00\nAuditing, analytics\n\n\nINTERVAL\nTime duration\n3 days 4 hours\nSession length, SLA",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#boolean-categorical-data-types",
    "href": "materials/sql/slides/session2.html#boolean-categorical-data-types",
    "title": "Intro to PostgreSQL",
    "section": "Boolean & Categorical Data Types",
    "text": "Boolean & Categorical Data Types\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nTypical Use Case\n\n\n\n\nBOOLEAN\nLogical true / false value\nTRUE, FALSE\nActive flags, eligibility\n\n\nCHAR(n)\nFixed-length category code\nCHAR(1) ‚Üí ‚ÄòY‚Äô\nYes/No indicators\n\n\nVARCHAR(n)\nShort categorical label\n‚Äòpremium‚Äô\nCustomer segments\n\n\nTEXT\nFree-form category label\n‚Äòhigh_value_customer‚Äô\nTags, labels\n\n\nENUM\nPredefined set of values\n(‚Äòlow‚Äô,‚Äòmedium‚Äô,‚Äòhigh‚Äô)\nControlled categories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#special-data-types-optional",
    "href": "materials/sql/slides/session2.html#special-data-types-optional",
    "title": "Intro to PostgreSQL",
    "section": "Special Data Types | OPTIONAL",
    "text": "Special Data Types | OPTIONAL\n\n\n\n\n\n\n\n\n\nData Type / Extension\nDescription\nExample Value\nTypical Use Case\n\n\n\n\nJSON / JSONB\nSemi-structured JSON data\n{‚Äúplan‚Äù:‚Äúpremium‚Äù,‚Äúusage‚Äù:120}\nLogs, APIs, configs\n\n\nARRAY\nArray of values\n{1,2,3}\nTags, multi-valued attributes\n\n\nUUID\nUniversally unique identifier\n550e8400-e29b-41d4-a716-446655440000\nDistributed IDs\n\n\nINET\nIP address\n192.168.1.1\nNetwork traffic\n\n\nCIDR\nNetwork block\n192.168.0.0/24\nSubnet modeling\n\n\nGEOMETRY (PostGIS)\nGeometric objects\nPOINT(40.18 44.51)\nMaps, locations\n\n\nGEOGRAPHY (PostGIS)\nEarth-based coordinates\nPOINT(44.51 40.18)\nDistance calculations\n\n\nltree\nHierarchical tree paths\nregion.city.store\nOrganizational trees\n\n\npgRouting\nGraph/network extension\nN/A\nNetwork routing, telecom",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#postgis-geospatial-extension",
    "href": "materials/sql/slides/session2.html#postgis-geospatial-extension",
    "title": "Intro to PostgreSQL",
    "section": "PostGIS (Geospatial Extension)",
    "text": "PostGIS (Geospatial Extension)\nPostGIS adds geospatial intelligence to PostgreSQL.\nCommon capabilities:\n\nDistance calculations\nSpatial joins\nRadius searches\nArea coverage\n\n\n\n\n\n\n\nImportant\n\n\nAs a data analyst you are not expected to design PostGIS schemas or routing graphs, but you will query them.\nDocumentation you can find here",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#what-is-a-schema",
    "href": "materials/sql/slides/session2.html#what-is-a-schema",
    "title": "Intro to PostgreSQL",
    "section": "What Is a Schema?",
    "text": "What Is a Schema?\nA schema describes the structure of a database: tables, columns, keys, and relationships.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#star-schema",
    "href": "materials/sql/slides/session2.html#star-schema",
    "title": "Intro to PostgreSQL",
    "section": "Star Schema",
    "text": "Star Schema\nA star schema consists of:\n\none central fact table\nmultiple surrounding dimension tables\n\n\n\n\n\n\n\n\nImportant\n\n\nAlthough our database is transactional, its structure closely resembles a star schema, making it well-suited for analytics.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#snowflake-schema",
    "href": "materials/sql/slides/session2.html#snowflake-schema",
    "title": "Intro to PostgreSQL",
    "section": "Snowflake Schema",
    "text": "Snowflake Schema\nA snowflake schema extends the star schema by normalizing dimension tables into sub-dimensions.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#entity-relationship-diagram-erd",
    "href": "materials/sql/slides/session2.html#entity-relationship-diagram-erd",
    "title": "Intro to PostgreSQL",
    "section": "Entity Relationship Diagram (ERD)",
    "text": "Entity Relationship Diagram (ERD)\nTry to generate the ERD for the sales database using your SQL client.\nIdentify primary keys, foreign keys, and relationships.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#what-is-ddl",
    "href": "materials/sql/slides/session2.html#what-is-ddl",
    "title": "Intro to PostgreSQL",
    "section": "What Is DDL?",
    "text": "What Is DDL?\nDDL (Data Definition Language) is used to define and manage database structure.\n\noperates on database objects, not rows\ndescribes how data is stored\nforms the foundation for all queries and analysis",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#ddl-scope",
    "href": "materials/sql/slides/session2.html#ddl-scope",
    "title": "Intro to PostgreSQL",
    "section": "DDL Scope",
    "text": "DDL Scope\nDDL affects:\n\ntables\ncolumns\nconstraints\nindexes\n\nDDL decisions directly impact:\n\ndata quality\nquery performance\nscalability",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#common-ddl-statements",
    "href": "materials/sql/slides/session2.html#common-ddl-statements",
    "title": "Intro to PostgreSQL",
    "section": "Common DDL Statements",
    "text": "Common DDL Statements\nThe most common DDL commands are:\n\nCREATE\nALTER\nDROP\nTRUNCATE\n\nEach command changes the database schema or metadata.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#create-ddl-context",
    "href": "materials/sql/slides/session2.html#create-ddl-context",
    "title": "Intro to PostgreSQL",
    "section": "CREATE (DDL Context)",
    "text": "CREATE (DDL Context)\nIn DDL, CREATE is used to define new database objects.\nTypical use cases:\n\ncreating tables\ncreating indexes\ncreating schemas",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#create-example-index",
    "href": "materials/sql/slides/session2.html#create-example-index",
    "title": "Intro to PostgreSQL",
    "section": "CREATE Example | Index",
    "text": "CREATE Example | Index\nExample: create an index to improve analytical queries.\nCREATE INDEX idx_sales_product_id\nON sales (product_id);\nThis command:\n\ndoes not change data\nimproves data access performance",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#alter",
    "href": "materials/sql/slides/session2.html#alter",
    "title": "Intro to PostgreSQL",
    "section": "ALTER",
    "text": "ALTER\nALTER modifies the structure of an existing object.\nIt allows schema evolution without recreating tables.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#alter-example-constraint",
    "href": "materials/sql/slides/session2.html#alter-example-constraint",
    "title": "Intro to PostgreSQL",
    "section": "ALTER Example | Constraint",
    "text": "ALTER Example | Constraint\nExample: enforce a business rule on existing data.\nALTER TABLE products\nADD CONSTRAINT chk_products_price\nCHECK (price &gt;= 0);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#drop",
    "href": "materials/sql/slides/session2.html#drop",
    "title": "Intro to PostgreSQL",
    "section": "DROP",
    "text": "DROP\nDROP permanently removes a database object.\n\nstructure is deleted\ndata is lost\noperation is irreversible",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#drop-example",
    "href": "materials/sql/slides/session2.html#drop-example",
    "title": "Intro to PostgreSQL",
    "section": "DROP Example",
    "text": "DROP Example\nExample: remove an unused index.\nDROP INDEX idx_sales_product_id;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#truncate",
    "href": "materials/sql/slides/session2.html#truncate",
    "title": "Intro to PostgreSQL",
    "section": "TRUNCATE",
    "text": "TRUNCATE\nTRUNCATE removes all rows from a table while keeping its structure.\nCompared to DELETE:\n\nmuch faster\nno WHERE clause\nlimited rollback support",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#truncate-example",
    "href": "materials/sql/slides/session2.html#truncate-example",
    "title": "Intro to PostgreSQL",
    "section": "TRUNCATE Example",
    "text": "TRUNCATE Example\nTRUNCATE TABLE sales_staging;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#what-is-crud",
    "href": "materials/sql/slides/session2.html#what-is-crud",
    "title": "Intro to PostgreSQL",
    "section": "What Is CRUD?",
    "text": "What Is CRUD?\nCRUD represents operations used to work with data inside tables.\nCRUD stands for:\n\nCREATE\nREAD\nUPDATE\nDELETE",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#critical-terminology-clarification",
    "href": "materials/sql/slides/session2.html#critical-terminology-clarification",
    "title": "Intro to PostgreSQL",
    "section": "Critical Terminology Clarification",
    "text": "Critical Terminology Clarification\nThe word CREATE means different things depending on context.\n\nDDL CREATE \\(\\rightarrow\\) defines structure\nCRUD CREATE \\(\\rightarrow\\) inserts data\n\nConfusing these is a common beginner mistake.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#ddl-vs-crud-mental-model",
    "href": "materials/sql/slides/session2.html#ddl-vs-crud-mental-model",
    "title": "Intro to PostgreSQL",
    "section": "DDL vs CRUD | Mental Model",
    "text": "DDL vs CRUD | Mental Model\n\nDDL \\(\\rightarrow\\) defines the rules\nCRUD \\(\\rightarrow\\) follows the rules\nDDL \\(\\rightarrow\\) structure\nCRUD \\(\\rightarrow\\) data",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#create-crud-rightarrow-insert",
    "href": "materials/sql/slides/session2.html#create-crud-rightarrow-insert",
    "title": "Intro to PostgreSQL",
    "section": "CREATE (CRUD \\(\\rightarrow\\) INSERT)",
    "text": "CREATE (CRUD \\(\\rightarrow\\) INSERT)\nIn CRUD, CREATE means adding new rows.\nThis is done using INSERT.\nINSERT INTO products (product_id, product_name, price, category)\nVALUES (101, 'Wireless Mouse', 24.99, 'Accessories');",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#read-select",
    "href": "materials/sql/slides/session2.html#read-select",
    "title": "Intro to PostgreSQL",
    "section": "READ (SELECT)",
    "text": "READ (SELECT)\nREAD retrieves data and is implemented using SELECT.\nSELECT\n  product_id,\n  product_name\nFROM products;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#read-with-filtering",
    "href": "materials/sql/slides/session2.html#read-with-filtering",
    "title": "Intro to PostgreSQL",
    "section": "READ with Filtering",
    "text": "READ with Filtering\nFiltering rows is done using WHERE.\nSELECT\n  *\nFROM sales\nWHERE total_sales &lt; 50;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#update",
    "href": "materials/sql/slides/session2.html#update",
    "title": "Intro to PostgreSQL",
    "section": "UPDATE",
    "text": "UPDATE\nUPDATE modifies existing records.\nAlways use WHERE to target specific rows.\nUPDATE products\nSET price = 49.99\nWHERE product_id = 12;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#delete",
    "href": "materials/sql/slides/session2.html#delete",
    "title": "Intro to PostgreSQL",
    "section": "DELETE",
    "text": "DELETE\nDELETE removes records from a table.\nDeletions are irreversible and must be used with caution.\nDELETE FROM sales\nWHERE transaction_id = 1004;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#why-constraints-matter",
    "href": "materials/sql/slides/session2.html#why-constraints-matter",
    "title": "Intro to PostgreSQL",
    "section": "Why Constraints Matter",
    "text": "Why Constraints Matter\nConstraints play a crucial role in keeping your data organized, consistent, and reliable.\nThey define what type of data a table or column can accept and are typically applied when tables are created or later via ALTER TABLE.\nWhen defined correctly, constraints:\nenforce data integrity\nprevent invalid or inconsistent data\nact as built-in data quality checks",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#types-of-constraints",
    "href": "materials/sql/slides/session2.html#types-of-constraints",
    "title": "Intro to PostgreSQL",
    "section": "Types of Constraints",
    "text": "Types of Constraints\nThe most common constraints you will encounter are:\nUNIQUE\nNOT NULL\nPRIMARY KEY\nFOREIGN KEY\nCHECK\nEach constraint enforces a specific business or technical rule.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#unique-constraint",
    "href": "materials/sql/slides/session2.html#unique-constraint",
    "title": "Intro to PostgreSQL",
    "section": "UNIQUE Constraint",
    "text": "UNIQUE Constraint\nThe UNIQUE constraint ensures that all values in a column are distinct.\nIt is commonly used for attributes that must be unique across entities.\nTypical use cases include:\nemail addresses\nusernames\nnational identification numbers",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#unique-constraint-example",
    "href": "materials/sql/slides/session2.html#unique-constraint-example",
    "title": "Intro to PostgreSQL",
    "section": "UNIQUE Constraint | Example",
    "text": "UNIQUE Constraint | Example\nEnsure each customer email is unique.\nCREATE TABLE customers (\n  customer_id INTEGER PRIMARY KEY,\n  email TEXT UNIQUE,\n  phone_number TEXT\n);\nWith this constraint in place, PostgreSQL will reject any attempt to insert a duplicate email address.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#not-null-constraint",
    "href": "materials/sql/slides/session2.html#not-null-constraint",
    "title": "Intro to PostgreSQL",
    "section": "NOT NULL Constraint",
    "text": "NOT NULL Constraint\nThe NOT NULL constraint ensures that a column cannot contain NULL values.\nUse this constraint for fields that are mandatory and must always be provided.\nTypical use cases include:\nprimary identifiers\ncontact information\ntransaction timestamps",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#not-null-constraint-example",
    "href": "materials/sql/slides/session2.html#not-null-constraint-example",
    "title": "Intro to PostgreSQL",
    "section": "NOT NULL Constraint | Example",
    "text": "NOT NULL Constraint | Example\nEnsure every customer has a phone number.\nCREATE TABLE customers (\n  customer_id INTEGER PRIMARY KEY,\n  phone_number TEXT NOT NULL,\n  email TEXT\n);\nIf an insert is attempted without a phone number, PostgreSQL will return an error.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#primary-key-constraint",
    "href": "materials/sql/slides/session2.html#primary-key-constraint",
    "title": "Intro to PostgreSQL",
    "section": "PRIMARY KEY Constraint",
    "text": "PRIMARY KEY Constraint\nA PRIMARY KEY uniquely identifies each row in a table.\nA primary key:\nmust be unique\ncannot contain NULL values\nexists only once per table",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#primary-key-constraint-example",
    "href": "materials/sql/slides/session2.html#primary-key-constraint-example",
    "title": "Intro to PostgreSQL",
    "section": "PRIMARY KEY Constraint | Example",
    "text": "PRIMARY KEY Constraint | Example\nDefine a primary key for a products table.\nCREATE TABLE products (\n  product_id INTEGER PRIMARY KEY,\n  product_name TEXT NOT NULL,\n  price NUMERIC(10, 2)\n);\nThe PRIMARY KEY constraint automatically enforces both UNIQUE and NOT NULL.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#foreign-key-constraint",
    "href": "materials/sql/slides/session2.html#foreign-key-constraint",
    "title": "Intro to PostgreSQL",
    "section": "FOREIGN KEY Constraint",
    "text": "FOREIGN KEY Constraint\nA FOREIGN KEY creates a relationship between two tables.\nIt ensures referential integrity, meaning referenced values must exist in the parent table.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#foreign-key-constraint-example",
    "href": "materials/sql/slides/session2.html#foreign-key-constraint-example",
    "title": "Intro to PostgreSQL",
    "section": "FOREIGN KEY Constraint | Example",
    "text": "FOREIGN KEY Constraint | Example\nLink sales records to customers.\nCREATE TABLE sales (\n  transaction_id INTEGER PRIMARY KEY,\n  customer_id INTEGER,\n  total_sales NUMERIC(10, 2),\n  FOREIGN KEY (customer_id)\n    REFERENCES customers (customer_id)\n);\nWith this constraint, you cannot insert a sale for a customer that does not exist.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#check-constraint",
    "href": "materials/sql/slides/session2.html#check-constraint",
    "title": "Intro to PostgreSQL",
    "section": "CHECK Constraint",
    "text": "CHECK Constraint\nThe CHECK constraint restricts the range or condition of values that can be inserted into a column.\nIt validates data using logical expressions.\nTypical use cases include:\nvalue ranges\nminimum or maximum limits\ndomain rules",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#check-constraint-example",
    "href": "materials/sql/slides/session2.html#check-constraint-example",
    "title": "Intro to PostgreSQL",
    "section": "CHECK Constraint | Example",
    "text": "CHECK Constraint | Example\nEnsure total sales values are non-negative.\nCREATE TABLE sales (\n  transaction_id INTEGER PRIMARY KEY,\n  total_sales NUMERIC(10, 2) CHECK (total_sales &gt;= 0)\n);\nIf a value violates the condition, the insert or update will fail.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#why-best-practices-matter",
    "href": "materials/sql/slides/session2.html#why-best-practices-matter",
    "title": "Intro to PostgreSQL",
    "section": "Why Best Practices Matter",
    "text": "Why Best Practices Matter\nWriting SQL that works is not enough.\nGood SQL should be:\nreadable\nconsistent\nmaintainable\neasy to debug\nThese rules help teams collaborate and reduce errors.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#numbers-and-underscores",
    "href": "materials/sql/slides/session2.html#numbers-and-underscores",
    "title": "Intro to PostgreSQL",
    "section": "Numbers and Underscores",
    "text": "Numbers and Underscores\nIn relational databases:\ntable names cannot start with a number\ncolumn names cannot contain spaces\nunderscores should be used to separate words\nThis improves compatibility and readability.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#naming-examples",
    "href": "materials/sql/slides/session2.html#naming-examples",
    "title": "Intro to PostgreSQL",
    "section": "Naming | Examples",
    "text": "Naming | Examples\nRecommended:\nCREATE TABLE customer_lifecycle;\nCREATE TABLE customer;\nNot recommended:\nCREATE TABLE customer lifecycle;\nCREATE TABLE 3customer;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#capitalization",
    "href": "materials/sql/slides/session2.html#capitalization",
    "title": "Intro to PostgreSQL",
    "section": "Capitalization",
    "text": "Capitalization\nA widely accepted convention is:\nSQL keywords \\(\\rightarrow\\) UPPERCASE\ntable names \\(\\rightarrow\\) lowercase\ncolumn names \\(\\rightarrow\\) lowercase\nThis improves readability and consistency.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#capitalization-examples",
    "href": "materials/sql/slides/session2.html#capitalization-examples",
    "title": "Intro to PostgreSQL",
    "section": "Capitalization | Examples",
    "text": "Capitalization | Examples\nRecommended:\nSELECT product_id, product_name\nFROM products;\nSELECT transaction_id, total_sales\nFROM sales;\nNot recommended:\nSelect PRODUCT_ID, PRODUCT_NAME From PRODUCTS;\nSELECT TRANSACTION_ID, TOTAL_SALES FROM SALES;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#commenting-sql-code",
    "href": "materials/sql/slides/session2.html#commenting-sql-code",
    "title": "Intro to PostgreSQL",
    "section": "Commenting SQL Code",
    "text": "Commenting SQL Code\nCommenting helps others (and your future self) understand your logic.\nComments:\ndo not affect execution\nimprove documentation\nhelp during debugging",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#commenting-examples",
    "href": "materials/sql/slides/session2.html#commenting-examples",
    "title": "Intro to PostgreSQL",
    "section": "Commenting | Examples",
    "text": "Commenting | Examples\nSingle-line comments:\n-- select and display the first 10 rows from the sales table\nSELECT *\nFROM sales\nLIMIT 10;\nMulti-line comments:\n/*\nSELECT\n  s.transaction_id,\n  p.product_name,\n  s.total_sales\nFROM sales s\nJOIN products p\n  ON s.product_id = p.product_id\nWHERE s.total_sales &gt; 100;\n*/",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#aliasing",
    "href": "materials/sql/slides/session2.html#aliasing",
    "title": "Intro to PostgreSQL",
    "section": "Aliasing",
    "text": "Aliasing\nAliases assign temporary names to tables or columns.\nThey are useful for:\nimproving readability\nshortening long names\nresolving ambiguity in joins",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#aliasing-examples",
    "href": "materials/sql/slides/session2.html#aliasing-examples",
    "title": "Intro to PostgreSQL",
    "section": "Aliasing | Examples",
    "text": "Aliasing | Examples\nColumn aliases:\nSELECT\n  product_name AS item_name,\n  price AS unit_price\nFROM products;\nTable aliases in joins:\nSELECT\n  s.transaction_id,\n  p.product_name,\n  s.total_sales\nFROM sales AS s\nJOIN products AS p\n  ON s.product_id = p.product_id;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session2.html#key-takeaways",
    "href": "materials/sql/slides/session2.html#key-takeaways",
    "title": "Intro to PostgreSQL",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nconstraints protect data quality\nbest practices improve readability\nconsistent SQL scales better in teams\nclean SQL enables better optimization",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#dates-as-stories",
    "href": "materials/sql/slides/session7.html#dates-as-stories",
    "title": "Date Functions",
    "section": "Dates as Stories",
    "text": "Dates as Stories\nAlmost every real business question is, at its core, a story over time.\n\nWhat happened before?\nWhat changed after?\nHow fast did something grow?\nWhen did behavior shift?\nHow often did something was happenng?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#dates-create-analytical-structure",
    "href": "materials/sql/slides/session7.html#dates-create-analytical-structure",
    "title": "Date Functions",
    "section": "Dates Create Analytical Structure",
    "text": "Dates Create Analytical Structure\nA single timestamp can represent many analytical dimensions at once.\n\nday\n\nweek\n\nmonth\n\nquarter\n\nyear\n\nweekday vs weekend\n\n\nWithout explicitly extracting and structuring these dimensions:\n\ntrends disappear\n\nseasonality remains hidden\n\ncomparisons become invalid\n\n\n\nDates turn events into patterns but only if we ask the right questions.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#the-story-we-will-follow",
    "href": "materials/sql/slides/session7.html#the-story-we-will-follow",
    "title": "Date Functions",
    "section": "The Story We Will Follow",
    "text": "The Story We Will Follow\nThroughout this session, we will use the sales_analysis table created earlier.\nEach row represents:\n\none transaction\n\noccurring at a specific point in time\n\nOur goal is to answer progressively deeper analytical questions:\n\nWhen did something happen?\n\nHow long did it take?\n\nHow many events happened in a period?\n\nHow does time aggregation change interpretation?\n\nEach new date function will add structure to this story.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#date-vs-timestamp-the-first-fork-in-the-road",
    "href": "materials/sql/slides/session7.html#date-vs-timestamp-the-first-fork-in-the-road",
    "title": "Date Functions",
    "section": "Date vs Timestamp | The First Fork in the Road",
    "text": "Date vs Timestamp | The First Fork in the Road\nBefore using any date function, we must answer a fundamental question:\n\nWhat kind of time do we have?\n\nDate:\n\nRepresents a calendar day only.\nno time of day\n\nno ordering within the day\n\nexample: 2024-05-12\n\n\nTIMESTAMP: Represents date + time.\n\nincludes hours, minutes, seconds\n\npreserves event sequence\n\nexample: 2024-05-12 14:37:22",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#why-this-distinction-matters",
    "href": "materials/sql/slides/session7.html#why-this-distinction-matters",
    "title": "Date Functions",
    "section": "Why This Distinction Matters",
    "text": "Why This Distinction Matters\n\ngrouping behaves differently\n\ncomparisons behave differently\n\ntruncation behaves differently\n\nMost analytical mistakes with dates begin right here.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#extract-1",
    "href": "materials/sql/slides/session7.html#extract-1",
    "title": "Date Functions",
    "section": "EXTRACT()",
    "text": "EXTRACT()\nDates contain multiple dimensions inside a single value \\(\\rightarrow\\) Pulling Meaning from Dates\n\nyear\nquarter\nmonths\ndate\nweekday\n\nEXTRACT() allows us to isolate these components.\nEXTRACT(field FROM date)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example",
    "href": "materials/sql/slides/session7.html#example",
    "title": "Date Functions",
    "section": "Example",
    "text": "Example\nSELECT\n  order_date_date,\n  EXTRACT(YEAR FROM order_date_date)  AS year,\n  EXTRACT(MONTH FROM order_date_date) AS month,\n  EXTRACT(DAY FROM order_date_date)   AS day, \n  EXTRACT(DOW FROM some_date) as weekday\nFROM sales_analysis\nLIMIT 5\n\\[\\downarrow\\]\n\n\n\norder_date_date\nyear\nmonth\nday\nweekday\n\n\n\n\n2021-02-11\n2021\n2\n11\n4\n\n\n2022-12-10\n2022\n12\n10\n6\n\n\n2021-02-22\n2021\n2\n22\n1\n\n\n2022-07-12\n2022\n7\n12\n2\n\n\n2021-04-19\n2021\n4\n19\n1",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#date_part",
    "href": "materials/sql/slides/session7.html#date_part",
    "title": "Date Functions",
    "section": "DATE_PART()",
    "text": "DATE_PART()\nDATE_PART() is functionally equivalent to EXTRACT().\nDATE_PART('month', order_date_date)\nBoth return numeric components of a date.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#when-to-prefer-which",
    "href": "materials/sql/slides/session7.html#when-to-prefer-which",
    "title": "Date Functions",
    "section": "When to Prefer Which",
    "text": "When to Prefer Which\n\nEXTRACT() ‚Üí ANSI-standard, portable SQL\n\nDATE_PART() ‚Üí PostgreSQL-native, readable\n\nIn this course, we prefer EXTRACT() for consistency.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#date_trunc-1",
    "href": "materials/sql/slides/session7.html#date_trunc-1",
    "title": "Date Functions",
    "section": "DATE_TRUNC()",
    "text": "DATE_TRUNC()\nIn analytics, grain matters.\nThe same data can tell very different stories depending on how time is grouped.\nDATE_TRUNC('unit', timestamp)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-1",
    "href": "materials/sql/slides/session7.html#example-1",
    "title": "Date Functions",
    "section": "Example 1",
    "text": "Example 1\nSELECT\n  DATE_TRUNC('month', order_date_date) AS month,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('month', order_date_date)\nORDER BY month;\n\\[\\downarrow\\]\n\n\n\nmonth\ntotal_revenue\n\n\n\n\n2020-01-01 00:00:00+00\n38621.59\n\n\n2020-02-01 00:00:00+00\n30460.16\n\n\n2020-04-01 00:00:00+00\n18096.39\n\n\n2020-05-01 00:00:00+00\n23722.28\n\n\n2020-06-01 00:00:00+00\n42599.07",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-2",
    "href": "materials/sql/slides/session7.html#example-2",
    "title": "Date Functions",
    "section": "Example 2",
    "text": "Example 2\nSELECT\n  DATE_TRUNC('month', order_date_date) AS month,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('month', order_date_date)\nORDER BY SUM(total_sales) DESC;\n\\[\\downarrow\\]\n\n\n\nmonth\ntotal_revenue\n\n\n\n\n2022-12-01 00:00:00+00\n52510.67\n\n\n2022-07-01 00:00:00+00\n49011.10\n\n\n2023-12-01 00:00:00+00\n46700.47\n\n\n2021-09-01 00:00:00+00\n46433.23\n\n\n2020-06-01 00:00:00+00\n42599.07",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-3",
    "href": "materials/sql/slides/session7.html#example-3",
    "title": "Date Functions",
    "section": "Example 3",
    "text": "Example 3\nSELECT\n  DATE_TRUNC('quarter', order_date_date) AS quarter,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('quarter', order_date_date)\nORDER BY quarter;\n\\[\\downarrow\\]\n\n\n\n\nquarter\ntotal_revenue\n\n\n\n\n2020-01-01 00:00:00+00\n69081.75\n\n\n2020-04-01 00:00:00+00\n84417.74\n\n\n2020-07-01 00:00:00+00\n62003.53\n\n\n2020-10-01 00:00:00+00\n69127.16\n\n\n2021-01-01 00:00:00+00\n74184.68\n\n\n2021-04-01 00:00:00+00\n68431.42\n\n\n2021-07-01 00:00:00+00\n84194.98\n\n\n2021-10-01 00:00:00+00\n63543.31",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-4",
    "href": "materials/sql/slides/session7.html#example-4",
    "title": "Date Functions",
    "section": "Example 4",
    "text": "Example 4\nSELECT\n  DATE_TRUNC('year', order_date_date) AS year,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('year', order_date_date)\nORDER BY year;\n\\[\\downarrow\\]\n\n\n\n\nyear\ntotal_revenue\n\n\n\n\n2020-01-01 00:00:00+00\n284630.18\n\n\n2021-01-01 00:00:00+00\n290354.39\n\n\n2022-01-01 00:00:00+00\n345714.03\n\n\n2023-01-01 00:00:00+00\n350319.25",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#analytical-best-practices",
    "href": "materials/sql/slides/session7.html#analytical-best-practices",
    "title": "Date Functions",
    "section": "Analytical Best Practices",
    "text": "Analytical Best Practices\n\nalways justify your chosen time grain\n\nnever mix different grains in the same comparison\n\nre-aggregate data before drawing conclusions\n\nvalidate insights by switching grains\n\nDATE_TRUNC() does not change the data.\nIt changes how you see the data.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#current_date-now",
    "href": "materials/sql/slides/session7.html#current_date-now",
    "title": "Date Functions",
    "section": "CURRENT_DATE, NOW()",
    "text": "CURRENT_DATE, NOW()\nIn analytics, we often need a reference point called now.\nThis allows us to answer questions such as:\n\nhow recent is this transaction?\n\nhow many days ago did something happen?\n\nis this record outdated?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#now",
    "href": "materials/sql/slides/session7.html#now",
    "title": "Date Functions",
    "section": "NOW()",
    "text": "NOW()\nIf you try:\nSELECT NOW();\n\ntype: TIMESTAMP WITH TIME ZONE\n\nincludes hours, minutes, seconds\n\nideal for sequence-sensitive logic",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#current_date",
    "href": "materials/sql/slides/session7.html#current_date",
    "title": "Date Functions",
    "section": "CURRENT_DATE",
    "text": "CURRENT_DATE\nIf you try:\nSELECT NOW();\n\ntype: DATE\n\nchanges once per day\n\nideal for day-level comparisons",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#measuring-time-gaps-date-arithmetic",
    "href": "materials/sql/slides/session7.html#measuring-time-gaps-date-arithmetic",
    "title": "Date Functions",
    "section": "Measuring Time Gaps | Date Arithmetic",
    "text": "Measuring Time Gaps | Date Arithmetic\nDates become analytical only when we compare them.\nPostgreSQL allows direct subtraction between dates.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-days-since-order",
    "href": "materials/sql/slides/session7.html#example-days-since-order",
    "title": "Date Functions",
    "section": "Example | Days Since Order",
    "text": "Example | Days Since Order\nSELECT\n  order_date_date,\n  CURRENT_DATE - order_date_date AS days_since_order\nFROM sales_analysis\nLIMIT 5;\n\n\\[\\downarrow\\]\n\n\n\norder_date_date\ndays_since_order\n\n\n\n\n2021-02-11\n1797\n\n\n2022-12-10\n1130\n\n\n2021-02-22\n1786\n\n\n2022-07-12\n1281\n\n\n2021-04-19\n1730",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#months-between-dates-naive-approach",
    "href": "materials/sql/slides/session7.html#months-between-dates-naive-approach",
    "title": "Date Functions",
    "section": "Months Between Dates | Naive Approach",
    "text": "Months Between Dates | Naive Approach\nSELECT\n  order_date_date,\n  (CURRENT_DATE - order_date_date) / 30 AS months_estimate\nFROM sales_analysis\nLIMIT 5;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#months-between-dates-approximation",
    "href": "materials/sql/slides/session7.html#months-between-dates-approximation",
    "title": "Date Functions",
    "section": "Months Between Dates | Approximation",
    "text": "Months Between Dates | Approximation\nSELECT\n  (CURRENT_DATE - order_date_date) / 30.4375 AS approx_months\nFROM sales_analysis\nLIMIT 5;\n\n\\[\n30.4375 = \\frac{365.25}{12}\n\\]",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#months-between-dates-calendar-safe",
    "href": "materials/sql/slides/session7.html#months-between-dates-calendar-safe",
    "title": "Date Functions",
    "section": "Months Between Dates | Calendar-Safe",
    "text": "Months Between Dates | Calendar-Safe\nSELECT\n  order_date_date,\n    (DATE_PART('year', CURRENT_DATE) - DATE_PART('year', order_date_date)) * 12\n  + (DATE_PART('month', CURRENT_DATE) - DATE_PART('month', order_date_date))\n  - CASE\n      WHEN DATE_PART('day', CURRENT_DATE)\n        &lt; DATE_PART('day', order_date_date)\n      THEN 1 ELSE 0\n  END AS full_months\nFROM sales_analysis\nORDER BY order_date_date DESC\nLIMIT 10;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#interval-expressing-duration",
    "href": "materials/sql/slides/session7.html#interval-expressing-duration",
    "title": "Date Functions",
    "section": "INTERVAL | Expressing Duration",
    "text": "INTERVAL | Expressing Duration\nAn INTERVAL represents a span of time, not a point.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-orders-in-last-30-days",
    "href": "materials/sql/slides/session7.html#example-orders-in-last-30-days",
    "title": "Date Functions",
    "section": "Example | Orders in Last 30 Days",
    "text": "Example | Orders in Last 30 Days\nSELECT\n  COUNT(*) AS recent_orders\nFROM sales_analysis\nWHERE order_date_date &gt;= CURRENT_DATE - INTERVAL '30 days';",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#common-interval-units",
    "href": "materials/sql/slides/session7.html#common-interval-units",
    "title": "Date Functions",
    "section": "Common INTERVAL Units",
    "text": "Common INTERVAL Units\n\nINTERVAL '7 days'\nINTERVAL '1 month'\nINTERVAL '1 year'",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#date-interval-shifting-time",
    "href": "materials/sql/slides/session7.html#date-interval-shifting-time",
    "title": "Date Functions",
    "section": "Date + INTERVAL | Shifting Time",
    "text": "Date + INTERVAL | Shifting Time\nsome_date + INTERVAL '2 days'\nsome_date - INTERVAL '3 months'\nsome_date + INTERVAL '5 years'",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-simulated-future-date",
    "href": "materials/sql/slides/session7.html#example-simulated-future-date",
    "title": "Date Functions",
    "section": "Example | Simulated Future Date",
    "text": "Example | Simulated Future Date\nSELECT\n  order_date_date,\n  order_date_date + INTERVAL '14 days' AS follow_up_date\nFROM sales_analysis\nLIMIT 5;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#age-calendar-aware-difference",
    "href": "materials/sql/slides/session7.html#age-calendar-aware-difference",
    "title": "Date Functions",
    "section": "AGE() | Calendar-Aware Difference",
    "text": "AGE() | Calendar-Aware Difference\nAGE() returns an INTERVAL representing true calendar distance.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#example-age-output",
    "href": "materials/sql/slides/session7.html#example-age-output",
    "title": "Date Functions",
    "section": "Example | AGE Output",
    "text": "Example | AGE Output\nSELECT\n  order_date_date,\n  AGE(CURRENT_DATE, order_date_date) AS order_age\nFROM sales_analysis\nLIMIT 5;\n\n\\[\\downarrow\\]\n\n\n\norder_date_date\norder_age\n\n\n\n\n2021-02-11\n4 years 11 mons 2 days\n\n\n2022-12-10\n3 years 1 mon 3 days\n\n\n2021-02-22\n4 years 10 mons 19 days",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session7.html#age-extracting-components",
    "href": "materials/sql/slides/session7.html#age-extracting-components",
    "title": "Date Functions",
    "section": "AGE() | Extracting Components",
    "text": "AGE() | Extracting Components\nSELECT\n  order_date_date,\n  EXTRACT(YEAR FROM AGE(CURRENT_DATE, order_date_date))  AS years,\n  EXTRACT(MONTH FROM AGE(CURRENT_DATE, order_date_date)) AS months,\n  EXTRACT(DAY FROM AGE(CURRENT_DATE, order_date_date))   AS days\nFROM sales_analysis\nLIMIT 5;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Date Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#run-docker",
    "href": "materials/sql/slides/session6.html#run-docker",
    "title": "String Functions",
    "section": "Run Docker",
    "text": "Run Docker\nOpen Docker Desktop, Run and open Pgadmin:\ndocker compose up -d",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#create-sales_analysis",
    "href": "materials/sql/slides/session6.html#create-sales_analysis",
    "title": "String Functions",
    "section": "Create sales_analysis",
    "text": "Create sales_analysis\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#indexes-for-filtering-performance",
    "href": "materials/sql/slides/session6.html#indexes-for-filtering-performance",
    "title": "String Functions",
    "section": "Indexes for Filtering Performance",
    "text": "Indexes for Filtering Performance\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#text-functions-analytical-reality",
    "href": "materials/sql/slides/session6.html#text-functions-analytical-reality",
    "title": "String Functions",
    "section": "Text Functions | Analytical Reality",
    "text": "Text Functions | Analytical Reality\nReal-world text data is rarely clean:\n\ninconsistent casing\nextra spaces\nannotations and symbols\nmixed formats\npartial or malformed values\n\nIf left untreated:\n\n\nGROUP BY fragments categories\nCOUNT(DISTINCT ...) overcounts\njoins silently fail\nKPIs drift across dashboards",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#workflow",
    "href": "materials/sql/slides/session6.html#workflow",
    "title": "String Functions",
    "section": "Workflow",
    "text": "Workflow\nCorrect analytical order:\n\ninspect\n\nmeasure\n\nclassify\n\nclean\n\nvalidate\n\naggregate\n\nWe always start with measurement, not transformation.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#raw-phone-numbers",
    "href": "materials/sql/slides/session6.html#raw-phone-numbers",
    "title": "String Functions",
    "section": "Raw Phone Numbers",
    "text": "Raw Phone Numbers\nWe use a controlled dummy table with deliberately inconsistent phone formats.\nDROP TABLE IF EXISTS customers_raw_text;\n\nCREATE TABLE customers_raw_text (\n  customer_id   INTEGER,\n  first_name    TEXT,\n  last_name     TEXT,\n  raw_phone     TEXT,\n  category_raw  TEXT,\n  birth_date    DATE\n);\n\nINSERT INTO customers_raw_text (\n  customer_id,\n  first_name,\n  last_name,\n  raw_phone,\n  category_raw,\n  birth_date\n) VALUES\n  (1, 'joHN',     'doE',        '   077600945  ',   'Accessories (Promo)', DATE '1994-03-12'),\n  (2, 'MARY',     'sMiTh',      '077-600-045',      'Electronics (Old)',   DATE '1988-11-05'),\n  (3, 'aLEx',     'johnSON',    '(374)-77-600-945', 'Accessories',         DATE '2001-07-23'),\n  (4, 'anna',     'VAN DYKE',   '37477600945',      'Electronics (Promo)', DATE '1999-01-30'),\n  (5, NULL,       'brOwn',      '77600945',         'Accessories (Test)',  DATE '1994-03-12');",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#target-standard",
    "href": "materials/sql/slides/session6.html#target-standard",
    "title": "String Functions",
    "section": "Target Standard",
    "text": "Target Standard\nOur target standardized phone number is: 77600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#what-it-does",
    "href": "materials/sql/slides/session6.html#what-it-does",
    "title": "String Functions",
    "section": "What It Does",
    "text": "What It Does\nLENGTH() returns the number of characters in a text value.\nIt is a diagnostic function, not a cleaning function.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-question",
    "href": "materials/sql/slides/session6.html#analytical-question",
    "title": "String Functions",
    "section": "Analytical Question",
    "text": "Analytical Question\nBefore cleaning, we ask:\n\nDo all values have the same length?\n\n\nIf not, they cannot be directly comparable.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#inspecting-the-data",
    "href": "materials/sql/slides/session6.html#inspecting-the-data",
    "title": "String Functions",
    "section": "Inspecting the Data",
    "text": "Inspecting the Data\nSELECT\n  raw_phone,\n  LENGTH(raw_phone) AS phone_length\nFROM customers_raw_text;\n\\[\\downarrow\\]\n\n\n\n\nraw_phone\nphone_length\n\n\n\n\n'   077600945  '\n13\n\n\n'077600945'\n9\n\n\n'77600945'\n8\n\n\n'077-600-045'\n11\n\n\n'(374)-77-600-945'\n15\n\n\n'37477600945'\n11",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#interpretation",
    "href": "materials/sql/slides/session6.html#interpretation",
    "title": "String Functions",
    "section": "Interpretation",
    "text": "Interpretation\nEven without cleaning, patterns emerge:\n\n8 characters ‚Üí already standardized\n\n9 characters ‚Üí leading zero\n\n11 characters ‚Üí country code or separators\n\n13+ characters ‚Üí whitespace and symbols",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#what-we-just-learned",
    "href": "materials/sql/slides/session6.html#what-we-just-learned",
    "title": "String Functions",
    "section": "What We Just Learned",
    "text": "What We Just Learned\nWithout modifying data, we detected:\n\nwhitespace issues\nformatting symbols\ncountry prefixes\nmultiple structural patterns\n\nThis prevents blind transformations later.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-this-is-critical",
    "href": "materials/sql/slides/session6.html#why-this-is-critical",
    "title": "String Functions",
    "section": "Why This Is Critical",
    "text": "Why This Is Critical\nUsing LENGTH() early allows you to:\n\ndetect malformed rows\nclassify data quality patterns\ndesign targeted cleaning rules\navoid one-size-fits-all logic",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-insight",
    "href": "materials/sql/slides/session6.html#analytical-insight",
    "title": "String Functions",
    "section": "Analytical Insight",
    "text": "Analytical Insight\n\nthe target 77600945 has a known expected length\nthis becomes a post-cleaning validation rule\nany row failing this rule is still dirty\n\nCASE WHEN LENGTH(cleaned_phone_number) = 8 THEN 1 ELSE 0 END as flag\n\nCASE WHEN LENGTH(cleaned_phone_number) = 8 THEN 'Accept' ELSE 'Reject' END as flag",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-trim-matters",
    "href": "materials/sql/slides/session6.html#why-trim-matters",
    "title": "String Functions",
    "section": "Why TRIM() Matters",
    "text": "Why TRIM() Matters\nWhitespace issues are:\n\nvisually invisible\ncommon in manual or legacy inputs\na frequent cause of failed joins and false duplicates\n\nYet they are often overlooked in analysis.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-principle",
    "href": "materials/sql/slides/session6.html#analytical-principle",
    "title": "String Functions",
    "section": "Analytical Principle",
    "text": "Analytical Principle\nWhitespace has no business meaning.\n\nremoving it does not change semantics\nbut significantly improves comparability\n\nThis makes TRIM() a safe first transformation.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#applying-trim",
    "href": "materials/sql/slides/session6.html#applying-trim",
    "title": "String Functions",
    "section": "Applying TRIM",
    "text": "Applying TRIM\nSELECT\n  raw_phone,\n  LENGTH(raw_phone) AS length,\n  TRIM(raw_phone) AS trimmed_phone,\n  LENGTH(TRIM(raw_phone)) AS trimmed_length\nFROM customers_raw_text;\n\\[\\downarrow\\]\n\n\n\n\n\n\n\n\n\nraw_phone\nlength\ntrimmed_phone\ntrimmed_length\n\n\n\n\n'   077600945  '\n13\n'077600945'\n9\n\n\n'077600945'\n9\n'077600945'\n9\n\n\n'77600945'\n8\n'77600945'\n8\n\n\n'077-600-045'\n11\n'077-600-045'\n11\n\n\n'(374)-77-600-945'\n15\n'(374)-77-600-945'\n15\n\n\n'37477600945'\n11\n'37477600945'\n11",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#ltrim-vs-rtrim",
    "href": "materials/sql/slides/session6.html#ltrim-vs-rtrim",
    "title": "String Functions",
    "section": "LTRIM vs RTRIM",
    "text": "LTRIM vs RTRIM\nSQL also provides directional variants:\nLTRIM(raw_phone)   -- removes leading spaces | only LEFT\nRTRIM(raw_phone)   -- removes trailing spaces | ONLY right\nTRIM() is equivalent to applying both.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-insight-1",
    "href": "materials/sql/slides/session6.html#analytical-insight-1",
    "title": "String Functions",
    "section": "Analytical Insight",
    "text": "Analytical Insight\nTRIM() helps us decide:\n\nwhich rows were only cosmetically broken\nwhich rows require structural cleaning\nhow many patterns remain\n\nMeasurement ‚Üí transformation ‚Üí re-measurement",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#try-yourself",
    "href": "materials/sql/slides/session6.html#try-yourself",
    "title": "String Functions",
    "section": "Try Yourself",
    "text": "Try Yourself\n\nApply LTRIM(raw_phone)\nApply RTRIM(raw_number)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-case-normalization-matters",
    "href": "materials/sql/slides/session6.html#why-case-normalization-matters",
    "title": "String Functions",
    "section": "Why Case Normalization Matters",
    "text": "Why Case Normalization Matters\nText values often differ only by capitalization, even though they represent the same entity.\n\nFrom an analytical perspective:\n\ncapitalization has no business meaning\nSQL treats differently cased strings as different values\nthis leads to fragmented groups and incorrect counts",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#case-normalization-the-problem",
    "href": "materials/sql/slides/session6.html#case-normalization-the-problem",
    "title": "String Functions",
    "section": "Case Normalization | The Problem",
    "text": "Case Normalization | The Problem\nExamples of equivalent values treated as different:\n\njohn, John, JOHN\nvan dyke, Van Dyke, VAN DYKE\n\nIf left untreated:\n\nGROUP BY fragments categories\nCOUNT(DISTINCT ...) overcounts\njoins silently fail",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#lower-normalize-for-analysis",
    "href": "materials/sql/slides/session6.html#lower-normalize-for-analysis",
    "title": "String Functions",
    "section": "LOWER() | Normalize for Analysis",
    "text": "LOWER() | Normalize for Analysis\nLOWER() converts all characters to lowercase.\nSELECT\n  first_name,\n  LOWER(first_name) AS first_name_lower\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nfirst_name\nfirst_name_lower\n\n\n\n\njohn\njohn\n\n\nANNa\nanna\n\n\nmARy\nmary\n\n\ngeORGe\ngeorge\n\n\nALEx\nalex\n\n\nlAuRA\nlaura",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#upper-normalize-for-codes",
    "href": "materials/sql/slides/session6.html#upper-normalize-for-codes",
    "title": "String Functions",
    "section": "UPPER() | Normalize for Codes",
    "text": "UPPER() | Normalize for Codes\nUPPER() converts all characters to uppercase.\nSELECT\n  last_name,\n  UPPER(last_name) AS last_name_upper\nFROM customers_raw_text;\n\\[\\downarrow\\]\n\n\n\nlast_name\nlast_name_upper\n\n\n\n\nDOE\nDOE\n\n\nsmith\nSMITH\n\n\njoHNson\nJOHNSON\n\n\nbrown\nBROWN\n\n\nO'NEILL\nO'NEILL\n\n\nvan dyke\nVAN DYKE",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#initcap-presentation-formatting",
    "href": "materials/sql/slides/session6.html#initcap-presentation-formatting",
    "title": "String Functions",
    "section": "INITCAP() | Presentation Formatting",
    "text": "INITCAP() | Presentation Formatting\nINITCAP() converts text to title case: jOhn sMiTH \\(\\rightarrow\\) John Smith\n\nSELECT\n  first_name,\n  INITCAP(first_name) AS first_name_clean\nFROM customers_raw_text;\n\n\n\\[\\downarrow\\]\n\n\n\nfirst_name\nfirst_name_clean\n\n\n\n\njohn\nJohn\n\n\nANNa\nAnna\n\n\nmARy\nMary\n\n\ngeORGe\nGeorge\n\n\nALEx\nAlex\n\n\nlAuRA\nLaura",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#choosing-the-right-function",
    "href": "materials/sql/slides/session6.html#choosing-the-right-function",
    "title": "String Functions",
    "section": "Choosing the Right Function",
    "text": "Choosing the Right Function\n\n\n\nFunction\nBest Used For\n\n\n\n\nLOWER()\njoins, grouping, deduplication\n\n\nUPPER()\ncodes, abbreviations\n\n\nINITCAP()\nnames, presentation\n\n\n\n\nnever mix raw and normalized text in analysis \n\n\nConsistency matters more than preference.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-replace-matters",
    "href": "materials/sql/slides/session6.html#why-replace-matters",
    "title": "String Functions",
    "section": "Why REPLACE() Matters",
    "text": "Why REPLACE() Matters\nAfter handling whitespace and capitalization, the next common issue is structural noise inside text values.\nTypical examples include:\n\nhyphens in phone numbers: 077-600-945\nspaces used as separators: 077600945\n\ndots (.), slashes(\\,/), or underscores:\n\ninconsistent formatting characters",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-principle-1",
    "href": "materials/sql/slides/session6.html#analytical-principle-1",
    "title": "String Functions",
    "section": "Analytical Principle",
    "text": "Analytical Principle\nREPLACE() removes or substitutes known, explicit characters.\nThis makes it ideal when:\n\nthe pattern is simple\n\nthe character to remove is known in advance\n\nrules are deterministic\n\n\nREPLACE() is not pattern-based &gt; it is literal and predictable.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#removing-hyphens",
    "href": "materials/sql/slides/session6.html#removing-hyphens",
    "title": "String Functions",
    "section": "Removing Hyphens",
    "text": "Removing Hyphens\nPhone numbers often contain hyphens as visual separators.\nSELECT\n  raw_phone,\n  REPLACE(raw_phone, '-', '') AS phone_no_hyphen\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\nphone_no_hyphen\n\n\n\n\n077-600-045\n077600045\n\n\n(374)-77-600-945\n(374)77600945\n\n\n077600945\n077600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#chaining-replacements",
    "href": "materials/sql/slides/session6.html#chaining-replacements",
    "title": "String Functions",
    "section": "Chaining Replacements",
    "text": "Chaining Replacements\nYou can apply REPLACE() multiple times to remove different characters.\nSELECT\n  raw_phone,\n  REPLACE(\n    REPLACE(\n      REPLACE(TRIM(raw_phone), '-', ''),\n    '(', ''),\n  ')', '') AS phone_clean_partial\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\nphone_clean_partial\n\n\n\n\n(374)-77-600-945\n37477600945\n\n\n077-600-045\n077600045\n\n\n077600945\n077600945\n\n\n\nThis is progress, but still not fully standardized.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-we-stop-here",
    "href": "materials/sql/slides/session6.html#why-we-stop-here",
    "title": "String Functions",
    "section": "Why We Stop Here",
    "text": "Why We Stop Here\nAt this stage:\n\nseparators are removed\n\nformatting noise is reduced\n\nbut digits are not guaranteed\n\nWe still have:\n\ncountry codes\n\nvariable lengths\n\ninconsistent prefixes\n\n\nThis tells us REPLACE() alone is not sufficient.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#replace-analytical-warning",
    "href": "materials/sql/slides/session6.html#replace-analytical-warning",
    "title": "String Functions",
    "section": "REPLACE() | Analytical Warning",
    "text": "REPLACE() | Analytical Warning\n\nEvery REPLACE() encodes a business assumption.\n\nwhat characters are allowed?\n\nwhat characters are noise?\n\nwhat if formats change later?\n\n\nDocument these decisions!",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#replace-when-to-use",
    "href": "materials/sql/slides/session6.html#replace-when-to-use",
    "title": "String Functions",
    "section": "REPLACE() | When to Use",
    "text": "REPLACE() | When to Use\nUse REPLACE() when:\n\nthe character to remove is known\n\nrules are simple and explicit\n\nyou want maximum transparency\n\nAvoid REPLACE() when:\n\npatterns vary\n\nrules depend on position\n\nyou need validation",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#regexp_replace-why-it-exists",
    "href": "materials/sql/slides/session6.html#regexp_replace-why-it-exists",
    "title": "String Functions",
    "section": "REGEXP_REPLACE() | Why It Exists",
    "text": "REGEXP_REPLACE() | Why It Exists\nREGEXP stands for ‚ÄúRegular Expression‚Äù\n\nREGEXP_REPLACE() allows you to define rules, not characters.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#regexp_replace-why-it-matters",
    "href": "materials/sql/slides/session6.html#regexp_replace-why-it-matters",
    "title": "String Functions",
    "section": "REGEXP_REPLACE() | Why It Matters",
    "text": "REGEXP_REPLACE() | Why It Matters\nFrom an analytical perspective:\n\nfewer hard-coded assumptions\nbetter generalization to unseen formats\nrobust, reusable cleaning logic",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#regexp_replace-syntax",
    "href": "materials/sql/slides/session6.html#regexp_replace-syntax",
    "title": "String Functions",
    "section": "REGEXP_REPLACE() | Syntax",
    "text": "REGEXP_REPLACE() | Syntax\nREGEXP_REPLACE(text, pattern, replacement [, flags])\n\ntext ‚Üí input string\npattern ‚Üí regex rule\nreplacement ‚Üí substitution\nflags ‚Üí modifiers (g = global)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-keep-only-digits",
    "href": "materials/sql/slides/session6.html#example-1-keep-only-digits",
    "title": "String Functions",
    "section": "Example 1 | Keep Only Digits",
    "text": "Example 1 | Keep Only Digits\nPattern: [^0-9]\nSELECT\n  raw_phone,\n  REGEXP_REPLACE(raw_phone, '[^0-9]', '', 'g') AS digits_only\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\ndigits_only\n\n\n\n\n077600945\n077600945\n\n\n077600945\n077600945\n\n\n77600945\n77600945\n\n\n077-600-045\n077600045\n\n\n(374)-77-600-945\n37477600945\n\n\n37477600945\n37477600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-parentheses-in-categories",
    "href": "materials/sql/slides/session6.html#example-2-parentheses-in-categories",
    "title": "String Functions",
    "section": "Example 2 | Parentheses in Categories",
    "text": "Example 2 | Parentheses in Categories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#pattern-1-remove-content",
    "href": "materials/sql/slides/session6.html#pattern-1-remove-content",
    "title": "String Functions",
    "section": "Pattern 1 | Remove () Content",
    "text": "Pattern 1 | Remove () Content\nPattern: \\([^)]*\\)\nSELECT\n  category_raw,\n  REGEXP_REPLACE(category_raw, '\\([^)]*\\)', '', 'g') AS category_clean1\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncategory_raw\ncategory_clean1\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#pattern-2-regexp_replace",
    "href": "materials/sql/slides/session6.html#pattern-2-regexp_replace",
    "title": "String Functions",
    "section": "Pattern 2 | REGEXP_REPLACE",
    "text": "Pattern 2 | REGEXP_REPLACE\n\nTRIM(REGEXP_REPLACE(category_raw, '\\([^)]*\\)', '', 'g'))\n\n\nSELECT\n  category_raw,\n  TRIM(\n    REGEXP_REPLACE(category_raw, '\\([^)]*\\)', '', 'g')\n  ) AS category_clean2\nFROM customers_raw_text;\n\n\n\\[\\downarrow\\]\n\n\n\ncategory_raw\ncategory_clean2\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#pattern-3-single-step-regex",
    "href": "materials/sql/slides/session6.html#pattern-3-single-step-regex",
    "title": "String Functions",
    "section": "Pattern 3 | Single-Step Regex",
    "text": "Pattern 3 | Single-Step Regex\nPattern: \\s*\\(.*?\\)\nSELECT\n  category_raw,\n  REGEXP_REPLACE(category_raw, '\\s*\\(.*?\\)', '', 'g')\n    AS category_clean3\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncategory_raw\ncategory_clean3\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#comparison-summary",
    "href": "materials/sql/slides/session6.html#comparison-summary",
    "title": "String Functions",
    "section": "Comparison Summary",
    "text": "Comparison Summary\n\n\n\nApproach\nResult\nNotes\n\n\n\n\nPattern 1\nAccessories\nLeaves spaces\n\n\nPattern 2\nAccessories\nSafe & explicit\n\n\nPattern 3\nAccessories\nCompact regex",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#regexp-vs-replace",
    "href": "materials/sql/slides/session6.html#regexp-vs-replace",
    "title": "String Functions",
    "section": "REGEXP vs REPLACE",
    "text": "REGEXP vs REPLACE\n\n\n\nScenario\nPrefer\n\n\n\n\nKnown characters\nREPLACE()\n\n\nVariable formats\nREGEXP_REPLACE()\n\n\nValidation & extraction\nREGEXP_REPLACE()\n\n\nSimplicity\nREPLACE()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#regexp_replace-attention",
    "href": "materials/sql/slides/session6.html#regexp_replace-attention",
    "title": "String Functions",
    "section": "REGEXP_REPLACE() | ATTENTION",
    "text": "REGEXP_REPLACE() | ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | Regex Is Powerful\n\n\n\nbroad patterns remove valid data\nunreadable regex creates technical debt\nrules must be documented and validated\n\nAlways validate with:\n\nLENGTH()\nCOUNT(DISTINCT ...)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#substring-1",
    "href": "materials/sql/slides/session6.html#substring-1",
    "title": "String Functions",
    "section": "SUBSTRING()",
    "text": "SUBSTRING()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#substring-basic-positional-extraction",
    "href": "materials/sql/slides/session6.html#substring-basic-positional-extraction",
    "title": "String Functions",
    "section": "SUBSTRING() | Basic Positional Extraction",
    "text": "SUBSTRING() | Basic Positional Extraction\nSUBSTRING(text FROM start_position FOR length)\nSUBSTRING() extracts a portion of text based on position or pattern.\nIt is used to isolate signal from compound fields.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-extract-last-8-digits-of-phone-numbers",
    "href": "materials/sql/slides/session6.html#example-1-extract-last-8-digits-of-phone-numbers",
    "title": "String Functions",
    "section": "Example 1 | Extract Last 8 Digits of Phone Numbers",
    "text": "Example 1 | Extract Last 8 Digits of Phone Numbers\nSELECT\n  raw_phone,\n  SUBSTRING(raw_phone FROM LENGTH(raw_phone) - 7 FOR 8) AS phone_core\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\nphone_core\n\n\n\n\n077600945\n77600945\n\n\n37477600945\n77600945\n\n\n77600945\n77600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-substring-with-fixed-structure",
    "href": "materials/sql/slides/session6.html#example-2-substring-with-fixed-structure",
    "title": "String Functions",
    "section": "Example 2 | SUBSTRING() with Fixed Structure",
    "text": "Example 2 | SUBSTRING() with Fixed Structure\nSELECT\n  category_raw,\n  SUBSTRING(category_raw FROM 1 FOR 11) AS category_prefix\nFROM customers_raw_text;\n\n\\[\\rightarrow\\]\n\n\n\ncategory_raw\ncategory_prefix\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-3-substring-with-regex",
    "href": "materials/sql/slides/session6.html#example-3-substring-with-regex",
    "title": "String Functions",
    "section": "Example 3 | SUBSTRING() with Regex",
    "text": "Example 3 | SUBSTRING() with Regex\nPostgreSQL supports regex-based extraction:\nSELECT\n  raw_phone,\n  SUBSTRING(raw_phone FROM '[0-9]+') AS first_digit_sequence\nFROM customers_raw_text;\n\n\\[\\downtarrow\\]\n\n\n\nraw_phone\nfirst_digit_sequence\n\n\n\n\n077600945\n077600945\n\n\n077-600-045\n077\n\n\n(374)-77-600-945\n374\n\n\n37477600945\n37477600945\n\n\n77600945\n77600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#positional-vs-pattern-based-substring",
    "href": "materials/sql/slides/session6.html#positional-vs-pattern-based-substring",
    "title": "String Functions",
    "section": "Positional vs Pattern-Based SUBSTRING()",
    "text": "Positional vs Pattern-Based SUBSTRING()\n\n\n\nUse Case\nPrefer\n\n\n\n\nFixed-length identifiers\nPositional\n\n\nVariable formats\nRegex-based\n\n\nPerformance-critical logic\nPositional\n\n\nUnknown structure\nRegex-based",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-best-practice",
    "href": "materials/sql/slides/session6.html#analytical-best-practice",
    "title": "String Functions",
    "section": "Analytical Best Practice",
    "text": "Analytical Best Practice\n\nuse positional SUBSTRING() when formats are stable\nuse regex SUBSTRING() when formats vary\nvalidate results with LENGTH() and COUNT(DISTINCT ...)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#concat-why-it-exists",
    "href": "materials/sql/slides/session6.html#concat-why-it-exists",
    "title": "String Functions",
    "section": "CONCAT() | Why It Exists",
    "text": "CONCAT() | Why It Exists\nAfter cleaning text with:\n\nTRIM()\nLOWER(), UPPER(), INITCAP()\nSUBSTRING()\n\nwe often need to construct new text values.\n\nCONCAT() and || allow us to combine atomic fields into meaningful analytical dimensions.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#concat-why-it-matters",
    "href": "materials/sql/slides/session6.html#concat-why-it-matters",
    "title": "String Functions",
    "section": "CONCAT() | Why It Matters",
    "text": "CONCAT() | Why It Matters\nFrom an analytical perspective:\n\nreporting requires combined labels\ndashboards need human-readable dimensions\njoins may rely on constructed keys\n\n\nConcatenation is feature engineering, not cosmetic formatting.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#concat-syntax",
    "href": "materials/sql/slides/session6.html#concat-syntax",
    "title": "String Functions",
    "section": "CONCAT() | Syntax",
    "text": "CONCAT() | Syntax\nCONCAT(value1, value2, ..., valueN)\nKey properties:\n\naccepts multiple arguments\ntreats NULL as empty\nalways returns text",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#concat-operator-form",
    "href": "materials/sql/slides/session6.html#concat-operator-form",
    "title": "String Functions",
    "section": "CONCAT() | Operator Form ||",
    "text": "CONCAT() | Operator Form ||\nPostgreSQL also supports string concatenation using:\nvalue1 || value2\n\nBehavior differs when NULL values are present.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-full-name-construction-raw",
    "href": "materials/sql/slides/session6.html#example-1-full-name-construction-raw",
    "title": "String Functions",
    "section": "Example 1 | Full Name Construction (Raw)",
    "text": "Example 1 | Full Name Construction (Raw)\nSELECT\n  customer_id,\n  first_name,\n  last_name,\n  CONCAT(first_name, ' ', last_name) AS full_name\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncustomer_id\nfirst_name\nlast_name\nfull_name\n\n\n\n\n1\njohn\ndoe\njohn doe\n\n\n2\nMARY\nSMITH\nMARY SMITH\n\n\n3\naLEx\njohnson\naLEx johnson\n\n\n4\nanna\nVAN DYKE\nanna VAN DYKE\n\n\n5\nNULL\nbrown\nbrown",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-observation",
    "href": "materials/sql/slides/session6.html#analytical-observation",
    "title": "String Functions",
    "section": "Analytical Observation",
    "text": "Analytical Observation\n\nconcatenation works mechanically\ncapitalization is inconsistent\nleading spaces appear when first_name is NULL\n\n\nThis output is technically valid but analytically weak.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-concat-initcap",
    "href": "materials/sql/slides/session6.html#example-2-concat-initcap",
    "title": "String Functions",
    "section": "Example 2 | CONCAT() + INITCAP()",
    "text": "Example 2 | CONCAT() + INITCAP()\nRecommended analytical pattern: first normalize case and then concatenate\nSELECT\n  customer_id,\n  INITCAP(first_name) AS first_name_clean,\n  INITCAP(last_name)  AS last_name_clean,\n  INITCAP(CONCAT(first_name, ' ', last_name)) AS full_name_clean\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncustomer_id\nfirst_name_clean\nlast_name_clean\nfull_name_clean\n\n\n\n\n1\nJohn\nDoe\nJohn Doe\n\n\n2\nMary\nSmith\nMary Smith\n\n\n3\nAlex\nJohnson\nAlex Johnson\n\n\n4\nAnna\nVan Dyke\nAnna Van Dyke\n\n\n5\nNULL\nBrown\nBrown",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#concat-vs",
    "href": "materials/sql/slides/session6.html#concat-vs",
    "title": "String Functions",
    "section": "CONCAT() vs ||",
    "text": "CONCAT() vs ||\nSame logic using the operator form:\nSELECT\n  customer_id,\n  INITCAP(first_name) || ' ' || INITCAP(last_name) AS full_name_clean\nFROM customers_raw_text;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#key-difference",
    "href": "materials/sql/slides/session6.html#key-difference",
    "title": "String Functions",
    "section": "Key Difference",
    "text": "Key Difference\n\nCONCAT() treats NULL as empty\n|| propagates NULL\n\n\nThis distinction directly affects analytical results.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#when-to-use-which",
    "href": "materials/sql/slides/session6.html#when-to-use-which",
    "title": "String Functions",
    "section": "When to Use Which",
    "text": "When to Use Which\n\n\n\nSituation\nPrefer\n\n\n\n\nPossible NULL values\nCONCAT()\n\n\nStrict NULL propagation\n||\n\n\nReporting and dashboards\nCONCAT()\n\n\nValidation logic\n||",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-best-practice-1",
    "href": "materials/sql/slides/session6.html#analytical-best-practice-1",
    "title": "String Functions",
    "section": "Analytical Best Practice",
    "text": "Analytical Best Practice\n\nnormalize text before concatenation\ndecide explicitly how NULL should behave\nvalidate constructed fields using:\n\nCOUNT(DISTINCT ...)\nsample inspection",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#position-strpos-what-problem-it-solves",
    "href": "materials/sql/slides/session6.html#position-strpos-what-problem-it-solves",
    "title": "String Functions",
    "section": "POSITION() / STRPOS() | What Problem It Solves",
    "text": "POSITION() / STRPOS() | What Problem It Solves\n\nWhere does a given substring start?\n\nThis is a diagnostic and validation step, not a cleaning step.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#position-strpos-why-it-matters",
    "href": "materials/sql/slides/session6.html#position-strpos-why-it-matters",
    "title": "String Functions",
    "section": "POSITION() / STRPOS() | Why It Matters",
    "text": "POSITION() / STRPOS() | Why It Matters\nKnowing the position of a substring allows you to:\n\nvalidate expected formats\ndetect malformed values\ndrive conditional logic\nprepare for controlled extraction",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#position-syntax",
    "href": "materials/sql/slides/session6.html#position-syntax",
    "title": "String Functions",
    "section": "POSITION() | Syntax",
    "text": "POSITION() | Syntax\nPOSITION(substring IN text)\n\nreturns a 1-based position\nreturns 0 if the substring is not found",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#strpos-postgresql-alias",
    "href": "materials/sql/slides/session6.html#strpos-postgresql-alias",
    "title": "String Functions",
    "section": "STRPOS() | PostgreSQL Alias",
    "text": "STRPOS() | PostgreSQL Alias\nPostgreSQL also supports:\nSTRPOS(text, substring)\n\nidentical behavior to POSITION()\noften preferred for readability",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-detect-parentheses-in-categories",
    "href": "materials/sql/slides/session6.html#example-1-detect-parentheses-in-categories",
    "title": "String Functions",
    "section": "Example 1 | Detect Parentheses in Categories",
    "text": "Example 1 | Detect Parentheses in Categories\nBefore cleaning categories, we first detect annotations.\nSELECT\n  category_raw,\n  POSITION('(' IN category_raw) AS open_paren_pos\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncategory_raw\nopen_paren_pos\n\n\n\n\nAccessories (Promo)\n13\n\n\nElectronics (Old)\n13\n\n\nAccessories\n0\n\n\nElectronics (Promo)\n13\n\n\nAccessories (Test)\n13",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#interpreting-the-result",
    "href": "materials/sql/slides/session6.html#interpreting-the-result",
    "title": "String Functions",
    "section": "Interpreting the Result",
    "text": "Interpreting the Result\n\nopen_paren_pos &gt; 0 ‚Üí annotation exists\n\nopen_paren_pos = 0 ‚Üí clean category\n\nThis tells us which rows require cleaning.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-strpos-for-phone-diagnostics",
    "href": "materials/sql/slides/session6.html#example-2-strpos-for-phone-diagnostics",
    "title": "String Functions",
    "section": "Example 2 | STRPOS() for Phone Diagnostics",
    "text": "Example 2 | STRPOS() for Phone Diagnostics\nSELECT\n  raw_phone,\n  STRPOS(raw_phone, '-') AS hyphen_pos,\n  STRPOS(raw_phone, '(') AS paren_pos\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\nhyphen_pos\nparen_pos\n\n\n\n\n077600945\n0\n0\n\n\n077-600-045\n4\n0\n\n\n(374)-77-600-945\n6\n1\n\n\n37477600945\n0\n0\n\n\n77600945\n0\n0",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-interpretation",
    "href": "materials/sql/slides/session6.html#analytical-interpretation",
    "title": "String Functions",
    "section": "Analytical Interpretation",
    "text": "Analytical Interpretation\n\n\n\n\n\n\nNote\n\n\nThis check is usually done before applying REGEXP_REPLACE().",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#position-as-a-validation-tool",
    "href": "materials/sql/slides/session6.html#position-as-a-validation-tool",
    "title": "String Functions",
    "section": "POSITION() as a Validation Tool",
    "text": "POSITION() as a Validation Tool\nA common analytical pattern is converting positions into boolean flags.\nSELECT\n  customer_id,\n  category_raw,\n  POSITION('(' IN category_raw) &gt; 0 AS has_annotation\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncustomer_id\ncategory_raw\nhas_annotation\n\n\n\n\n1\nAccessories (Promo)\ntrue\n\n\n2\nElectronics (Old)\ntrue\n\n\n3\nAccessories\nfalse\n\n\n4\nElectronics (Promo)\ntrue\n\n\n5\nAccessories (Test)\ntrue",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-this-pattern-is-powerful",
    "href": "materials/sql/slides/session6.html#why-this-pattern-is-powerful",
    "title": "String Functions",
    "section": "Why This Pattern Is Powerful",
    "text": "Why This Pattern Is Powerful\nThis enables:\n\ndata quality reporting\n\naudit checks\n\nconditional cleaning logic\n\n\nWithout altering the original data.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#position-vs-substring",
    "href": "materials/sql/slides/session6.html#position-vs-substring",
    "title": "String Functions",
    "section": "POSITION() vs SUBSTRING()",
    "text": "POSITION() vs SUBSTRING()\n\n\n\nQuestion\nPrefer\n\n\n\n\nWhere is it?\nPOSITION() / STRPOS()\n\n\nExtract it\nSUBSTRING()\n\n\nValidate format\nPOSITION()\n\n\nClean text\nREGEXP_REPLACE()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-best-practice-2",
    "href": "materials/sql/slides/session6.html#analytical-best-practice-2",
    "title": "String Functions",
    "section": "Analytical Best Practice",
    "text": "Analytical Best Practice\n\nuse POSITION() to measure and detect\nuse SUBSTRING() to extract\nuse REGEXP_REPLACE() to clean\nnever assume structure without checking positions",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#split_part-what-problem-it-solves",
    "href": "materials/sql/slides/session6.html#split_part-what-problem-it-solves",
    "title": "String Functions",
    "section": "SPLIT_PART() | What Problem It Solves",
    "text": "SPLIT_PART() | What Problem It Solves\nSPLIT_PART() allows you to extract a specific segment from such strings\nwithout using regular expressions.\nIt answers the question:\n\n\nWhich part of a delimited string do I need?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#split_part-why-it-exists",
    "href": "materials/sql/slides/session6.html#split_part-why-it-exists",
    "title": "String Functions",
    "section": "SPLIT_PART() | Why It Exists",
    "text": "SPLIT_PART() | Why It Exists\nUsed to extract a specific segment from delimited text.\nKnown structure ‚Üí simple logic ‚Üí high reliability.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#split_part-syntax",
    "href": "materials/sql/slides/session6.html#split_part-syntax",
    "title": "String Functions",
    "section": "SPLIT_PART() | Syntax",
    "text": "SPLIT_PART() | Syntax\nSPLIT_PART(text, delimiter, position)\n\nposition is 1-based\nmissing part ‚Üí empty string",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-first-segment",
    "href": "materials/sql/slides/session6.html#example-1-first-segment",
    "title": "String Functions",
    "section": "Example 1 | First Segment",
    "text": "Example 1 | First Segment\nSELECT\n  raw_phone,\n  SPLIT_PART(raw_phone, '-', 1) AS first_part\nFROM customers_raw_text;\n\n\n\n\nraw_phone\nfirst_part\n\n\n\n\n077-600-045\n077\n\n\n(374)-77-600-945\n(374)\n\n\n77600945\n77600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-second-segment",
    "href": "materials/sql/slides/session6.html#example-2-second-segment",
    "title": "String Functions",
    "section": "Example 2 | Second Segment",
    "text": "Example 2 | Second Segment\nSELECT\n  raw_phone,\n  SPLIT_PART(raw_phone, '-', 2) AS second_part\nFROM customers_raw_text;\n\n\n\n\nraw_phone\nsecond_part\n\n\n\n\n077-600-045\n600\n\n\n(374)-77-600-945\n77\n\n\n77600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#split_part-attention",
    "href": "materials/sql/slides/session6.html#split_part-attention",
    "title": "String Functions",
    "section": "SPLIT_PART() | ATTENTION",
    "text": "SPLIT_PART() | ATTENTION\n\nempty string ‚â† NULL\ndelimiter may not exist\nstructure must be validated\n\nUse together with:\n\nPOSITION()\nNULLIF()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#nullif-why-it-exists",
    "href": "materials/sql/slides/session6.html#nullif-why-it-exists",
    "title": "String Functions",
    "section": "NULLIF() | Why It Exists",
    "text": "NULLIF() | Why It Exists\nReal-world data often encodes missing information as fake values.\n\nNULLIF() lets you explicitly decide:\n\nWhen should a value be treated as missing?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#nullif-why-it-matters",
    "href": "materials/sql/slides/session6.html#nullif-why-it-matters",
    "title": "String Functions",
    "section": "NULLIF() | Why It Matters",
    "text": "NULLIF() | Why It Matters\nFrom an analytical perspective:\n\nplaceholder values distort metrics\n\nempty strings inflate counts\n\nfake defaults hide data quality issues\n\nNULLIF() restores semantic correctness.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#nullif-syntax",
    "href": "materials/sql/slides/session6.html#nullif-syntax",
    "title": "String Functions",
    "section": "NULLIF() | Syntax",
    "text": "NULLIF() | Syntax\nNULLIF(value, comparison_value)\n\nreturns NULL if values are equal\n\notherwise returns the original value",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-empty-strings-to-null",
    "href": "materials/sql/slides/session6.html#example-1-empty-strings-to-null",
    "title": "String Functions",
    "section": "Example 1 | Empty Strings to NULL",
    "text": "Example 1 | Empty Strings to NULL\nSELECT\n  customer_id,\n  last_name,\n  NULLIF(last_name, '') AS last_name_clean\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncustomer_id\nlast_name\nlast_name_clean\n\n\n\n\n1\ndoe\ndoe\n\n\n2\nsmith\nsmith\n\n\n3\njohnson\njohnson\n\n\n4\nVAN DYKE\nVAN DYKE\n\n\n5\n\nNULL",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#why-this-matters",
    "href": "materials/sql/slides/session6.html#why-this-matters",
    "title": "String Functions",
    "section": "Why This Matters",
    "text": "Why This Matters\nWithout NULLIF():\n\nempty strings count as real values\n\nCOUNT(DISTINCT last_name) is wrong\n\ncompleteness checks are misleading",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-placeholder-values",
    "href": "materials/sql/slides/session6.html#example-2-placeholder-values",
    "title": "String Functions",
    "section": "Example 2 | Placeholder Values",
    "text": "Example 2 | Placeholder Values\nSELECT\n  category_raw,\n  NULLIF(category_raw, 'UNKNOWN') AS category_clean\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\ncategory_raw\ncategory_clean\n\n\n\n\nAccessories\nAccessories\n\n\nUNKNOWN\nNULL",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-3-nullif-before-aggregation",
    "href": "materials/sql/slides/session6.html#example-3-nullif-before-aggregation",
    "title": "String Functions",
    "section": "Example 3 | NULLIF() Before Aggregation",
    "text": "Example 3 | NULLIF() Before Aggregation\nSELECT\n  COUNT(NULLIF(category_raw, 'UNKNOWN')) AS valid_categories\nFROM customers_raw_text;\n\nThis prevents placeholder values from inflating KPIs.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#nullif-with-concat",
    "href": "materials/sql/slides/session6.html#nullif-with-concat",
    "title": "String Functions",
    "section": "NULLIF() with CONCAT()",
    "text": "NULLIF() with CONCAT()\nSELECT\n  customer_id,\n  CONCAT(\n    first_name,\n    ' ',\n    NULLIF(last_name, '')\n  ) AS full_name_safe\nFROM customers_raw_text;\n\nAvoids malformed labels caused by empty strings.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#nullif-vs-coalesce",
    "href": "materials/sql/slides/session6.html#nullif-vs-coalesce",
    "title": "String Functions",
    "section": "NULLIF() vs COALESCE()",
    "text": "NULLIF() vs COALESCE()\n\n\n\nGoal\nFunction\n\n\n\n\nConvert bad value to NULL\nNULLIF()\n\n\nReplace NULL with value\nCOALESCE()\n\n\nRestore missing meaning\nNULLIF()\n\n\nImpute values\nCOALESCE()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-best-practice-3",
    "href": "materials/sql/slides/session6.html#analytical-best-practice-3",
    "title": "String Functions",
    "section": "Analytical Best Practice",
    "text": "Analytical Best Practice\n\naudit text fields for placeholders\n\nconvert fake values to NULL first\n\naggregate only meaningful values",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#left-right-why-they-exist",
    "href": "materials/sql/slides/session6.html#left-right-why-they-exist",
    "title": "String Functions",
    "section": "LEFT() / RIGHT() | Why They Exist",
    "text": "LEFT() / RIGHT() | Why They Exist\nSome analytical fields follow a fixed directional structure.\n\n\nprefixes carry meaning\n\nsuffixes carry meaning\n\nlength is known or enforced\n\nLEFT() and RIGHT() let you extract directional segments without complex logic.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#left-right-why-they-matter",
    "href": "materials/sql/slides/session6.html#left-right-why-they-matter",
    "title": "String Functions",
    "section": "LEFT() / RIGHT() | Why They Matter",
    "text": "LEFT() / RIGHT() | Why They Matter\nFrom an analytical perspective:\n\nsimpler than SUBSTRING()\n\nmore readable intent\n\nsafer for fixed-format fields\n\nThey are often used for:\n\ncountry or region prefixes\n\ncategory codes\n\nversion suffixes\n\nshort identifiers",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#left-right-syntax",
    "href": "materials/sql/slides/session6.html#left-right-syntax",
    "title": "String Functions",
    "section": "LEFT() / RIGHT() | Syntax",
    "text": "LEFT() / RIGHT() | Syntax\nLEFT(text, n)\nRIGHT(text, n)\n\nn = number of characters\n\nextraction starts from left or right",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-1-phone-prefix-detection",
    "href": "materials/sql/slides/session6.html#example-1-phone-prefix-detection",
    "title": "String Functions",
    "section": "Example 1 | Phone Prefix Detection",
    "text": "Example 1 | Phone Prefix Detection\nSELECT\n  raw_phone,\n  LEFT(\n    REGEXP_REPLACE(raw_phone, '[^0-9]', '', 'g'),\n    3\n  ) AS phone_prefix\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\nphone_prefix\n\n\n\n\n077600945\n077\n\n\n077-600-045\n077\n\n\n(374)-77-600-945\n374\n\n\n37477600945\n374\n\n\n77600945\n776",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#analytical-interpretation-1",
    "href": "materials/sql/slides/session6.html#analytical-interpretation-1",
    "title": "String Functions",
    "section": "Analytical Interpretation",
    "text": "Analytical Interpretation\n\nprefixes reveal country or operator\n\nsupports validation and routing logic\n\nuseful before normalization",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#example-2-core-identifier-via-right",
    "href": "materials/sql/slides/session6.html#example-2-core-identifier-via-right",
    "title": "String Functions",
    "section": "Example 2 | Core Identifier via RIGHT()",
    "text": "Example 2 | Core Identifier via RIGHT()\nSELECT\n  raw_phone,\n  RIGHT(\n    REGEXP_REPLACE(raw_phone, '[^0-9]', '', 'g'),\n    8\n  ) AS phone_core\nFROM customers_raw_text;\n\n\\[\\downarrow\\]\n\n\n\nraw_phone\nphone_core\n\n\n\n\n077600945\n77600945\n\n\n37477600945\n77600945\n\n\n77600945\n77600945",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#leftright-vs-substring",
    "href": "materials/sql/slides/session6.html#leftright-vs-substring",
    "title": "String Functions",
    "section": "LEFT()/RIGHT() vs SUBSTRING()",
    "text": "LEFT()/RIGHT() vs SUBSTRING()\n\n\n\nScenario\nPrefer\n\n\n\n\nFixed prefix/suffix\nLEFT()/RIGHT()\n\n\nDynamic positions\nSUBSTRING()\n\n\nReadability\nLEFT()/RIGHT()\n\n\nComplex extraction\nSUBSTRING()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#goal-of-the-exercise",
    "href": "materials/sql/slides/session6.html#goal-of-the-exercise",
    "title": "String Functions",
    "section": "Goal of the Exercise",
    "text": "Goal of the Exercise\nIn this in-class task, you will work with intentionally messy transaction data to understand:\n\nhow dirty text breaks GROUP BY\nwhy measurement must come before cleaning\nhow text functions affect analytical results\nTime: 10‚Äì15 minutes\n\nMode: Individual or pairs",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#step-1-create-messy-transaction-data",
    "href": "materials/sql/slides/session6.html#step-1-create-messy-transaction-data",
    "title": "String Functions",
    "section": "Step 1 | Create Messy Transaction Data",
    "text": "Step 1 | Create Messy Transaction Data\nWe simulate real-world dirty text commonly found in transactional systems.\nDROP TABLE IF EXISTS transactions_text_demo;\n\nCREATE TABLE transactions_text_demo (\n  transaction_id INTEGER,\n  customer_id    INTEGER,\n  raw_phone      TEXT,\n  category_raw   TEXT,\n  quantity       INTEGER,\n  price          NUMERIC(10,2)\n);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#step-2-insert-1000-rows-of-messy-data",
    "href": "materials/sql/slides/session6.html#step-2-insert-1000-rows-of-messy-data",
    "title": "String Functions",
    "section": "Step 2 | Insert 1,000 Rows of Messy Data",
    "text": "Step 2 | Insert 1,000 Rows of Messy Data\nINSERT INTO transactions_text_demo\nSELECT\n  gs AS transaction_id,\n  (RANDOM() * 200)::INT + 1 AS customer_id,\n\n  CASE (gs % 6)\n    WHEN 0 THEN '   077600945  '\n    WHEN 1 THEN '077-600-045'\n    WHEN 2 THEN '(374)-77-600-945'\n    WHEN 3 THEN '37477600945'\n    WHEN 4 THEN '77600945'\n    ELSE '077600945'\n  END AS raw_phone,\n\n  CASE (gs % 5)\n    WHEN 0 THEN 'Accessories (Promo)'\n    WHEN 1 THEN 'Accessories (Test)'\n    WHEN 2 THEN 'Electronics (Old)'\n    WHEN 3 THEN 'Electronics (Promo)'\n    ELSE 'Accessories'\n  END AS category_raw,\n\n  (RANDOM() * 5)::INT + 1 AS quantity,\n  (RANDOM() * 500 + 10)::NUMERIC(10,2) AS price\nFROM generate_series(1, 1000) AS gs;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#step-3-sanity-check",
    "href": "materials/sql/slides/session6.html#step-3-sanity-check",
    "title": "String Functions",
    "section": "Step 3 | Sanity Check",
    "text": "Step 3 | Sanity Check\nSELECT\n  COUNT(*) AS total_rows,\n  COUNT(DISTINCT raw_phone) AS distinct_raw_phones,\n  COUNT(DISTINCT category_raw) AS distinct_categories\nFROM transactions_text_demo;\nThink:\n\nDo these numbers look reasonable?\nWhat already looks suspicious?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#task-1-phone-number-diagnostics",
    "href": "materials/sql/slides/session6.html#task-1-phone-number-diagnostics",
    "title": "String Functions",
    "section": "Task 1 | Phone Number Diagnostics",
    "text": "Task 1 | Phone Number Diagnostics\nWrite a query that shows:\n\nraw_phone\nLENGTH(raw_phone)\nposition of '-'\nposition of '('\nnumber of rows per pattern\n\nFunctions to use:\n\nLENGTH()\nPOSITION() or STRPOS()\nGROUP BY",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#task-2-category-fragmentation",
    "href": "materials/sql/slides/session6.html#task-2-category-fragmentation",
    "title": "String Functions",
    "section": "Task 2 | Category Fragmentation",
    "text": "Task 2 | Category Fragmentation\nGroup by raw category values.\nSELECT\n  category_raw,\n  COUNT(*) AS transactions\nFROM transactions_text_demo\nGROUP BY category_raw\nORDER BY transactions DESC;\nAnswer:\n\nhow many categories appear?\nhow many are actually the same category?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#expected-insights",
    "href": "materials/sql/slides/session6.html#expected-insights",
    "title": "String Functions",
    "section": "Expected Insights",
    "text": "Expected Insights\nBy the end of this exercise, you should clearly see:\n\nthe same phone number appears in many formats\ncategory annotations fragment grouping\nGROUP BY blindly trusts text values\ndirty text leads to misleading KPIs",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session6.html#key-analytical-lesson",
    "href": "materials/sql/slides/session6.html#key-analytical-lesson",
    "title": "String Functions",
    "section": "Key Analytical Lesson",
    "text": "Key Analytical Lesson\n\ntext cleanliness is not cosmetic\nit directly affects:\n\ncounts\nrevenue aggregation\nuniqueness\n\ncleaning must be intentional and documented\n\nNext, we will clean this data and measure how KPIs change.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "String Functions"
    ]
  },
  {
    "objectID": "materials/sql/session1.html",
    "href": "materials/sql/session1.html",
    "title": "Session 01: Intro to Relational Databases",
    "section": "",
    "text": "Explain the difference between relational databases and non-relational databases\nExplain the importance of online analytical processing databases (OLAP) and relational database management systems (RDBMS)\nData Warehouse vs Data Mart vs Data Lake vs Data Lakehouse\nSet up a database environment using docker",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 01: Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/session1.html#learning-goals",
    "href": "materials/sql/session1.html#learning-goals",
    "title": "Session 01: Intro to Relational Databases",
    "section": "",
    "text": "Explain the difference between relational databases and non-relational databases\nExplain the importance of online analytical processing databases (OLAP) and relational database management systems (RDBMS)\nData Warehouse vs Data Mart vs Data Lake vs Data Lakehouse\nSet up a database environment using docker",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 01: Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/session1.html#introduction",
    "href": "materials/sql/session1.html#introduction",
    "title": "Session 01: Intro to Relational Databases",
    "section": "Introduction",
    "text": "Introduction\nNow, you‚Äôve reached the stage in the Program where you need to learn how to manage your data in an institutional way.\nTo do so, you‚Äôll need to know structured query language (SQL), the first language of:\n\nData analysts\nAnalytics Engineers\nData engineers\nData scientists.\n\n\n\n\n\n\n\nTipAbout the importance of SQL\n\n\n\nYou can‚Äôt call yourself a data analyst until you know the basics of SQL\n\nSQL is the language of Tables!\n\n\nBesides SQL, you‚Äôll explore various types of databases and what makes them different from Excel. During the rest of the session, you will get in-depth look at current, everyday practices in the field.\nSince the best way to learn SQL is to use it, you‚Äôll get hands-on tasks at the end of every Exercise.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 01: Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/session1.html#what-is-a-database",
    "href": "materials/sql/session1.html#what-is-a-database",
    "title": "Session 01: Intro to Relational Databases",
    "section": "What is a Database",
    "text": "What is a Database\nA database is a collection of stored data, usually organized in tables of rows and columns and managed by a database management system (DBMS). Analysts collectively refer to the data, the DBMS, and other related tools as the database.\nWhile there are several ways to store data in a database, the most common approach is to store the data in columns and rows. The intersection of each column and row is called a cell, and each cell represents a data element.\nA Data element could be:\n\nnumeric:\ntext\nyou name it‚Ä¶ \n\nTable: Example of rows, columns, and a highlighted cell\n\n\n\n\nCol A\nCol B\nCol C\n\n\n\n\nRow 1\nA1\nB1\nC1\n\n\nRow 2\nA2\nB2 (Cell)\nC2\n\n\nRow 3\nA3\nB3\nC3\n\n\n\nCell = intersection of Row 2 and Column B ‚Üí B2.\n\nSpreadsheets vs.¬†Databases\nDatabases differ from spreadsheets (excel-like tools) in several important ways, particularly in scalability, performance, and the ability to support multiple users. The key distinctions include:\n\nUser Access\n\nSpreadsheets are typically designed for single-user access.\nDatabases allow multiple users to access and manipulate the data simultaneously.\n\nData Capacity\n\nSpreadsheets handle relatively small datasets (usually up to a few hundred thousand rows).\nDatabases can efficiently store and process millions or even billions of records.\n\nPerformance\n\nSpreadsheets load the entire dataset into memory when opened.\nDatabases retrieve only the necessary records, improving speed and scalability.\n\nQuery Capability\n\nSpreadsheets require manual filtering, sorting, and counting.\nDatabases allow precise and fast queries using SQL.\n\n\nTo illustrate the difference, consider the following dataset of actors stored in a database table:\nTable: Example of Actor Records in a Database\n\n\n\nactor_id\nfirst_name\nlast_name\nlast_update\n\n\n\n\n1\nPenelope\nGuiness\n2013-05-26 14:47:57.62\n\n\n2\nNick\nWahlberg\n2013-05-26 14:47:57.62\n\n\n3\nEd\nChase\n2013-05-26 14:47:57.62\n\n\n4\nJennifer\nDavis\n2013-05-26 14:47:57.62\n\n\n5\nJohnny\nLollobrigida\n2013-05-26 14:47:57.62\n\n\n\n\nSpreadsheet vs.¬†Database Workflow\nWhen performing tasks such as counting how many actors have the first name ‚ÄúEd‚Äù, the difference between spreadsheets and databases becomes clear:\n\nUsing a Spreadsheet:\n\nYou must manually filter the first_name column to show only rows containing ‚ÄúEd.‚Äù\nAfter filtering, you manually count the matching rows or read the count displayed by the spreadsheet application.\nWith large datasets, loading, filtering, and recalculating can be slow and resource-intensive, causing delays and potential crashes.\nSpreadsheets can process only one query at a time, limiting efficiency.\n\nUsing a Database\n\nInstead of loading the entire dataset, you can execute a single SQL query such as:\nSELECT COUNT(*) \nFROM actor\nWHERE first_name = 'Ed';\nThe result is returned almost instantly, even with very large datasets.\nDatabases support multiple users running queries simultaneously without performance degradation.\nThey are optimized for complex, concurrent data manipulation and retrieval, making them more suitable for scalable and shared environments.\n\n\n\n\n\n\n\n\nNote\n\n\n\nOverall, spreadsheets may be adequate for small, single-user datasets, but databases provide far superior performance, scalability, and efficiency when working with larger datasets or multiple users.\n\n\n\n\nSpreadsheets vs.¬†Databases\n\n\n\n\n\n\n\nSPREADSHEET\nDATABASE\n\n\n\n\nDesigned for single-user access\nDesigned for multiple-user access\n\n\nOnly one user can manipulate the data at one point in time\nMultiple users can manipulate the data at one point in time\n\n\nHandles a limited amount of data\nHandles small to massive amounts of data\n\n\nBasic to moderate data operations\nBasic to complex data operations\n\n\nSlow or unable to manipulate, extract, transform, and aggregate large amounts of data\nFaster to manipulate, extract, transform, and aggregate large amounts of data\n\n\n\n\n\n\nStructured vs Unstructured Data\nBefore we learn about different types of databases, we should understand the distinction between them. Broadly speaking, these below terms refer to how data is organized:\n\nstructured\nunstructured\nsemi-structured data.\n\n\nStructured Data\nEach row in the table represents a distinct entity (in this case, an actor), while each column corresponds to a specific attribute such as the actor‚Äôs first name. The intersection of a row and a column forms a cell, and each cell contains a single data value.\nImportantly, every column is assigned a predefined data type, which determines the kind of information it can store. For example, the first_name column uses a text data type, meaning all values in this column must consist of alphabetical characters only.\n\nSample Data\n\n\n\nactor_id\nfirst_name\nlast_name\nlast_update\n\n\n\n\n1\nPenelope\nGuiness\n2013-05-26 14:47:57.62\n\n\n2\nNick\nWahlberg\n2013-05-26 14:47:57.62\n\n\n3\nEd\nChase\n2013-05-26 14:47:57.62\n\n\n4\nJennifer\nDavis\n2013-05-26 14:47:57.62\n\n\n5\nJohnny\nLollobrigida\n2013-05-26 14:47:57.62\n\n\n6\nBette\nNicholson\n2013-05-26 14:47:57.62\n\n\n7\nGrace\nMostel\n2013-05-26 14:47:57.62\n\n\n8\nMatthew\nJohansson\n2013-05-26 14:47:57.62\n\n\n9\nJoe\nSwank\n2013-05-26 14:47:57.62\n\n\n10\nChristian\nGable\n2013-05-26 14:47:57.62\n\n\n\n\n\n\nUnstructured Data\nUnstructured data refers to information that does not follow a predefined model or tabular format. Unlike structured data, it has no fixed rows, columns, or consistent organization. Common examples include:\n\nEmails\n\nText messages\n\nImages and graphics\n\nAudio and video files\n\nBecause it lacks a clear structure, unstructured data is more difficult to search, process, and analyze. Extracting useful information often requires significant manual effort or specialized tools. It can be compared to searching for every instance of a specific word in a book‚Äîpossible, but time-consuming and inefficient without the right indexing or processing methods.\n\n\n\nSemi-Structured Data\nSemi-structured data lies between structured and unstructured data. While it does not follow a rigid tabular format, it still maintains a discernible organizational framework, making it more flexible than strictly structured data.\nThis type of data can be analyzed effectively only when you understand the organizational rules embedded within it. A common example is HTML: although it is not stored in rows and columns, it contains predictable tags and nested elements that define the structure of the information. To locate specific content within an HTML file, you must first understand what each tag represents.\nSemi-structured data therefore offers:\n\nMore flexibility than structured tables\n\nMore organization than raw unstructured content\n\nBetter adaptability for evolving or complex data formats\n\n\n\nExamples of Each Data Type\n\n\n\n\n\n\n\nType\nExamples\n\n\n\n\nStructured Data\nCustomer tables, sales transactions, employee records, relational database tables (SQL).\n\n\nUnstructured Data\nEmails, PDFs, text documents, social media posts, images, videos, audio recordings.\n\n\nSemi-Structured Data\nHTML documents, JSON files, XML files, log files, key‚Äìvalue store outputs.\n\n\n\n\n\n\n\nIntro to SQL\nSQL (Structured Query Language) was first developed by IBM in the early 1970s and released publicly in 1979. It was later standardized by the American National Standards Institute (ANSI), establishing it as the primary language for interacting with relational databases.\nSQL is used for three core tasks:\n\nManipulating Data: inserting, updating, or deleting records within a database.\nSearching Data: retrieving all films that feature a specific actor (e.g., Ed Chase).\nDefining Database Structures: For instance, creating new tables or adding columns to existing ones.\n\nLike any language, SQL follows specific syntax rules.\n\nAn English sentence ends with a period.\n\nA SQL statement ends with a semicolon (;).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile the foundational syntax is consistent, slight variations may occur depending on the database system (e.g., PostgreSQL vs.¬†SQL Server). Understanding these conventions is essential before writing your own SQL queries.\n\n\n\n\n\n\n\n\n\nNoteIs SQL a Programing Language?\n\n\n\nSQL generally counts as code even though it‚Äôs not technically a programming language. You‚Äôll encounter a common phenomenon when you start working with it‚Äîyou‚Äôll find yourself in situations where you can‚Äôt solve a coding problem using only your existing knowledge. Don‚Äôt worry‚Äîthis happens to all beginners, and it‚Äôs something you‚Äôll have to learn to deal with.\nMost coders spend a fair amount of time researching answers to their problems online. There‚Äôs no need to get frustrated if you run into a brick wall‚Äîsimply Google what you‚Äôre trying to achieve in SQL (or any other language), and you may be surprised how quickly you‚Äôll find an answer to your problem!\n\n\n\n\nTypes of Databases\nAs you learn about SQL, you‚Äôll be working with a relational database, a common type of database that all analysts need to be familiar with. That said, relational databases aren‚Äôt the only type of database out there. Let‚Äôs look at the most important ones and their defining features:\n\nRelational Databases\nNoSQL Databases\nGraph Databases\n\n\n\n\n\n\n\n\nDatabases could also be centralized or distributed.\nDatabases could also be online analytical processing (OLAP) or online transaction processing (OLTP) system.\n\n\n\n\n\nRelational Databases\nA relational database is a collection of data stored in one or more tables of rows and columns.\nBetween the different tables, there are links, known as relationships. As you‚Äôve already learned, data analysts use SQL to query and manipulate the data in relational databases.\n\nPopular relational database management systems (RDBMS) include Microsoft SQL Server,MySQL, Oracle DB and PostgreSQL. Each of these RDBMS is owned by different companies:\n\nPostgreSQL: PostgreSQL Community\nMySQL: Oracle\nOracle: Oracle\nMSSQL: Microsoft\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nPostgreSQL\nSQL Server\nOracle\nMySQL\n\n\n\n\nLicense\nOpen-source\nCommercial + Express\nCommercial\nOpen-source (Oracle-owned)\n\n\nOS Support\nLinux, macOS, Windows\nWindows, Linux\nLinux, Unix, Windows\nLinux, Windows\n\n\nStrengths\nStandards compliant, JSONB, extensible\nEnterprise BI, T-SQL, Microsoft ecosystem\nScalability, security, enterprise features\nFast reads, simple web workloads\n\n\nBest Use Cases\nAnalytics, complex apps, data science\nBI, ERP, corporate apps\nFinance, telecom, large enterprises\nWeb apps, lightweight services\n\n\n\n\n\nNoSQL Databases\n\nNoSQL stands for Not Only SQL and refers to database systems that are generally non-relational, designed for flexible schemas and distributed workloads.\nSome NoSQL systems provide SQL-like query interfaces, but they do not follow the relational model.\n\nUnlike relational databases, NoSQL databases have a flexible data model, and they‚Äôre used to store unstructured, semi-structured, and even structured data. In general, you wouldn‚Äôt use SQL to access a NoSQL database, though some have SQL-like languages.\nAs web applications have become more complex, NoSQL databases have become popular thanks to their flexibility. NoSQL databases are used for applications that change frequently as new functions and features are added; for instance, the social networking site LinkedIn uses a NoSQL database.\nThus, NoSQL databases are non-relational systems designed for scalability, flexibility, and high-volume data. They fall into several major categories depending on their data model.\n\nKey‚ÄìValue Databases\n\nConcept: Simple key ‚Üí value storage.\nPopular systems: Redis, Amazon DynamoDB, Riak KV\nExample (Redis):\n\n{\n  'course':'Data Analytics',\n  'company':'ACA'\n}\n\n\n\n\nDocument Databases\n\nConcept: JSON-like documents with flexible schema.\nPopular systems: MongoDB, CouchDB, Firestore\nExample (MongoDB):\n\n{\n  \"user_id\": 1001,\n  \"name\": \"Maria\",\n  \"roles\": [\"admin\", \"manager\"]\n}\n\n\n\n\nColumn-Family (Wide-Column) Databases\n\nConcept: Data stored in column families across distributed nodes.\nPopular systems: Apache Cassandra, HBase, ScyllaDB\nExample (Cassandra CQL):\n\nINSERT INTO users (user_id, name, age)\nVALUES (1001, 'Maria', 33);\n\n\n\n\nGraph Databases\n\nConcept: Nodes and relationships.\nPopular systems: Neo4j, ArangoDB, Amazon Neptune\nExample (Neo4j Cypher):\n\nCREATE (u:User {name: \"Maria\"})\nCREATE (p:Project {title: \"Analytics App\"})\nCREATE (u)-[:LEADS]-&gt;(p);\n\n\n\n\nTime-Series Databases\n\nConcept: Optimized for timestamped metrics.\nPopular systems: InfluxDB, TimescaleDB, Prometheus\nExample (InfluxDB Line Protocol):\n\n\nMostly used among DevOps for monitoring  ‚Äî\n\n\nSearch Engine Databases\n\nConcept: Full-text indexing and search.\nPopular systems: Elasticsearch, OpenSearch, Apache Solr\nExample (Elasticsearch Query):\n\n{\n  \"query\": {\n    \"match\": {\n      \"message\": \"database error\"\n    }\n  }\n}\n\n\nMostly used for querying logs\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\nNoSQL Type\nKey Concept\nPopular Databases\n\n\n\n\nKey‚ÄìValue\nKey ‚Üí Value lookups\nRedis, DynamoDB, Riak\n\n\nDocument\nJSON-like documents\nMongoDB, CouchDB, Firestore\n\n\nColumn-Family\nWide-column storage\nCassandra, HBase, ScyllaDB\n\n\nGraph\nNodes + edges\nNeo4j, ArangoDB, Amazon Neptune\n\n\nTime-Series\nTime-stamped metrics\nInfluxDB, TimescaleDB, Prometheus\n\n\nSearch Engine\nText search and indexing\nElasticsearch, OpenSearch, Solr\n\n\n\n\n\n\n\n\n\nNoneReading\n\n\n\nSQL vs NoSQL\n\n\n\n\n\n\nCentralized vs Distributed Databases\nThe three kinds of databases discussed above can be centralized or distributed. Put simply, these concepts refer to where the database is stored.\n\n\n\n\nIllustration of centralized vs distributed databases\n\n\n\n\nCentralized Databases\nA centralized database runs on a single machine (single-node). All users, analysts, and administrators connect to this one server.\nKey Features:\n\nAll data lives in one powerful machine.\nEasy to manage and maintain.\nCan handle billions of records and terabytes of data.\nScaling requires buying larger, more expensive hardware (vertical scaling).\nOnce the system reaches hardware limits, performance and management become challenging.\n\nCommon in: Traditional enterprises, internal IT systems, on-premise setups.\nAnalogy: A workstation or laptop, but with high-end server specifications.\n\n\n\nDistributed Databases\nA distributed database stores data across multiple machines (nodes). These nodes work together to form one logical database.\n\nKey Features:\n\nDesigned for massive-scale data.\nScales horizontally by adding more machines.\nFaster data access and search due to parallel processing.\nMore cost-effective storage.\nHigher availability and fault tolerance.\n\n\n\nCommon in:\n\nBanking systems\n\nTelecommunications\n\nLarge tech companies (Google, Facebook, Amazon, Netflix)\n\n\n\nWhy it‚Äôs used?\nTo handle constantly growing data, high query loads, global users, and real-time applications.\n\n\n\nCentralized vs Distributed Databases: Summary\n\n\n\n\n\n\n\n\nFeature\nCentralized Database\nDistributed Database\n\n\n\n\nStorage location\nOne machine\nMany machines\n\n\nScalability\nVertical (add more power)\nHorizontal (add more nodes)\n\n\nTypical capacity\nHigh but limited by hardware\nExtremely high, virtually unlimited\n\n\nCost\nExpensive to scale\nMore cost-efficient\n\n\n\n\n\nOLAP Systems (Online Analytical Processing)\n\nOLAP systems are optimized for READ-heavy analytical workloads. They store large volumes of historical data and support complex queries, aggregations, and trend analysis.\nKey Features:\n\nDesigned primarily for reading and analyzing data.\nNot optimized for frequent inserts/updates/deletes.\nStores historical and summarized information.\nSupports advanced analytics: dashboards, reports, trends, forecasting.\nTypically powered by data warehouses or data marts.\n\nExample: An e-commerce analyst retrieves last year‚Äôs order data from a data warehouse and analyzes trends across months, categories, or customer segments.\nCommon Technologies: Amazon Redshift, Snowflake,BigQuery, Apache Hive, Microsoft SQL Server Analysis Services.\n\n\n\n\nOLTP Systems (Online Transaction Processing)\n\nOLTP systems are optimized for fast inserts, updates, and deletes. They handle high-volume transactional operations in real time.\nKey Features:\n\nProcesses large numbers of small, atomic transactions.\nEnsures consistency using ACID properties.\nOptimized for real-time operations.\nUsed by operational systems rather than analytics.\n\nExample: A bank‚Äôs ATM network continuously inserts, updates, or deletes records as customers make withdrawals, deposits, or transfers.\nCommon Technologies: PostgreSQL, MySQL, Oracle, SQL Server, MongoDB (as transactional store).\n\n\n\n\n\n\nImportantAbout ACID, BASE and CAP Theorem\n\n\n\nIn computer science, ACID (atomicity, consistency, isolation, durability)** is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction.\n\nThe opposite of ACID is BASE (basically available, soft state, and eventually consistent).\nIn ACID the whole transaction fails if an error occurs in any step within the transaction; in contrast, BASE databases prioritize availability over consistency: instead of failing the transaction, users can access inconsistent data temporarily: data consistency is achieved, but not immediately.\nIn distributed systems, trade-offs described by the CAP theorem mean that databases often lean toward ACID or BASE characteristics, depending on design and workload.\nModern systems may implement hybrid approaches, but trade-offs always exist. But it cannot be both according to CAP theorem.\nFor example, SQL databases (like MySQL, PostgreSQL, AWS RedShift) are structured over the ACID model, while NoSQL databases (like DynamoDB[4] or MongoDB) use the BASE architecture. However, some NoSQL databases might exhibit certain ACID traits\n\n\n\n\n\nOLAP vs OLTP: Summary\n\n\n\n\n\n\n\n\nFeature\nOLAP\nOLTP\n\n\n\n\nPrimary purpose\nAnalytics, reporting\nReal-time transactions\n\n\nQuery type\nRead-heavy, complex\nWrite-heavy, simple\n\n\nData volume\nHistorical, large datasets\nCurrent transactional data\n\n\nPerformance optimized for\nAggregations, scans\nFast inserts/updates/deletes\n\n\nCommon use cases\nDashboards, BI, trend analysis\nBanking, retail transactions, booking systems\n\n\nUsers\nAnalysts, data scientists\nEnd-users, applications, machines\n\n\n\n\n\n\n\n\n\n\nNoteReading\n\n\n\nOLTPvsOLAP",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 01: Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/session1.html#data-management-architectures",
    "href": "materials/sql/session1.html#data-management-architectures",
    "title": "Session 01: Intro to Relational Databases",
    "section": "Data Management Architectures",
    "text": "Data Management Architectures\nOkay, we got high level understanding about the databases and their types, however in real life applications we need to master the so called Data Management Architecture (DMA). Let‚Äôs go over them chronologically:\n\nData Warehouse (DW)\nData Mart (DM)\nData Lake (DL)\nData Lakehouse (DLH)\n\n\nData Warehouse (DW)\nThe earliest large-scale analytics storage (1980-90s).\nA Data Warehouse is a centralized, structured, and governed analytical storage designed for reporting, business intelligence, and decision-making. Everything inside is modeled, cleaned, validated, and optimized for SQL analytics.\n\n\nPurpose\n\nProvide a single version of truth for the organization.\nServe business reporting, dashboards, KPIs.\nAllow analysts to run consistent, repeatable metrics.\n\n\n\nArchitecture\n\nCore storage: relational tables in Star or Snowflake schema.\nETL/ELT pipelines transform raw data into structured models.\nStrong governance: data quality rules, validation layers, semantic models.\n\n\n\nSchema Approach\n\nSchema-on-write: data must be modeled before it enters.\nFacts (large numerical tables).\nDimensions (lookup tables with business attributes).\nConformed dimensions for uniform interpretation (e.g., Customer, Product).\n\n\n\nQuery Model\n\nAggregations, joins, slicing/dicing, reporting.\nOptimized for read-heavy workloads.\n\n\n\nAdvantages\n\nHigh data quality\nStrong governance\nFast SQL analytics\nBusiness-friendly, predictable models\n\n\n\nLimitations\n\nRigid: changes require redesign\nPoor for raw, unstructured data\nExpensive for large-scale storage\nNot ideal for ML pipelines that need raw historical data\n\n\n\nTypical Tools/Platforms\n\nSnowflake (warehouse mode)\nAmazon Redshift\nGoogle BigQuery (classic DW usage)\nTeradata\nOracle Exadata\n\n\n\nRelationship to others\n\nDW is the parent of DM.\nDW predates DL and DLH.\n\n\n\n\nData Mart (DM)\nIntroduced after Data Warehouses (late 1990s).\nA Data Mart is a subject-specific, department-focused subset of a Data Warehouse. It is optimized for a particular business domain such as Marketing, Finance, Sales, HR, or Operations.\n\n\nPurpose\n\nReduce load on the central Data Warehouse.\nDeliver faster, domain-specific analytics.\nAllow departments to have tailored KPIs and models.\nImprove performance by minimizing unnecessary joins and tables.\n\n\n\nArchitecture\n\nUsually built as dependent data marts, sourced from the DW.\nSometimes built as independent data marts (less common today).\nDomain-specific schemas (e.g., Marketing Mart, Finance Mart).\nCan use star/snowflake schemas but simplified for a business unit.\n\n\n\nSchema Approach\n\nSchema-on-write (same as DW).\nContains facts/dimensions relevant only to one business domain.\nMay extend or customize dimensions (e.g., Marketing_Customer).\n\n\n\nQuery Model\n\nFaster analytical queries for a single domain.\nReduced table complexity.\nSupports dashboards, KPI reporting, and departmental insights.\n\n\n\nAdvantages\n\nHigh performance for domain-specific workloads\nReduces contention on the central DW\nSimpler models for end users\nEnables data ownership by individual departments\n\n\n\nLimitations\n\nCan create data silos if unmanaged\nRequires synchronization with DW\nPotential duplication of logic if marts diverge\n\n\nData Silo\n\n\n\nTypical Tools/Platforms\n\nSnowflake Data Marts\nRedshift Data Marts\nBigQuery departmental datasets\nMicrosoft SQL Server departmental warehouses\n\n\n\nRelationship to others\n\nDM is the child of DW.\nRelies on DW for consistency.\nNot directly related to Data Lakes or Lakehouses except in hybrid architectures.\n\n\n\n\n\nData Lake (DL)\nIntroduced during the Big Data era (2010s).\nA Data Lake is a large-scale, low-cost, raw-data storage system that can ingest structured, semi-structured, and unstructured data without modeling.\n\n\nPurpose\n\nStore massive amounts of raw data cheaply.\nSupport machine learning and exploratory analytics.\nCapture logs, JSON files, clickstreams, IoT data, images, audio.\nPreserve granular, historical data with no loss.\n\n\n\nArchitecture\n\nBuilt on object storage (S3, GCS, ADLS, or HDFS).\nOrganized into zones: raw, curated, prepared.\nIngestion via ELT pipelines.\nOften uses frameworks like Spark, Hive, Presto, or Dremio.\n\n\n\nSchema Approach\n\nSchema-on-read: structure is applied only at query time.\nSupports open formats: Parquet, ORC, Avro, CSV, JSON.\nCan handle unstructured formats (text, logs, images).\n\n\n\nQuery Model\n\nFlexible but slower than DW for SQL analytics.\nBest for ML workloads requiring raw historical data.\nHandles batch and streaming ingestion.\n\n\n\nAdvantages\n\nExtremely flexible storage model\nVery low cost for large-scale data\nExcellent for ML, experimentation, and data science\nDecouples storage from compute\n\n\n\nLimitations\n\nNo ACID guarantees in traditional lakes\nPoor governance (risk of ‚Äúdata swamp‚Äù)\nHard for business teams to use directly\nNot optimized for BI dashboards\n\n\n\nTypical Tools/Platforms\n\nAWS S3 + Athena / EMR\nAzure Data Lake Storage\nGoogle Cloud Storage + BigQuery external tables\nHadoop HDFS\nDatabricks in lake mode\n\n\n\nRelationship to others\n\nDL evolved after DW and DM.\nPredecessor of the DLH.\nDW and DL often coexist in two-tier architectures.\n\n\n\n\nData Lakehouse (DLH)\nThe most modern architecture (2020s).\nA Data Lakehouse is a unified, ACID-compliant, analytics + ML architecture built on top of Data Lake storage.\nIt merges the strengths of Data Warehouses and Data Lakes into a single system.\n\n\nPurpose\n\nProvide a single platform for SQL analytics, BI, ML, and streaming.\nAvoid duplication between DW and DL.\nGuarantee ACID transactions and strong governance on cheap lake storage.\nSupport both structured and unstructured data.\n\n\n\nArchitecture\n\nStorage on cloud object stores (S3, ADLS, GCS).\nACID table formats: Delta Lake, Apache Iceberg, Apache Hudi.\nMetadata layers for schema enforcement, governance, versioning.\nCompute engines: Spark, Databricks, Presto/Trino, Snowflake, BigQuery.\n\n\n\nSchema Approach\n\nSupports both schema-on-write and schema-on-read.\nAllows schema evolution, time travel, and versioning.\nEnforces constraints even on raw lake storage.\n\n\n\nQuery Model\n\nHigh-performance SQL (warehouse-like).\nML workloads (lake-like).\nSupports batch, streaming, and real-time analytics.\nWorks for both BI dashboards and ML pipelines.\n\n\n\nAdvantages\n\nUnifies DW + DL into one architecture\nACID guarantees on object storage\nSuitable for both BI and ML\nSimplifies pipelines and governance\nScalable and cost-effective\n\n\n\nLimitations\n\nRequires organizational adoption and re-skilling\nGovernance maturity varies by platform\nClassic DW teams may struggle to transition\n\n\n\nTypical Tools/Platforms\n\nDatabricks Lakehouse (Delta)\nSnowflake (Unistore + Iceberg)\nBigQuery (Lakehouse-friendly design)\nApache Iceberg + Trino/Presto\nApache Hudi platforms\n\n\n\nRelationship to others\n\nDLH is the evolution of the Data Lake.\nDLH aims to unify and simplify analytics workloads.\nDLH may replace DW in modern architectures.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 01: Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/session1.html#building-our-first-database",
    "href": "materials/sql/session1.html#building-our-first-database",
    "title": "Session 01: Intro to Relational Databases",
    "section": "Building our first Database",
    "text": "Building our first Database\nNow that we have high level understanding about the databases, their types and architectures, it is high time to build our first database and continue.\nIn order to be able to build and run the database you need to keep docker desktop active. If you haven‚Äôt installed Docker yet, check out the installation and testing steps here\nPS you will see how easy it is.\n\nWhat are we going to achieve?\nWe are going to build a relational database which will consist of the following tables.\n\nSales\nTime Dimension\nProduct Dimension\nEmployee Dimension\nCustomer Dimension\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is going to be part of your portfolio, make sure the make it as structured as possible.\n\n\n\n\nStep 1: Create New GitHub Repository\n\nLog in to GitHub.\n\nClick New Repository.\n\nRepository name: sql-analytics-portfolio\n\nCheck Add a README file.\n\nCheck Add .gitignore and choose None.\n\nClick Create Repository.\n\n\n\nStep 2: Clone the Repository\n\nCreate a folder named aca on your Desktop or wherever you want\nOpen your terminal there. You should see ~/aca\nType in your terminal:\n\ngit clone &lt;your-repository-url&gt;\n\nNavigate to that folder by typing:\n\ncd  &lt;your-repository-url&gt;\n\ntype code . to open the project with VS Code\nRECOMMENDED save the project in VS Code using the Project Manager extension\n\n\n\nStep 3: Download the data\n\nCreate a new folder inside of the project named data\nDownload the following csv files and put in the data folder\n\ncustomers.csv\n\nemployees.csv\n\norders.csv\n\nproducts.csv\n\nsales.csv\n\nAdd and commit the changes done so far\n\ngit add .\ngit commit -m \"Added data folder with CSV datasets\"\n\n\nStep 4: Update .gitignore\n\nOpen the .gitignore file in your repository and add the following two lines at the bottom:\n\npgadmin_data/\npostgres_data/\n\nAgain add and commit\n\ngit add .gitignore\ngit commit -m \"Updated .gitignore to exclude database volumes\"\n\n\nStep 5: Creating Init folder and SQL files\nWe are going to create init files which will be executed only during the first db initialization.\n\nCreate a folder named init\nInside of the init folder create a file named 01_schema.sql for database schema code. Copy the respective chunk there\nInside of the init folder create a file named 02_etl.sql for data loading to SQL\nAdd and Commit the changes\n\n  git add init/\n  git commit -m \"Added initial schema and ETL SQL files\"\n\n01_schema.sql\n-- 01_schema.sql\n\n-- Safety: drop if you are iterating (comment these in production)\n-- DROP TABLE IF EXISTS sales CASCADE;\n-- DROP TABLE IF EXISTS orders CASCADE;\n-- DROP TABLE IF EXISTS products CASCADE;\n-- DROP TABLE IF EXISTS customers CASCADE;\n-- DROP TABLE IF EXISTS employees CASCADE;\n\nCREATE TABLE IF NOT EXISTS employees (\n  employee_id   SERIAL PRIMARY KEY,\n  first_name    TEXT,\n  last_name     TEXT,\n  email         TEXT,\n  salary        NUMERIC\n);\n\nCREATE TABLE IF NOT EXISTS customers (\n  customer_id   INTEGER PRIMARY KEY,\n  customer_name TEXT,\n  address       TEXT,\n  city          TEXT,\n  zip_code      TEXT\n);\n\nCREATE TABLE IF NOT EXISTS products (\n  product_id    INTEGER PRIMARY KEY,\n  product_name  TEXT,\n  price         NUMERIC,\n  description   TEXT,\n  category      TEXT\n);\n\n-- orders: include year/quarter/month as stored columns (loaded from CSV)\nCREATE TABLE IF NOT EXISTS orders (\n  order_id    INTEGER PRIMARY KEY,\n  order_date  TIMESTAMP,\n  year        INT,\n  quarter     INT,\n  month       TEXT\n);\n\nCREATE TABLE IF NOT EXISTS sales (\n  transaction_id INTEGER PRIMARY KEY,\n  order_id       INTEGER REFERENCES orders(order_id)     ON DELETE RESTRICT,\n  product_id     INTEGER REFERENCES products(product_id) ON DELETE RESTRICT,\n  customer_id    INTEGER REFERENCES customers(customer_id) ON DELETE RESTRICT,\n  employee_id    INTEGER REFERENCES employees(employee_id) ON DELETE SET NULL,\n  total_sales    NUMERIC,\n  quantity       INTEGER,\n  discount       NUMERIC\n);\n\n-- Helpful indexes\nCREATE INDEX IF NOT EXISTS idx_sales_order_id   ON sales(order_id);\nCREATE INDEX IF NOT EXISTS idx_sales_product_id ON sales(product_id);\nCREATE INDEX IF NOT EXISTS idx_sales_customer_id ON sales(customer_id);\nCREATE INDEX IF NOT EXISTS idx_orders_date      ON orders(order_date);\n\n\n02_etl.sql\n-- COPY must read files inside the container; we mounted ./data to /docker-entrypoint-initdb.d/data\n\\echo 'Loading employees...'\nCOPY employees(employee_id,first_name,last_name,email,salary)\nFROM '/docker-entrypoint-initdb.d/data/employees.csv'\nWITH (FORMAT csv, HEADER true);\n\n\\echo 'Loading customers...'\nCOPY customers(customer_id,customer_name,address,city,zip_code)\nFROM '/docker-entrypoint-initdb.d/data/customers.csv'\nWITH (FORMAT csv, HEADER true);\n\n\\echo 'Loading products...'\nCOPY products(product_id,product_name,price,description,category)\nFROM '/docker-entrypoint-initdb.d/data/products.csv'\nWITH (FORMAT csv, HEADER true);\n\n\\echo 'Loading orders...'\nCOPY orders(order_id,order_date,year,quarter,month)\nFROM '/docker-entrypoint-initdb.d/data/orders.csv'\nWITH (FORMAT csv, HEADER true);\n\n\\echo 'Loading sales...'\nCOPY sales(transaction_id,order_id,product_id,customer_id,employee_id,total_sales,quantity,discount)\nFROM '/docker-entrypoint-initdb.d/data/sales.csv'\nWITH (FORMAT csv, HEADER true);\n\n\n\n\n\n\nCaution\n\n\n\nPay attention to the orders and think about it.\n\n\n\n\n\nStep 6: Create .env file\nThis file is going to setup the database parameters\n\nCreate .env file in the root\nCopy and paste the following there\n\nPORT=5432\nDB_USER=admin\nDB_PASSWORD=password\nDB_NAME=aca\nPGADMIN_EMAIL=admin@admin.com\nPGADMIN_PASSWORD=admin\n\nAdd and commit .env file\n\n\ngit add .env\ngit commit -m \"Added environment variables for PostgreSQL\"\n\n\nStep 7: Create the docker-compose.yaml File\n\nCreate docker-compose.yaml file\ncopy and paste the following code there\n\nservices:\n  db:\n    container_name: postgresql_db_aca\n    image: postgres:17\n    restart: always\n    env_file: .env\n    environment:\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_DB: ${DB_NAME}\n    ports:\n      - \"5432:5432\"\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER} -d ${DB_NAME}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 10\n    volumes:\n      # Persist database data (keep your bind mount; a named volume also works)\n      - ./postgres_data:/var/lib/postgresql/data\n      # Run these once on first initialization\n      - ./init:/docker-entrypoint-initdb.d\n      # Make CSVs available to COPY during init (read-only)\n      - ./data:/docker-entrypoint-initdb.d/data:ro\n\n  pgadmin:\n    container_name: pgadmin_aca\n    image: dpage/pgadmin4\n    restart: always\n    env_file: .env\n    environment:\n      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}\n      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}\n    ports:\n      - \"5050:80\"\n    volumes:\n      - ./pgadmin_data:/var/lib/pgadmin\n    depends_on:\n      - db\n\nAdd and commit\n\n\ngit add docker-compose.yaml\ngit commit -m \"Added Docker Compose configuration for PostgreSQL and pgAdmin\"\n\nNow your repository must have the following structure\n\n.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore                     # Ignoring\n‚îú‚îÄ‚îÄ .env                     # Environment variables for PostgreSQL\n‚îú‚îÄ‚îÄ docker-compose.yaml      # Docker Compose configuration\n‚îú‚îÄ‚îÄ data/                    # CSV datasets\n‚îÇ   ‚îú‚îÄ‚îÄ customers.csv\n‚îÇ   ‚îú‚îÄ‚îÄ employees.csv\n‚îÇ   ‚îú‚îÄ‚îÄ orders.csv\n‚îÇ   ‚îú‚îÄ‚îÄ products.csv\n‚îÇ   ‚îú‚îÄ‚îÄ sales.csv\n‚îú‚îÄ‚îÄ init/                    # SQL initialization scripts\n‚îÇ   ‚îú‚îÄ‚îÄ 01_schema.sql        # Database schema creation\n‚îÇ   ‚îú‚îÄ‚îÄ 02_etl.sql           # Data loading and ETL process\n\n\nStep 8: Run the Docker Environment\n\nOpen the integrated terminal in VS Code.\nEnsure Docker Desktop is running on your machine.\nThen start the database services:\n\ndocker compose up\n\n\nStep 9: View the Tables\n\nOnce running, access pgAdmin in your browser by http://localhost:5050\nusername: admin@admin.com\npassword: admin\n\n\n\nOpen Object\\(\\rightarrow\\)Register\\(\\rightarrow\\)Server \nIn the general under the name column \nClick on the connection and fill in:\n\nhostname must be the same as the postgres service name in docker-compose.yaml, which is db,\nport 5432\nmaintenance database: aca\nusername: admin\npassword: password \n\nNavigate to in my case: \\(any_name \\rightarrow aca \\rightarrow Schemas \\rightarrow public \\rightarrow Tables\\)\n\nClick on the query tool copy and paste the queries provided in the Step 10\n\n\n\n\nStep 10: Our First Queries | Preview\nUse the following SQL statements to preview the first 10 rows from each table after loading your CSV data.\nA schema is a logical container inside a PostgreSQL database. You can think of it as a folder that holds database objects.\nThe default schema is public.\nSELECT * FROM public.customers LIMIT 10;\n\nSELECT * FROM public.employees LIMIT 10;\n\nSELECT * FROM public.orders LIMIT 10;\n\nSELECT * FROM public.products LIMIT 10;\n\nSELECT * FROM public.sales LIMIT 10;\n\n\nStep 11: Close and Delete Containers\nAs soon as you finish the job related to the DB, remove the containers by typing\ndocker-compose down\n\n\nStep 12: Every time you re-run\nWe are going to deactivate and activate constantly, so you will get used to it. Don‚Äôt worry :)\ndocker-compose up\n\n\n\n\n\n\nImportant\n\n\n\nIn case of ANY CHANGE in the docker-compose.yaml file:\n\ndocker compose down -v\ndelete postgres_data folder\ndocker compose up --build",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 01: Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/session6.html",
    "href": "materials/sql/session6.html",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "",
    "text": "Before jumping to the built-in SQL Funcions let‚Äôs ensure that we have sales_analysis table.\n\n\nLets run the Docker in detached mode:\ndocker compose up -d\n\n\n\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;\n\n\n\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);\n\n\n\n\n\n\nNote\n\n\n\nNote, that we are going to take closer look on JOINs during the next session.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#from-session-4",
    "href": "materials/sql/session6.html#from-session-4",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "",
    "text": "Before jumping to the built-in SQL Funcions let‚Äôs ensure that we have sales_analysis table.\n\n\nLets run the Docker in detached mode:\ndocker compose up -d\n\n\n\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;\n\n\n\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);\n\n\n\n\n\n\nNote\n\n\n\nNote, that we are going to take closer look on JOINs during the next session.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#text-functions-general-overview",
    "href": "materials/sql/session6.html#text-functions-general-overview",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "Text Functions | General Overview",
    "text": "Text Functions | General Overview\nText (string) functions are used to inspect, clean, standardize, and transform textual data at the row level.\nUnlike numeric aggregate functions (SUM, AVG, COUNT), most text functions:\n\noperate per row\nreturn derived text columns\ndo not reduce the number of rows on their own\n\nIn analytical workflows, text functions are typically applied before aggregation to ensure that grouping, counting, and summarization behave correctly.\n\n\nWhy Text Functions Matter in Analytics\nReal-world text data is often dirty, inconsistent, and ambiguous.\nCommon issues include:\n\ninconsistent casing (Electronics vs electronics)\nleading or trailing spaces\nannotations inside text (Category (Promo))\nmixed formats (ID-123, ID_123, 123)\nmissing or partially filled values\n\nIf these issues are not handled explicitly:\n\nGROUP BY produces fragmented categories\nCOUNT(DISTINCT ...) overcounts entities\nKPIs become inconsistent across dashboards\njoins silently fail\n\nText functions are therefore data quality tools, not cosmetic helpers.\n\n\n\nTypical Analytical Use Cases\nText functions are commonly used for:\n\ndata cleaning\n\ntrimming spaces\nnormalizing case\nremoving annotations\n\nstandardization\n\nmaking categories comparable\nensuring consistent grouping keys\n\nfeature engineering\n\nextracting parts of strings\nbuilding derived text features\n\nvalidation\n\ndetecting malformed values\nidentifying unexpected formats\n\n\n\n\nAnalytical Principle\nText transformations change how rows are grouped.\nBecause of this:\n\ntext cleaning is an analytical decision\nnot a purely technical step\nand must be done intentionally and consistently\n\nIn the following sections, we will cover built-in text functions one by one, using the same structured approach as in Session 05:\n\nwhat the function does\n\nwhy it matters analytically\n\ninput ‚Üí query ‚Üí output\n\ncommon pitfalls and best practices\n\nThis foundation will prepare you to build trustworthy categorical metrics and clean analytical dimensions using SQL.\n\n\nDummy Setup | Raw Phone Numbers and Names\nTo demonstrate text normalization beyond phone numbers, we extend the dummy table to include first name and last name fields with inconsistent capitalization and formatting.\n\nThis allows us to later apply functions such as UPPER(), LOWER(), INITCAP(), and validation logic on names.\nDROP TABLE IF EXISTS customers_raw_text;\n\nCREATE TABLE customers_raw_text (\n  customer_id   INTEGER,\n  first_name    TEXT,\n  last_name     TEXT,\n  raw_phone     TEXT,\n  category_raw  TEXT,\n  birth_date    DATE\n);\n\nINSERT INTO customers_raw_text (\n  customer_id,\n  first_name,\n  last_name,\n  raw_phone,\n  category_raw,\n  birth_date\n) VALUES\n  (1, 'joHN',     'doE',        '   077600945  ',   'Accessories (Promo)', DATE '1994-03-12'),\n  (2, 'MARY',     'sMiTh',      '077-600-045',      'Electronics (Old)',   DATE '1988-11-05'),\n  (3, 'aLEx',     'johnSON',    '(374)-77-600-945', 'Accessories',         DATE '2001-07-23'),\n  (4, 'anna',     'VAN DYKE',   '37477600945',      'Electronics (Promo)', DATE '1999-01-30'),\n  (5, NULL,       'brOwn',      '77600945',         'Accessories (Test)',  DATE '1994-03-12');\nWhat this setup gives us\n\nphone numbers with inconsistent formatting\n\nnames with:\n\nrandom capitalization\n\nmixed casing within the same value\n\nmulti-part last names\n\nunstructured category\n\n\nThis mirrors real-world dirty text data and prepares us for the next set of string functions focused on standardization and normalization.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#length",
    "href": "materials/sql/session6.html#length",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "LENGTH()",
    "text": "LENGTH()\nText fields such as phone numbers often arrive in many inconsistent formats.\nBefore attempting to clean or standardize them, a good analyst first tries to detect which rows are malformed.\nThe LENGTH() function is a simple but powerful diagnostic tool for this purpose.\n\nInspecting the Lengths\nThe first question to ask is:\n\nDo all values have the same length?\n\nSELECT\n  raw_phone,\n  LENGTH(raw_phone) AS phone_length\nFROM customers_raw_text;\n\nOutput\n\n\n\nraw_phone\nphone_length\n\n\n\n\n'   077600945  '\n13\n\n\n'077600945'\n9\n\n\n'77600945'\n8\n\n\n'077-600-045'\n11\n\n\n'(374)-77-600-945'\n15\n\n\n'37477600945'\n11\n\n\n\n\n\n\nInterpretation\nEven before cleaning, we can already classify rows:\n\n8 characters: likely already standardized ‚Üí 77600945\n9 characters likely correct number with leading zero ‚Üí 077600945\n11 characters likely contains country code or separators\n13+ characters clearly contains whitespace and formatting symbols\n\nAt this point:\n\nspaces\n\nhyphens\n\nparentheses\n\ncountry prefixes\n\n\n\nWhy This Step Is Critical\nBy using LENGTH() before cleaning, you can:\n\ndetect formatting issues early\n\nidentify multiple data quality patterns\n\navoid applying a one-size-fits-all transformation\n\ndesign targeted cleaning rules\n\nThis avoids silent errors later when:\n\ngrouping\n\njoining\n\ndeduplicating\n\n\n\n\nAnalytical Insight\nNotice that:\n\nLENGTH() alone already tells us which rows cannot possibly match the target\nthe target 77600945 has a clear expected length\n\nThis gives us a validation rule we can enforce after cleaning.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#trim",
    "href": "materials/sql/session6.html#trim",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "TRIM()",
    "text": "TRIM()\nText fields often contain invisible formatting noise, such as leading or trailing spaces.\nAlthough these spaces are not obvious when visually inspecting data, they can:\n\nbreak joins\n\nfragment groups\n\ncause false duplicates\n\ninvalidate validation rules\n\nThe TRIM() function is used to remove unnecessary whitespace from the beginning and end of text values.\nImportantly, TRIM() removes noise, not meaning.\n\n\nWhy TRIM() Matters in Analytics\nWhitespace has no business semantics.\nThat means:\n\nremoving it does not change the underlying value\nbut failing to remove it can distort analytical results\n\nBecause of this, TRIM() is considered a safe and low-risk transformation and is usually applied early in data-cleaning pipelines.\n\n\n\nApplying TRIM()\nBelow we apply TRIM() to raw phone numbers and compare lengths before and after trimming.\nSELECT\n  raw_phone,\n  LENGTH(raw_phone) AS length,\n  TRIM(raw_phone) AS trimmed_phone,\n  LENGTH(TRIM(raw_phone)) AS trimmed_length\nFROM customers_raw_text;\n\n\n\nResult Interpretation\nFrom the output, we observe that:\n\nleading and trailing spaces are removed\n\ninternal characters (digits, hyphens, parentheses) remain unchanged\n\nsome rows move closer to the expected target length\n\nothers still violate formatting rules\n\nThis confirms that:\n\nwhitespace was only one of several issues\nfurther transformations are required for full standardization\n\n\n\n\nDirectional Variants\nSQL also provides directional trimming functions:\nLTRIM(raw_phone)   -- removes leading spaces only\nRTRIM(raw_phone)   -- removes trailing spaces only\nUsing TRIM() is equivalent to applying both.\n\n\n\nAnalytical Best Practice\nAlways follow this pattern:\n\nMeasure (e.g., LENGTH())\nTransform (e.g., TRIM())\nRe-measure to validate impact\n\nThis makes transformations:\n\ntransparent\n\nauditable\n\nanalytically defensible\n\n\n\n\n\n\n\nImportant\n\n\n\n\nTRIM() removes invisible formatting noise\n\nit does not alter business meaning\n\nit prepares text data for structural cleaning\n\nit should almost always be applied before more aggressive text transformations\n\nWhitespace issues are subtle, but ignoring them leads to silent analytical errors.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#lower-upper-initcap",
    "href": "materials/sql/session6.html#lower-upper-initcap",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "LOWER(), UPPER(), INITCAP()",
    "text": "LOWER(), UPPER(), INITCAP()\nText values often differ only by capitalization, even though they represent the same entity.\nFor example:\n\njohn, John, JOHN\nvan dyke, Van Dyke, VAN DYKE\n\nFrom an analytical perspective, these differences are purely cosmetic, but if left untreated they lead to:\n\nfragmented groups\n\ninflated COUNT(DISTINCT ...)\n\nfailed joins\n\ninconsistent reporting\n\nSQL provides several functions to normalize text case.\n\n\nWhy Case Normalization Matters\nCapitalization has no business meaning, but SQL treats differently cased strings as different values.\nThat means:\n\n'john' ‚â† 'John' ‚â† 'JOHN'\ngrouping becomes unreliable\ndeduplication becomes inaccurate\n\nCase normalization is therefore a data quality operation, not a formatting preference.\n\n\n\nLOWER() | Normalize to Lowercase\nLOWER() converts all characters in a string to lowercase.\nThis is the most common choice for:\n\ngrouping keys\n\njoins\n\ndeduplication\n\ncategorical dimensions\n\nSELECT\n  first_name,\n  LOWER(first_name) AS first_name_lower\nFROM customers_raw_text;\nWhen to Use\n\ninternal identifiers\n\ngrouping and joins\n\ntext comparison logic\n\n\n\n\nUPPER() | Normalize to Uppercase\nUPPER() converts all characters in a string to uppercase.\nThis is often used for:\n\ncodes\n\nabbreviations\n\ncountry or region identifiers\n\nSELECT\n  last_name,\n  UPPER(last_name) AS last_name_upper\nFROM customers_raw_text;\nWhen to Use\n\nstandardized codes\n\nreporting conventions\n\nlegacy system alignment\n\n\n\n\nINITCAP() | Proper Name Formatting\nINITCAP() converts text to title case:\n\nfirst letter uppercase\n\nremaining letters lowercase\n\nSELECT\n  first_name,\n  INITCAP(first_name) AS first_name_clean\nFROM customers_raw_text;\nExample Transformations\n\n\n\nraw value\nINITCAP result\n\n\n\n\njohn\nJohn\n\n\nmARy\nMary\n\n\nvan dyke\nVan Dyke\n\n\n\n\n\n\nAnalytical Comparison\n\n\n\nFunction\nBest Use Case\n\n\n\n\nLOWER()\njoins, grouping, deduplication\n\n\nUPPER()\ncodes, abbreviations\n\n\nINITCAP()\npresentation, names\n\n\n\n\n\n\nAnalytical Best Practice\nA common and robust pattern is:\n\nuse LOWER() for keys and joins\nuse INITCAP() for display purposes\nnever mix raw and normalized text in analysis\n\nCase normalization:\n\nchanges how rows group\naffects counts and uniqueness\nmust be applied consistently\n\nLike TRIM(), these functions remove noise, not meaning.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#replace",
    "href": "materials/sql/session6.html#replace",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "REPLACE()",
    "text": "REPLACE()\nAfter handling whitespace (TRIM()) and capitalization (LOWER(), UPPER(), INITCAP()), the next common problem in real-world text data is structural noise inside strings.\nStructural noise refers to characters that are part of visual formatting but carry no analytical meaning.\nTypical examples include:\n\nhyphens in phone numbers\n\nspaces used as separators\n\nparentheses around country codes\n\ndots, slashes, or underscores\n\nIf these characters are not removed, text values that represent the same entity remain incomparable.\n\n\nWhy REPLACE() Matters in Analytics\nREPLACE() allows you to explicitly remove or substitute known characters.\nFrom an analytical perspective:\n\nthe rule is deterministic\nthe transformation is transparent\nthe outcome is fully explainable\n\nThis makes REPLACE() suitable for early-stage structural cleanup, before applying more advanced pattern-based logic.\n\n\n\nBasic Usage\nThe REPLACE() function has the following structure:\nREPLACE(text, old_value, new_value)\nIt replaces all occurrences of old_value with new_value.\n\n\n\nRemoving Known Separators\nPhone numbers often include hyphens for readability.\nSELECT\n  raw_phone,\n  REPLACE(raw_phone, '-', '') AS phone_no_hyphen\nFROM customers_raw_text;\nConceptual Effect\n\n\n\nraw_phone\nphone_no_hyphen\n\n\n\n\n077-600-045\n077600045\n\n\n(374)-77-600-945\n(374)77600945\n\n\n\nOnly the hyphen is removed.\nAll other characters remain untouched.\n\n\n\nChaining REPLACE() Calls\nWhen multiple unwanted characters are present, REPLACE() calls can be nested.\nSELECT\n  raw_phone,\n  REPLACE(\n    REPLACE(\n      REPLACE(TRIM(raw_phone), '-', ''),\n    '(', ''),\n  ')', '') AS phone_partial_clean\nFROM customers_raw_text;\nThis removes:\n\nleading and trailing spaces\n\nhyphens\n\nparentheses\n\nResulting Pattern\n\n\n\nraw_phone\nphone_partial_clean\n\n\n\n\n(374)-77-600-945\n37477600945\n\n\n077-600-045\n077600045\n\n\n\nAt this stage, formatting noise is reduced, but the value is still not standardized.\n\n\n\nAnalytical Limitations of REPLACE()\nWhile REPLACE() is useful, it has important limitations:\n\nit works only on explicit characters\nit does not understand patterns\nit cannot validate correctness\nrules must be hard-coded\n\nFor example:\n\nit cannot remove ‚Äúeverything that is not a digit‚Äù\nit cannot enforce length rules\nit cannot adapt to new formats\n\nBecause of this, REPLACE() is rarely the final cleaning step.\n\n\n\nWhen to Use REPLACE()\nUse REPLACE() when:\n\nthe unwanted character is known\n\nformatting rules are simple\n\ntransformations must be transparent\n\nAvoid relying on REPLACE() when:\n\nformats vary widely\n\npatterns are unknown\n\nvalidation is required\n\n\n\n\n\n\n\nWarningAnalytical Warning | Hard-Coded Assumptions\n\n\n\nEvery REPLACE() call encodes an assumption:\n\nwhich characters are noise\n\nwhich characters are meaningful\n\nwhat happens if formats change\n\nThese assumptions must be:\n\ndocumented\n\nreviewed\n\njustified by business logic",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#regexp_replace",
    "href": "materials/sql/session6.html#regexp_replace",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "REGEXP_REPLACE()",
    "text": "REGEXP_REPLACE()\nAfter handling explicit characters with REPLACE(), we move to pattern-based cleaning.\nReal-world text often contains unknown or variable noise.\nIn such cases, hard-coded replacements do not scale.\nREGEXP_REPLACE() allows you to define rules, not characters.\n\n\nWhy REGEXP_REPLACE() Matters in Analytics\nRegular expressions let you describe what to keep or what to remove, rather than listing every unwanted symbol.\nFrom an analytical perspective, this means:\n\nfewer hard-coded assumptions\n\nbetter generalization to unseen formats\n\ncleaner, more robust standardization rules\n\nThis is especially important for identifiers such as:\n\nphone numbers\n\nIDs\n\ncodes\n\nreference keys\n\n\n\n\nREGEXP_REPLACE() | Syntax Overview\nREGEXP_REPLACE(text, pattern, replacement [, flags])\nKey components:\n\ntext ‚Üí the input string\n\npattern ‚Üí a regular expression\n\nreplacement ‚Üí what to substitute\n\nflags ‚Üí optional modifiers (e.g.¬†global replacement)\n\n\n\n\nExtracting Digits Only\nA very common analytical task is to remove everything except digits.\nThe regex pattern:\n[^0-9]\n\\[\\Downarrow\\]\n\n[] \\(\\rightarrow\\) character set\n\n^ \\(\\rightarrow\\) NOT\n\n0-9 \\(\\rightarrow\\) digits\n\nSo the pattern matches any non-digit character.\n\n\nQuery | Digits-Only Cleaning\nLet‚Äôs run the bellow code\nSELECT\n  raw_phone,\n  REGEXP_REPLACE(raw_phone, '[^0-9]', '', 'g') AS digits_only\nFROM customers_raw_text;\n\n\n'[^0-9]': regular expression pattern that defines what to match.\n'': Replacement\n'g':Global Flag, The 'g' flag stands for global replacement. Without 'g', only the first match would be replaced\n\n\n\n\n\n\n\nNoteExperiment‚Ä¶\n\n\n\nFeel free to experiment as much as you need!\n\n\n\n\nOutput | Conceptual Output\nThe above query would return\n\n\n\nraw_phone\ndigits_only\n\n\n\n\n'   077600945  '\n077600945\n\n\n'077600945'\n077600945\n\n\n'77600945'\n77600945\n\n\n'077-600-045'\n077600045\n\n\n'(374)-77-600-945'\n37477600945\n\n\n'37477600945'\n37477600945\n\n\n\n\\[\\Downarrow\\]\n\nall formatting symbols are removed\n\nonly numeric content remains\n\nlengths still vary\n\n\n\n\n\n\n\nImportantPrimary Goal‚Ä¶\n\n\n\nThe goal here is not to memorize regex syntax, but to recognize reusable analytical patterns.\n\n\n\n\n\nRemoving Annotations Inside Parentheses\nText fields often contain annotations or comments that should not be part of the analytical key\nRecall our dummy table:\n\n\n\n\n\n\n\n\n\ncustomer_id\nraw_phone\ncategory_raw\nbirth_date\n\n\n\n\n1\n077600945\nAccessories (Promo)\n1994-03-12\n\n\n2\n077-600-045\nElectronics (Old)\n1988-11-05\n\n\n3\n(374)-77-600-945\nAccessories\n2001-07-23\n\n\n4\n37477600945\nElectronics (Promo)\n1999-01-30\n\n\n5\n77600945\nAccessories (Test)\n1994-03-12\n\n\n\nText fields often include annotations inside parentheses that are useful for humans but harmful for analytics.\nIn analytics, these annotations usually break grouping and counting.\nExamples:\n\nAccessories (Promo)\nElectronics (Old)\nGold Customer (Test)\n\nFrom an analytical standpoint, these annotations:\n\nfragment categories\n\nbreak grouping logic\n\ninflate distinct counts\n\n\nRegex Pattern 1 | Remove Everything Inside ()\nThe following pattern removes:\n\nthe opening parenthesis (\n\neverything inside\n\nthe closing parenthesis )\n\n\\([^)]*\\)\nPattern breakdown:\n\n\\( ‚Üí literal opening parenthesis\n\n[^)]* ‚Üí any characters except ) (zero or more)\n\n\\) ‚Üí literal closing parenthesis\n\n\nQuery | Cleaning Categories\nSELECT\n    category_raw,\n    REGEXP_REPLACE(category_raw, '\\([^)]*\\)', '', 'g')\n   AS category_clean1\nFROM customers_raw_text;\n\n\nOutput | Cleaning Categories\n\n\n\ncategory_raw\ncategory_clean1\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories\n\n\n\n\n\n\n\n\n\nCautionWhitespaces\n\n\n\nCheck the whitespaces!\n\n\n\n\n\nRegex Pattern 2 | Remove Parentheses Content + TRIM Whitespaces\nAfter removing annotations inside parentheses, trailing whitespaces often remain.\nThis leads to categories that look identical but fail equality checks.\nTo fix this, we chain TRIM() after REGEXP_REPLACE().\nTRIM(REGEXP_REPLACE(category_raw, '\\([^)]*\\)', '', 'g'))\nWhat this does:\n\nremoves everything inside ()\n\nremoves the parentheses themselves\n\ntrims leading and trailing spaces\n\n\n\nQuery | Cleaning Categories + TRIM\nSELECT\n    category_raw,\n    TRIM(\n      REGEXP_REPLACE(category_raw, '\\([^)]*\\)', '', 'g')\n    ) AS category_clean2\nFROM customers_raw_text;\n\n\n\nOutput | Cleaning Categories (After TRIM)\n\n\n\ncategory_raw\ncategory_clean2\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories\n\n\n\n\n\n\n\n\n\n\nNoteWhy TRIM Matters\n\n\n\nWithout TRIM(), values like 'Accessories ' and 'Accessories'\nwould be treated as different categories during grouping.\n\n\n\n\n\n\nRegex Pattern 3 | Remove Parentheses + Leading Space in One Step\nInstead of chaining functions, we can encode whitespace logic directly into the regex.\n\\s*\\(.*?\\)\nPattern breakdown:\n\n\\s* ‚Üí zero or more whitespace characters\n\n\\( ‚Üí opening parenthesis\n\n.*? ‚Üí any characters (non-greedy)\n\n\\) ‚Üí closing parenthesis\n\nThis pattern removes:\n\nthe parentheses\n\nthe annotation\n\nany space before the parenthesis\n\n\n\nQuery | Single-Step Category Cleaning\nSELECT\n    category_raw,\n    REGEXP_REPLACE(category_raw, '\\s*\\(.*?\\)', '', 'g')\n      AS category_clean3\nFROM customers_raw_text;\n\n\n\nOutput | Cleaning Categories (Single Regex)\n\n\n\ncategory_raw\ncategory_clean3\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories\n\n\n\n\n\n\n\nComparison Summary\n\n\n\nApproach\nResult\nNotes\n\n\n\n\nPattern 1\nAccessories\nLeaves trailing spaces\n\n\nPattern 2\nAccessories\nSafe and explicit\n\n\nPattern 3\nAccessories\nCompact, regex-heavy\n\n\n\n\n\n\n\n\n\nWarningAnalytical Guidance\n\n\n\n\nGo over the Pattern 1 when learning, teaching or collaborating\nPrefer Pattern 2 when teaching or collaborating\n\nPrefer Pattern 3 when rules are stable and well-documented\n\nAlways validate results with COUNT(DISTINCT ...) after cleaning\n\nRegex-based cleaning directly impacts grouping, KPIs, and dashboards.\n\n\n\n\n\nREGEXP vs REPLACE | When to Choose Which\n\n\n\nScenario\nPrefer\n\n\n\n\nKnown characters only\nREPLACE()\n\n\nVariable formatting\nREGEXP_REPLACE()\n\n\nValidation and extraction\nREGEXP_REPLACE()\n\n\nTransparency and simplicity\nREPLACE()\n\n\n\n\n\n\nAnalytical Warning\n\n\n\n\n\n\nWarningAnalytical Warning | Regex Is Powerful but‚Ä¶\n\n\n\nRegular expressions are powerful but dangerous if misused.\n\noverly broad patterns can remove valid information\n\nunreadable regex becomes technical debt\n\nrules must be documented and tested\n\nAlways combine regex cleaning with measurement and validation.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#substring",
    "href": "materials/sql/session6.html#substring",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "SUBSTRING()",
    "text": "SUBSTRING()\nAfter cleaning whitespace, case, and structural noise, the next common analytical task is extracting meaningful parts of text.\nSUBSTRING() allows you to select a specific portion of a string based on position or pattern.\nThis is essential when a single text field contains multiple pieces of information.\nUnlike REPLACE() or REGEXP_REPLACE(), which remove noise,\nSUBSTRING() is used to isolate signal.\n\n\nWhy SUBSTRING() Matters in Analytics\nMany real-world text fields are compound fields, for example:\n\nphone numbers with prefixes\ncodes with embedded meaning\nidentifiers with fixed segments\ndates or versions encoded in strings\n\nFrom an analytical perspective, this means:\n\ngrouping requires extracted keys\njoins depend on consistent substrings\nvalidation depends on expected positions\n\n\n\n\nSUBSTRING() | Basic Positional Extraction\nThe most common form of SUBSTRING() uses starting position and length.\nSUBSTRING(text FROM start_position FOR length)\n\n\n\nExample 1 | Extract Last 8 Digits of Phone Numbers\nAssume we have already removed non-digit characters and want to extract the core phone number.\nInput\n\n\n\nraw_phone\n\n\n\n\n077600945\n\n\n37477600945\n\n\n77600945\n\n\n\n\\[\\downarrow\\]\nSELECT\n  raw_phone,\n  SUBSTRING(raw_phone FROM LENGTH(raw_phone) - 7 FOR 8) AS phone_core\nFROM customers_raw_text;\n\\[\\downarrow\\]\nOutput\n\n\n\nraw_phone\nphone_core\n\n\n\n\n077600945\n77600945\n\n\n37477600945\n77600945\n\n\n77600945\n77600945\n\n\n\n\n\nAnalytical Interpretation\nThis logic:\n\nassumes the last 8 digits uniquely identify the phone number\nremoves country codes and prefixes\ncreates a standardized join key\n\nThis pattern is extremely common in:\n\ntelecom analytics\ncustomer deduplication\nidentity resolution\n\n\n\n\n\nExample 2 | SUBSTRING() with Fixed Structure\nWhen text follows a known fixed format, SUBSTRING() is deterministic and fast.\nSELECT\n  category_raw,\n  SUBSTRING(category_raw FROM 1 FOR 11) AS category_prefix\nFROM customers_raw_text;\n\\[\\rightarrow\\]\n\n\n\ncategory_raw\ncategory_prefix\n\n\n\n\nAccessories (Promo)\nAccessories\n\n\nElectronics (Old)\nElectronics\n\n\nAccessories\nAccessories\n\n\nElectronics (Promo)\nElectronics\n\n\nAccessories (Test)\nAccessories\n\n\n\nThis is useful when:\n\nnaming conventions are enforced\nformats are guaranteed upstream\n\n\n\nExample 3 | Extract Digits Using Regex\nPostgreSQL also supports regex-based substring extraction.\nSUBSTRING(text FROM pattern)\n\nSELECT\n  raw_phone,\n  SUBSTRING(raw_phone FROM '[0-9]+') AS first_digit_sequence\nFROM customers_raw_text;\nThis extracts the first continuous digit block.\n\\[\\downarrow\\]\n\n\n\nraw_phone\nfirst_digit_sequence\n\n\n\n\n077600945\n077\n\n\n077-600-045\n077\n\n\n(374)-77-600-945\n374\n\n\n37477600945\n37477600945\n\n\n77600945\n77600945\n\n\n\n\n\n\nPositional vs Pattern-Based SUBSTRING()\n\n\n\nUse Case\nPrefer\n\n\n\n\nFixed-length identifiers\nPositional\n\n\nVariable formats\nRegex-based\n\n\nPerformance-critical logic\nPositional\n\n\nUnknown structure\nRegex-based\n\n\n\n\n\n\nAnalytical Best Practice\n\nuse positional SUBSTRING() when formats are stable\nuse regex SUBSTRING() when formats vary\nalways validate results with:\n\nLENGTH()\nCOUNT(DISTINCT ...)\n\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Hidden Assumptions\n\n\n\nEvery SUBSTRING() encodes assumptions about:\n\nstring length\nformat stability\nsemantic meaning of positions\n\nIf formats change upstream, SUBSTRING() can silently break logic.\nAlways document:\n\nwhy a position was chosen\nwhat happens if length changes\n\n\n\nSUBSTRING() is a powerful extraction tool, but only when its assumptions are explicit and validated.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#concat-and",
    "href": "materials/sql/session6.html#concat-and",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "CONCAT() and ||",
    "text": "CONCAT() and ||\nAfter extracting and cleaning text (TRIM(), LOWER(), INITCAP(), SUBSTRING()), a very common analytical task is to construct new text values from multiple columns.\nThis is where CONCAT() and the string concatenation operator || are used.\nThey allow you to build full identifiers, labels, and display fields from atomic components.\n\n\nWhy CONCAT() Matters in Analytics\nIn real analytical datasets, information is often split across columns:\n\nfirst name + last name\n\ncity + zip code\n\ncategory + subcategory\n\nyear + quarter\n\nFrom an analytical perspective, this means:\n\nreporting often requires combined fields\njoins may rely on constructed keys\ndashboards need human-readable labels\n\nConcatenation is therefore a feature engineering operation, not just formatting.\n\n\nCONCAT() | Syntax Overview\nCONCAT(value1, value2, ..., valueN)\nKey properties:\n\naccepts multiple arguments\ntreats NULL as an empty string\nalways returns text\n\nPostgreSQL also supports the operator form:\nvalue1 || value2\n\n\n\nExample 1 | Building Full Names (Raw)\nAssume we have separate first_name and last_name columns.\nSELECT\n  customer_id,\n  first_name,\n  last_name,\n  CONCAT(first_name, ' ', last_name) AS full_name\nFROM customers_raw_text;\n\n\n\nOutput\n\n\n\ncustomer_id\nfirst_name\nlast_name\nfull_name\n\n\n\n\n1\njohn\ndoe\njohn doe\n\n\n2\nMARY\nSMITH\nMARY SMITH\n\n\n3\naLEx\njohnson\naLEx johnson\n\n\n4\nanna\nVAN DYKE\nanna VAN DYKE\n\n\n5\nNULL\nbrown\nbrown\n\n\n\n\n\nExample 2 | CONCAT() + INITCAP() (Recommended Pattern)\nA very common and safe analytical pattern is:\n\nnormalize case\n\nthen concatenate\n\nSELECT\n  customer_id,\n  INITCAP(first_name) AS first_name_clean,\n  INITCAP(last_name)  AS last_name_clean,\n  INITCAP(CONCAT(\n    first_name,\n    ' ',\n    last_name\n  )) AS full_name_clean\nFROM customers_raw_text;\n\n\n\nOutput\n\n\n\ncustomer_id\nfirst_name_clean\nlast_name_clean\nfull_name_clean\n\n\n\n\n1\nJohn\nDoe\nJohn Doe\n\n\n2\nMary\nSmith\nMary Smith\n\n\n3\nAlex\nJohnson\nAlex Johnson\n\n\n4\nAnna\nVan Dyke\nAnna Van Dyke\n\n\n5\nNULL\nBrown\nBrown\n\n\n\n\n\n\nCONCAT() vs ||\nThe same result can be achieved using the operator form.\nSELECT\n  customer_id,\n  INITCAP(first_name) || ' ' || INITCAP(last_name) AS full_name_clean\nFROM customers_raw_text;\n\n\n\nKey Difference\n\nCONCAT() treats NULL as empty\n\n|| returns NULL if any operand is NULL\n\nThis difference is analytically important.\n\nuse CONCAT() when NULL values are possible\n\nuse || when you want NULL propagation\n\nalways normalize text before concatenation\n\nvalidate results with COUNT(DISTINCT ...)\n\n\n\n\n\n\n\nWarningAnalytical Warning | Constructed Fields\n\n\n\nConcatenated fields:\n\noften look harmless\n\nbut frequently become join keys or grouping dimensions\n\nIf constructed inconsistently, they lead to:\n\nduplicate entities\n\nbroken joins\n\nmisleading KPIs\n\nAlways document:\n\nconstruction logic\n\nordering\n\ncasing rules\n\n\n\nCONCAT() is a simple function, but it plays a critical role in building reliable analytical dimensions.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#position-strpos",
    "href": "materials/sql/session6.html#position-strpos",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "POSITION() / STRPOS()",
    "text": "POSITION() / STRPOS()\nThe next analytical task is often to locate where something appears inside a string.\nThis is exactly what POSITION() and its PostgreSQL-specific alias STRPOS() do.\nThey answer a simple but powerful question:\n\nWhere does a given substring start?\n\n\n\nWhy POSITION() / STRPOS() Matters in Analytics\nIn analytical workflows, knowing the position of a character or substring allows you to:\n\nvalidate expected formats\n\ndetect malformed values\n\ndrive conditional logic\n\nprepare for controlled extraction\n\nTypical analytical scenarios include:\n\nchecking if a delimiter exists\n\nidentifying country codes in phone numbers\n\ndetecting annotations in categories\n\nvalidating structured identifiers\n\n\n\n\nPOSITION() | Syntax Overview\nPOSITION(substring IN text)\n\nreturns the 1-based position of substring\nreturns 0 if the substring is not found\n\nPostgreSQL also supports:\nSTRPOS(text, substring)\nBoth behave identically.\n\n\n\nExample 1 | Detect Parentheses in Categories\nBefore removing annotations, we may want to detect which rows contain them.\nSELECT\n  category_raw,\n  POSITION('(' IN category_raw) AS open_paren_pos\nFROM customers_raw_text;\n\nOutput\n\n\n\ncategory_raw\nopen_paren_pos\n\n\n\n\nAccessories (Promo)\n13\n\n\nElectronics (Old)\n13\n\n\nAccessories\n0\n\n\nElectronics (Promo)\n13\n\n\nAccessories (Test)\n13\n\n\n\n\n\n\nAnalytical Interpretation\n\nopen_paren_pos &gt; 0 ‚Üí annotation exists\n\nopen_paren_pos = 0 ‚Üí clean category\n\nThis allows you to:\n\nflag rows for cleaning,\n\napply conditional logic\n\nmeasure data quality issues\n\n\n\n\nExample 2 | STRPOS() for Phone Number Diagnostics\nWe can also use STRPOS() to detect formatting characters.\nSELECT\n  raw_phone,\n  STRPOS(raw_phone, '-') AS hyphen_pos,\n  STRPOS(raw_phone, '(') AS paren_pos\n\nFROM customers_raw_text;\n\nOutput\n\n\n\nraw_phone\nhyphen_pos\nparen_pos\n\n\n\n\n077600945\n0\n0\n\n\n077-600-045\n4\n0\n\n\n(374)-77-600-945\n6\n1\n\n\n37477600945\n0\n0\n\n\n77600945\n0\n0\n\n\n\n\n\n\nAnalytical Interpretation\nThis immediately tells us:\n\nwhich rows contain separators\n\nwhich rows include country codes\n\nwhich rows are already clean\n\n\n\n\n\n\n\nNote\n\n\n\nThis is often done before applying REGEXP_REPLACE().\n\n\n\n\n\nPOSITION() as a Validation Tool\nA common analytical pattern is to convert position checks into flags.\nSELECT\n  customer_id,\n  category_raw,\n  POSITION('(' IN category_raw) &gt; 0 AS has_annotation\nFROM customers_raw_text;\n\nConceptual Output\n\n\n\ncustomer_id\ncategory_raw\nhas_annotation\n\n\n\n\n1\nAccessories (Promo)\ntrue\n\n\n2\nElectronics (Old)\ntrue\n\n\n3\nAccessories\nfalse\n\n\n4\nElectronics (Promo)\ntrue\n\n\n5\nAccessories (Test)\ntrue\n\n\n\nThis is useful for:\n\ndata quality reporting\n\naudit checks\n\nconditional cleaning logic\n\n\n\n\nPOSITION() vs SUBSTRING()\n\n\n\nQuestion\nPrefer\n\n\n\n\nWhere is it?\nPOSITION() / STRPOS()\n\n\nExtract it\nSUBSTRING()\n\n\nValidate format\nPOSITION()\n\n\nClean text\nREGEXP_REPLACE()\n\n\n\n\n\n\nAnalytical Best Practice\n\nuse POSITION() to measure and detect\nuse SUBSTRING() to extract\nuse REGEXP_REPLACE() to clean\nnever assume structure without checking positions\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Silent Assumptions\n\n\n\nAssuming a substring exists without checking its position leads to:\n\nincorrect extractions\n\nunexpected NULLs\n\nsilent logic failures\n\nAlways validate structure before relying on it.\n\n\nPOSITION() and STRPOS() are small functions with high analytical value when used for validation and diagnostics.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#split_part",
    "href": "materials/sql/session6.html#split_part",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "SPLIT_PART()",
    "text": "SPLIT_PART()\nAfter locating structure with POSITION() and before applying complex regex logic,\na very common analytical task is to split a string by a known delimiter.\nSPLIT_PART() extracts a specific segment from a delimited text field.\nIt is simpler and safer than regex when the structure is known and consistent.\n\n\nWhy SPLIT_PART() Matters in Analytics\nIn real datasets, multiple values are often stored in a single column, separated by delimiters.\n\nphone numbers (077-600-045)\ncomposite IDs (ORD-2024-001)\nhierarchical categories (Electronics-Mobile-Android)\nversion strings (v1.2.5)\n\nFrom an analytical perspective:\n\ngrouping often requires one specific segment\njoins depend on consistent extracted parts\nvalidation depends on expected segment positions\n\n\n\n\nSPLIT_PART() | Syntax\nSPLIT_PART(text, delimiter, position)\n\ntext ‚Üí input string\n\ndelimiter ‚Üí character or string used to split\n\nposition ‚Üí 1-based index of the part to return\n\nIf the requested position does not exist, SPLIT_PART() returns an empty string.\n\n\n\nExample 1 | Split Phone Numbers by Hyphen (First Part)\nInput\n\n\n\nraw_phone\n\n\n\n\n077-600-045\n\n\n(374)-77-600-945\n\n\n77600945\n\n\n\n\\[\\downarrow\\]\nSELECT\n  raw_phone,\n  SPLIT_PART(raw_phone, '-', 1) AS first_part\nFROM customers_raw_text;\n\\[\\downarrow\\]\nOutput\n\n\n\nraw_phone\nfirst_part\n\n\n\n\n077-600-045\n077\n\n\n(374)-77-600-945\n(374)\n\n\n77600945\n77600945\n\n\n\n\n\n\nExample 2 | Split Phone Numbers by Hyphen (Second Part)\nWe can also extract the second segment.\n\\[\\downarrow\\]\nSELECT\n  raw_phone,\n  SPLIT_PART(raw_phone, '-', 2) AS second_part\nFROM customers_raw_text;\n\\[\\downarrow\\]\nOutput\n\n\n\nraw_phone\nsecond_part\n\n\n\n\n077-600-045\n600\n\n\n(374)-77-600-945\n77\n\n\n77600945\n\n\n\n\n\n\n\nAnalytical Interpretation\n\nempty result means the delimiter does not exist\nempty string is not NULL\nmalformed or inconsistent rows are exposed immediately\n\nThis step is often used as a diagnostic, not just extraction.\n\n\n\nImportant Behavior to Remember\n\nmissing segments return an empty string\nempty strings can silently affect:\n\ngrouping\ncounting\njoins\n\n\nBecause of this, SPLIT_PART() is often paired with NULLIF().\n\n\n\nExample 3 | SPLIT_PART() After Cleaning\nIn practice, SPLIT_PART() is rarely used on raw data.\n\\[\\downarrow\\]\nSELECT\n  raw_phone,\n  SPLIT_PART(\n    REGEXP_REPLACE(raw_phone, '[^0-9-]', '', 'g'),\n    '-',\n    2\n  ) AS clean_second_part\nFROM customers_raw_text;\nThis combines:\n\npattern-based cleaning\n\ndelimiter-based extraction\n\n\n\n\nSPLIT_PART() vs Other String Functions\n\n\n\nQuestion\nPrefer\n\n\n\n\nKnown delimiter\nSPLIT_PART()\n\n\nFixed character positions\nSUBSTRING()\n\n\nVariable formats\nREGEXP_REPLACE()\n\n\nStructure validation\nPOSITION()\n\n\n\n\n\n\nAnalytical Best Practice\n\nvalidate delimiter presence with POSITION()\nclean text before splitting\nhandle empty strings explicitly\nnever assume all rows share the same structure\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Empty Strings\n\n\n\nSPLIT_PART() returns empty strings, not NULL.\nThis can:\n\ninflate COUNT(DISTINCT ...)\nbreak joins\nhide malformed records\n\nAlways validate results using:\n\nLENGTH()\nNULLIF()\nCOUNT(DISTINCT ...)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#nullif",
    "href": "materials/sql/session6.html#nullif",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "NULLIF()",
    "text": "NULLIF()\nAfter handling extraction, cleaning, and construction of text values, the next important analytical task is preventing misleading values from entering metrics and logic.\nNULLIF() is used to convert specific values into NULL intentionally.\nIt answers the question:\n\nWhen should a value be treated as ‚Äúmissing‚Äù?\n\n\n\nWhy NULLIF() Matters in Analytics\nIn real datasets, missing information is often encoded as fake values, such as:\n\nempty strings ''\nplaceholders like 'N/A', 'UNKNOWN'\ndefault values like '0'\n\nFrom an analytical perspective, these values:\n\ndistort counts\n\nbreak averages\n\npollute grouping keys\n\nhide true data quality issues\n\nNULLIF() allows you to restore semantic meaning by converting such placeholders into NULL.\n\n\n\nNULLIF() | Syntax Overview\nNULLIF(value, comparison_value)\nHow it works:\n\nif value = comparison_value ‚Üí returns NULL\notherwise ‚Üí returns value\n\n\n\n\nExample 1 | Empty Strings to NULL\nAssume some customers have missing last names stored as empty strings.\nSELECT\n  customer_id,\n  last_name,\n  NULLIF(last_name, '') AS last_name_clean\nFROM customers_raw_text;\n\nConceptual Output\n\n\n\ncustomer_id\nlast_name\nlast_name_clean\n\n\n\n\n1\ndoe\ndoe\n\n\n2\nsmith\nsmith\n\n\n3\njohnson\njohnson\n\n\n4\nVAN DYKE\nVAN DYKE\n\n\n5\n\nNULL\n\n\n\n\n\n\nAnalytical Interpretation\n\nempty strings are not meaningful values\nconverting them to NULL allows:\n\ncorrect COUNT(last_name)\ncorrect COUNT(DISTINCT last_name)\naccurate completeness checks\n\n\n\n\n\nExample 2 | Placeholder Text to NULL\nSometimes missing values are encoded explicitly.\nSELECT\n  category_raw,\n  NULLIF(category_raw, 'UNKNOWN') AS category_clean\nFROM customers_raw_text;\n\n\n\nExample 3 | NULLIF() Before Aggregation\nNULLIF() is often used before aggregation to prevent bad values from entering metrics.\nSELECT\n  COUNT(NULLIF(category_raw, 'UNKNOWN')) AS valid_categories\nFROM customers_raw_text;\nThis ensures that placeholder values do not inflate counts.\n\n\n\nNULLIF() with CONCAT() (Defensive Pattern)\nWhen building labels, NULLIF() can prevent ugly outputs.\nSELECT\n  customer_id,\n  CONCAT(\n    first_name,\n    ' ',\n    NULLIF(last_name, '')\n  ) AS full_name_safe\nFROM customers_raw_text;\nWithout NULLIF(), empty strings may silently produce malformed labels.\n\n\n\nNULLIF() vs COALESCE()\n\n\n\nGoal\nUse\n\n\n\n\nConvert bad value to NULL\nNULLIF()\n\n\nReplace NULL with value\nCOALESCE()\n\n\nRestore missing semantics\nNULLIF()\n\n\nImpute missing values\nCOALESCE()\n\n\n\nThese functions are often used together, but serve opposite purposes.\n\n\n\nAnalytical Best Practice\nBefore aggregation or joins:\n\nidentify placeholder values\n\nconvert them to NULL using NULLIF()\n\nthen apply aggregation or imputation logic\n\nThis keeps business meaning intact.\n\n\n\n\n\n\n\nWarningAnalytical Warning | Silent Placeholders\n\n\n\nPlaceholder values are more dangerous than NULLs.\n\nthey look valid\n\nthey pass filters\n\nthey corrupt metrics silently\n\nAlways audit text fields for placeholder values\nbefore trusting analytical results.\n\n\nNULLIF() is a small function with high analytical impact, especially in data quality and KPI correctness.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#left-right",
    "href": "materials/sql/session6.html#left-right",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "LEFT() / RIGHT()",
    "text": "LEFT() / RIGHT()\nAfter working with positional extraction (SUBSTRING()) and pattern-based logic, we often encounter text fields with a fixed directional structure.\nIn such cases, extracting characters from the beginning or the end of a string is clearer and safer than calculating positions.\nThis is exactly what LEFT() and RIGHT() are designed for.\n\n\nWhy LEFT() / RIGHT() Matter in Analytics\nMany analytical identifiers encode meaning at the edges of a string:\n\ncountry or operator prefixes\n\nregional or product codes\n\nfixed-length identifiers\n\nversion or batch suffixes\n\nWhen structure is stable, directional extraction is:\n\nmore readable\n\nless error-prone\n\neasier to audit\n\n\n\n\nSyntax Overview\nLEFT(text, n)\nRIGHT(text, n)\n\ntext ‚Üí input string\n\nn ‚Üí number of characters to extract\n\nextraction starts strictly from the left or right\n\n\n\n\nExample 1 | Extract Phone Prefix with LEFT()\nAssume phone numbers have already been cleaned to digits only\nand the first 3 digits represent a country or operator code.\nSELECT\n  raw_phone,\n  LEFT(\n    REGEXP_REPLACE(raw_phone, '[^0-9]', '', 'g'),\n    3\n  ) AS phone_prefix\nFROM customers_raw_text;\n\nOutput\n\n\n\nraw_phone\nphone_prefix\n\n\n\n\n077600945\n077\n\n\n077-600-045\n077\n\n\n(374)-77-600-945\n374\n\n\n37477600945\n374\n\n\n77600945\n776\n\n\n\n\n\n\nAnalytical Interpretation\n\nprefixes reveal country or operator\n\nenables routing or segmentation logic\n\nsupports validation of expected formats\n\n\n\n\nExample 2 | Extract Core Identifier with RIGHT()\nIf the last 8 digits uniquely identify a phone number,\nRIGHT() provides a clean and expressive solution.\nSELECT\n  raw_phone,\n  RIGHT(\n    REGEXP_REPLACE(raw_phone, '[^0-9]', '', 'g'),\n    8\n  ) AS phone_core\nFROM customers_raw_text;\n\nOutput\n\n\n\nraw_phone\nphone_core\n\n\n\n\n077600945\n77600945\n\n\n37477600945\n77600945\n\n\n77600945\n77600945\n\n\n\n\n\n\nLEFT() / RIGHT() vs SUBSTRING()\n\n\n\nScenario\nPrefer\n\n\n\n\nFixed prefix or suffix\nLEFT() / RIGHT()\n\n\nDynamic position\nSUBSTRING()\n\n\nReadability\nLEFT() / RIGHT()\n\n\nComplex extraction\nSUBSTRING()\n\n\n\n\n\n\nAnalytical Best Practice\n\napply cleaning before directional extraction\n\nvalidate assumptions with LENGTH()\n\nuse LEFT()/RIGHT() only when format is guaranteed\n\ndocument why a specific length was chosen\n\n\n\n\n\n\n\n\nWarningAnalytical Warning | Assumed Structure\n\n\n\nDirectional extraction assumes:\n\nstable formatting\n\nconsistent string length\n\nno missing segments\n\nIf upstream formats change, LEFT() and RIGHT() can silently produce incorrect results.\nAlways validate structure before relying on directional logic.\n\n\nLEFT() and RIGHT() are simple functions, but when used correctly, they provide clear, auditable, and efficient extraction for fixed-format analytical fields.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#dummy-dataset-messy-transaction-text-data",
    "href": "materials/sql/session6.html#dummy-dataset-messy-transaction-text-data",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "Dummy Dataset | Messy Transaction Text Data",
    "text": "Dummy Dataset | Messy Transaction Text Data\nThis dataset simulates realistic dirty text data commonly seen in transactional systems.\nIt will be used for both in-class exercises and the take-home case study.\nThe table intentionally contains:\n\ninconsistent phone formats\n\nannotated categories\n\nmixed casing\n\nformatting symbols\n\nduplicate logical entities\n\n\n\nStep 1 | Create Dummy Transaction Table\nDROP TABLE IF EXISTS transactions_text_demo;\n\nCREATE TABLE transactions_text_demo (\n  transaction_id INTEGER,\n  customer_id    INTEGER,\n  raw_phone      TEXT,\n  category_raw   TEXT,\n  quantity       INTEGER,\n  price          NUMERIC(10,2)\n);\n\n\n\nStep 2 | Populate with 1,000 Rows of Messy Data\nINSERT INTO transactions_text_demo\nSELECT\n  gs AS transaction_id,\n  (RANDOM() * 200)::INT + 1 AS customer_id,\n\n  CASE (gs % 6)\n    WHEN 0 THEN '   077600945  '\n    WHEN 1 THEN '077-600-045'\n    WHEN 2 THEN '(374)-77-600-945'\n    WHEN 3 THEN '37477600945'\n    WHEN 4 THEN '77600945'\n    ELSE '077600945'\n  END AS raw_phone,\n\n  CASE (gs % 5)\n    WHEN 0 THEN 'Accessories (Promo)'\n    WHEN 1 THEN 'Accessories (Test)'\n    WHEN 2 THEN 'Electronics (Old)'\n    WHEN 3 THEN 'Electronics (Promo)'\n    ELSE 'Accessories'\n  END AS category_raw,\n\n  (RANDOM() * 5)::INT + 1 AS quantity,\n  (RANDOM() * 500 + 10)::NUMERIC(10,2) AS price\nFROM generate_series(1, 1000) AS gs;\n\n\n\nStep 3 | Sanity Check\nSELECT\n  COUNT(*) AS total_rows,\n  COUNT(DISTINCT raw_phone) AS distinct_raw_phones,\n  COUNT(DISTINCT category_raw) AS distinct_categories\nFROM transactions_text_demo;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#in-class-assignment",
    "href": "materials/sql/session6.html#in-class-assignment",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "In-Class Assignment",
    "text": "In-Class Assignment\n\nObjective\nUse measurement-first SQL to understand how dirty text affects grouping and counts.\n\n\n\nTask 1 | Phone Number Diagnostics\nWrite a query that shows:\n\nraw_phone\nLENGTH(raw_phone)\nposition of '-'\nposition of '('\ncount of rows per pattern\n\nUse:\n\nLENGTH()\nPOSITION() or STRPOS()\nGROUP BY\n\n\n\n\nTask 2 | Category Fragmentation\nWrite a query that returns:\n\ncategory_raw\nnumber of transactions per category\n\nGROUP BY category_raw\nORDER BY COUNT(*) DESC;\nAnswer:\n\nhow many logical categories exist?\nhow many categories appear due to annotations?\n\n\n\n\nExpected Insight\nYou should clearly see that:\n\ngrouping on raw text produces fragmented dimensions\nphone numbers represent the same entity in multiple formats",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session6.html#take-home-case-study",
    "href": "materials/sql/session6.html#take-home-case-study",
    "title": "Session 06: Data Analysis with SQL | Functions",
    "section": "Take-Home Case Study",
    "text": "Take-Home Case Study\n\nScenario\nYou are asked to produce reliable KPIs from transactions_text_demo.\nBusiness questions:\n\nrevenue by category\n\nnumber of unique customers\n\naverage transaction value\n\n\n\n\nPart 1 | Profiling\nWrite SQL queries to assess:\n\nphone number format diversity\ncategory fragmentation\nimpact of dirty text on GROUP BY\n\nUse only diagnostic functions.\n\n\n\nPart 2 | Standardization Layer\nCreate a cleaned SELECT projection (no updates) that includes:\n\nstandardized phone number (last 8 digits)\ncleaned category (no annotations, trimmed)\nrevenue per transaction\n\nUse:\n\nREGEXP_REPLACE()\nTRIM()\nSUBSTRING()\nCONCAT() where relevant\n\n\n\n\nPart 3 | KPI Comparison\nCompute and compare:\n\nrevenue by raw category\nrevenue by cleaned category\nunique customers (raw vs cleaned phone)\n\nUse GROUP BY in all comparisons.\n\n\n\nPart 4 | Analytical Explanation\nBriefly explain:\n\nwhy KPIs changed\nwhich cleaning step had the biggest impact\nwhat assumptions you made\nwhat could silently break in production\n\n\n\n\nSubmission Rules\n\nCreate string_functions.sql\nSQL only (no updates or deletes)\nall cleaning must be done in SELECT queries\nresults must be reproducible\nexplanations must be concise and analytical\npush it into GitHub\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis assignment tests analytical thinking, not just SQL syntax.\nDirty text ‚Üí broken GROUP BY ‚Üí misleading KPIs.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 06: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/python/slides/session3.html#comming.-soon",
    "href": "materials/python/slides/session3.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/python/slides/session5.html#comming.-soon",
    "href": "materials/python/slides/session5.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/python/slides/session1.html#comming.-soon",
    "href": "materials/python/slides/session1.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/python/slides/session4.html#comming.-soon",
    "href": "materials/python/slides/session4.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/python/index.html",
    "href": "materials/python/index.html",
    "title": "Python",
    "section": "",
    "text": "Python Session 01: Coming Soon\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "Python"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html",
    "href": "materials/statistics/session2.html",
    "title": "Statistics Session 02: Data Types",
    "section": "",
    "text": "Types of Data\nCentral Tendancy measurs\nIntro to visualization",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#agenda",
    "href": "materials/statistics/session2.html#agenda",
    "title": "Statistics Session 02: Data Types",
    "section": "",
    "text": "Types of Data\nCentral Tendancy measurs\nIntro to visualization",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#what-is-statistics",
    "href": "materials/statistics/session2.html#what-is-statistics",
    "title": "Statistics Session 02: Data Types",
    "section": "What is Statistics?",
    "text": "What is Statistics?\nStatistics is the branch of mathematics that transforms numbers (data) into useful information fohr decision makers.\nWhy learn Statistics?\n\nstatistics helps you make better sense of the world.\nstatistics helps you make better business decisions.\n\nThe statistical methods you use for these tasks come from one of the two branches of statistics: descriptive statistics and inferential statistics.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#basic-vocabulary-of-statistics",
    "href": "materials/statistics/session2.html#basic-vocabulary-of-statistics",
    "title": "Statistics Session 02: Data Types",
    "section": "Basic Vocabulary of Statistics",
    "text": "Basic Vocabulary of Statistics\n\nVariable: A variable is a characteristic of an item or individual.\nData: Data are the different values associated with a variable.\nPopulation: A population consists of all the items or individuals about which you want to reach conclusions.\nSample: A sample is the portion of a population selected for analysis.\nParameter: A parameter is a measure that describes a characteristic of a population.\nStatistic: A statistic is a measure that describes a characteristic of a sample.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#why-data-types-matter",
    "href": "materials/statistics/session2.html#why-data-types-matter",
    "title": "Statistics Session 02: Data Types",
    "section": "Why Data Types Matter?",
    "text": "Why Data Types Matter?\nBefore calculating averages or plotting charts, it‚Äôs essential to recognize what kind of data you‚Äôre working with.\nThe classification of a variable determines:\n\nWhich summary statistics are meaningful\n\nWhich visualizations can be used\n\nHow relationships between variables should be interpreted",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#qualitative-categorical-data",
    "href": "materials/statistics/session2.html#qualitative-categorical-data",
    "title": "Statistics Session 02: Data Types",
    "section": "Qualitative (Categorical) Data",
    "text": "Qualitative (Categorical) Data\nQualitative data describe qualities, categories, or labels rather than numbers.\n\n\n\n\n\n\n\n\nSubtype\nDefinition\nExamples\n\n\n\n\nNominal\nCategories with no natural order\nGender (Male/Female), City (Paris, Yerevan, Tokyo)\n\n\nOrdinal\nCategories with a meaningful order, but unequal spacing\nEducation Level (High &lt; Bachelor &lt; Master &lt; PhD), Satisfaction (Low‚ÄìMedium‚ÄìHigh)\n\n\n\n\n\n\n\n\n\nTipRemember\n\n\n\n\nNominal ‚Üí just names, no order\n\nOrdinal ‚Üí names with rank",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#quantitative-numerical-data",
    "href": "materials/statistics/session2.html#quantitative-numerical-data",
    "title": "Statistics Session 02: Data Types",
    "section": "Quantitative (Numerical) Data",
    "text": "Quantitative (Numerical) Data\nQuantitative data represent measurable quantities that can be used in arithmetic operations.\n\n\n\n\n\n\n\n\nSubtype\nDefinition\nExamples\n\n\n\n\nDiscrete\nCountable numbers, often integers\nNumber of customers, Complaints per day\n\n\nContinuous\nMeasured values within a range\nTemperature, Age, Revenue, Weight\n\n\n\n\nInterval Data\nInterval values represent ordered units that have the same difference.\nA variable is considered interval data when it contains numeric values that are ordered and where the exact differences between values are known.\nExample (Temperature in Celsius):\n[-15, -10, -5, 0, 5, 10, 15, 20, 25, 30]\nA key limitation of interval data is that it does not have a true zero.\nIn the Celsius example, there is no such thing as ‚Äúno temperature.‚Äù\n\n\nRatio Data\nRatio values are also ordered units that have the same difference, similar to interval values.\nThe key distinction is that ratio data has an absolute zero.\nExamples: Income, Weight, Length, etc.\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete ‚Üí counting\n\nContinuous ‚Üí measuring",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#additional-data-types-common-in-analytics",
    "href": "materials/statistics/session2.html#additional-data-types-common-in-analytics",
    "title": "Statistics Session 02: Data Types",
    "section": "Additional Data Types (Common in Analytics)",
    "text": "Additional Data Types (Common in Analytics)\n\n\n\n\n\n\n\n\nType\nDefinition\nExample Applications\n\n\n\n\nBinary\nTwo possible outcomes (Yes/No, 0/1)\nSubscription status, churn indicator\n\n\nTime-Series\nObservations recorded sequentially over time\nDaily sales, hourly temperature\n\n\nTextual / Unstructured\nWords, sentences, or documents\nCustomer reviews, tweets\n\n\nSpatial / Geographical\nLocation-based information\nStore coordinates, delivery zones\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"default\", \"logLevel\": \"fatal\"}}%%\ngraph TD\n    A[\"Data Types\"] --&gt; B[\"Qualitative (Categorical)\"]\n    A --&gt; C[\"Quantitative (Numerical)\"]\n    A --&gt; H[\"Other Types\"]\n\n    B --&gt; D[\"Nominal\"]\n    B --&gt; E[\"Ordinal\"]\n\n    C --&gt; F[\"Discrete\"]\n    C --&gt; G[\"Continuous\"]\n\n    \n    H --&gt; I[\"Binary\"]\n    H --&gt; J[\"Time-Series\"]\n    H --&gt; K[\"Textual\"]\n    H --&gt; L[\"Spatial\"]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#mean",
    "href": "materials/statistics/session2.html#mean",
    "title": "Statistics Session 02: Data Types",
    "section": "Mean",
    "text": "Mean\nThe mean is the sum of all values divided by the number of observations.\n\\[\n\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\n\n\n\nExample\nCalculation\n\n\n\n\nData: 5, 7, 8, 10\nMean = (5 + 7 + 8 + 10) / 4 = 7.5\n\n\n\n\nMean for Sample\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\n\n\nPopulation Mean\n(same, only notation is different)\n\\[\n\\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n\\]\n\n\nWeighted Mean\n\n\n\nType\nScore\nWeight (%)\n\n\n\n\nExam\n94\n50\n\n\nProject\n92\n35\n\n\nHomework\n100\n15\n\n\n\n(Weights do not need to add up to one!)\n\\[\n\\bar{x} =\n\\frac{\\displaystyle \\sum_{i=1}^{n} (w_i x_i)}\n     {\\displaystyle \\sum_{i=1}^{n} w_i}\n\\]\n\n\nSample Mean Example\n\n\n\n\n\n\nTipWhen to Use?\n\n\n\n\nConveinent measurement clear to everybody\nWorks well with continuous or discrete numerical data.\n\nSensitive to outliers (extreme values can distort the result).\n\n\n\nGiven the 5 observations:\n\\[\n\\bar{x}\n= \\frac{\\sum_{i=1}^{5} x_i}{5}\n= \\frac{14.2 + 19.6 + 22.7 + 13.1 + 20.9}{5}\n= \\frac{90.5}{5}\n= 18.1 \\\n\\]\n\n\\[\n\\mu\n= \\frac{\\sum_{i=1}^{5} x_i}{5}\n= \\frac{300 + 320 + 270 + 210 + 8{,}000}{5}\n= \\frac{9{,}100}{5}\n= 1{,}820\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#median",
    "href": "materials/statistics/session2.html#median",
    "title": "Statistics Session 02: Data Types",
    "section": "Median",
    "text": "Median\nThe median is the value that separates the dataset into two equal halves.\nSteps to calculate:\n\nOrder the data from smallest to largest.\n\nIf the number of observations is odd ‚Üí middle value.\n\nIf even ‚Üí average of the two middle values.\n\n\n\n\nExample\nCalculation\n\n\n\n\nData: 2, 5, 7, 9, 12\nMedian = 7\n\n\nData: 3, 5, 8, 10\nMedian = (5 + 8)/2 = 6.5\n\n\n\n\n\n\n\n\n\nNoteWhen to Use?\n\n\n\n\nBetter than the mean for skewed data or when outliers are present.\n\nCommon for income, property prices, etc.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#mode",
    "href": "materials/statistics/session2.html#mode",
    "title": "Statistics Session 02: Data Types",
    "section": "Mode",
    "text": "Mode\nThe mode is the value that appears most often.\n\n\n\nExample\nCalculation\n\n\n\n\nData: 2, 3, 3, 4, 5, 5, 5, 7\nMode = 5\n\n\n\n\n\n\n\n\n\nTipWhen to Use?\n\n\n\n\nIdeal for categorical or discrete data.\n\nA dataset can have:\n\nOne mode ‚Üí unimodal\n\nTwo modes ‚Üí bimodal\n\nMore than two ‚Üí multimodal",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#comparison-of-mean-median-and-mode",
    "href": "materials/statistics/session2.html#comparison-of-mean-median-and-mode",
    "title": "Statistics Session 02: Data Types",
    "section": "Comparison of Mean, Median, and Mode",
    "text": "Comparison of Mean, Median, and Mode\n\n\n\n\n\n\n\n\n\n\nMeasure\nBest For\nSensitive to Outliers?\nData Type\nExample Context\n\n\n\n\nMean\nSymmetrical distributions\nYes\nContinuous, Discrete\nAverage income\n\n\nMedian\nSkewed distributions\nNo\nContinuous\nTypical housing price\n\n\nMode\nCategorical / Repeated values\nNo\nNominal, Ordinal\nMost common product category\n\n\n\n\n\n\n\n\n\nTipVisual Insight\n\n\n\n\nIn a perfectly symmetrical distribution ‚Üí Mean = Median = Mode\n\nIn a right-skewed distribution ‚Üí Mean &gt; Median &gt; Mode\n\nIn a left-skewed distribution ‚Üí Mean &lt; Median &lt; Mode",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#frequency-distributions",
    "href": "materials/statistics/session2.html#frequency-distributions",
    "title": "Statistics Session 02: Data Types",
    "section": "Frequency Distributions",
    "text": "Frequency Distributions\n\nSymmetric Distribution\nMean ~ Median\n\n\n\n\n\n\n\n\n\n\n\nLeft Skewed Distributions\n\n\n\n\n\n\n\n\n\n\n\nRight Skewed Distributions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#symmetric-left-skewed-and-right-skewed-distributions",
    "href": "materials/statistics/session2.html#symmetric-left-skewed-and-right-skewed-distributions",
    "title": "Statistics Session 02: Data Types",
    "section": "Symmetric, Left-Skewed, and Right-Skewed Distributions",
    "text": "Symmetric, Left-Skewed, and Right-Skewed Distributions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#enriched-plot",
    "href": "materials/statistics/session2.html#enriched-plot",
    "title": "Statistics Session 02: Data Types",
    "section": "Enriched Plot",
    "text": "Enriched Plot\n\n\nCode\nnp.random.seed(1)\n\n# ---- Generate data ----\ndata_sym  = np.random.normal(loc=50, scale=10, size=500)\ndata_left = 100 - np.random.gamma(shape=5, scale=4, size=500)\ndata_right = np.random.gamma(shape=4, scale=4, size=500)\n\ndatasets = [\n    (\"Symmetric\\nMean ‚âà Median\", data_sym),\n    (\"Left-Skewed\\nMean &lt; Median\", data_left),\n    (\"Right-Skewed\\nMean &gt; Median\", data_right)\n]\n\n# ---- Create subplots ----\nfig, axes = plt.subplots(1, 3, figsize=(16, 4.5))  # slightly wider for annotations\nbins = 20\n\nfor ax, (title, data) in zip(axes, datasets):\n\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n\n    # Histogram\n    counts, bins_hist, patches = ax.hist(\n        data, bins=bins, alpha=0.6, edgecolor=\"black\"\n    )\n\n    # KDE smooth density\n    kde = gaussian_kde(data)\n    xline = np.linspace(min(data), max(data), 400)\n    ax.plot(xline, kde(xline) * len(data) * (bins_hist[1] - bins_hist[0]),\n            color=\"black\", linewidth=1.5, label=\"KDE\")\n\n    # Mean line (blue dashed)\n    ax.axvline(mean_val, color=\"blue\", linestyle=\"--\", linewidth=2)\n    ax.annotate(\"Mean\", xy=(mean_val, max(counts)*0.9),\n                xytext=(mean_val, max(counts)*1.1),\n                ha=\"center\", color=\"blue\", fontsize=8,\n                arrowprops=dict(arrowstyle=\"-&gt;\", color=\"blue\"))\n\n    # Median line (red solid)\n    ax.axvline(median_val, color=\"red\", linestyle=\"-\", linewidth=2)\n    ax.annotate(\"Median\", xy=(median_val, max(counts)*0.9),\n                xytext=(median_val, max(counts)*1.1),\n                ha=\"center\", color=\"red\", fontsize=8,\n                arrowprops=dict(arrowstyle=\"-&gt;\", color=\"red\"))\n\n    # Shaded tails depending on skew\n    if \"Left\" in title:\n        ax.axvspan(min(data), mean_val, alpha=0.12, color=\"blue\")\n    elif \"Right\" in title:\n        ax.axvspan(mean_val, max(data), alpha=0.12, color=\"blue\")\n\n    # Titles & labels\n    ax.set_title(title, fontsize=12)\n    ax.set_xlabel(\"Value\", fontsize=9)\n    ax.set_ylabel(\"Frequency\", fontsize=9)\n    ax.tick_params(axis=\"both\", labelsize=8)\n\n    # Summary table under plot\n    table_data = [\n        [\"Mean\", f\"{mean_val:.2f}\"],\n        [\"Median\", f\"{median_val:.2f}\"],\n        [\"Std Dev\", f\"{np.std(data):.2f}\"]\n    ]\n    table = ax.table(\n        cellText=table_data,\n        colLabels=[\"Statistic\", \"Value\"],\n        cellLoc=\"center\",\n        loc=\"bottom\",\n        fontsize=8\n    )\n    table.scale(1, 1.1)\n\nplt.tight_layout(pad=2)\nplt.show()",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#measures-of-variability",
    "href": "materials/statistics/session2.html#measures-of-variability",
    "title": "Statistics Session 02: Data Types",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\nRange\nVariance\nStandard Deviation (SD)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#range",
    "href": "materials/statistics/session2.html#range",
    "title": "Statistics Session 02: Data Types",
    "section": "Range",
    "text": "Range\n\\[\n\\text{Range = Highest Value - Lowest Value}\n\\]\n\n\n\n\n\n\nNoteThink about‚Ä¶\n\n\n\n\nThink about where is it applicable?\nAre there any limitations",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#variance",
    "href": "materials/statistics/session2.html#variance",
    "title": "Statistics Session 02: Data Types",
    "section": "Variance",
    "text": "Variance\nSample Variance:\n\\[\ns^{2} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}{n - 1}\n\\]\nPopulation Variance\n\\[\n\\sigma^{2}\n=\n\\frac{\n\\displaystyle \\sum_{i=1}^{N} (x_i - \\mu)^{2}\n}{\nN\n}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#standard-deviation",
    "href": "materials/statistics/session2.html#standard-deviation",
    "title": "Statistics Session 02: Data Types",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nSample Standard Deviation (Standard Error)\n\\[\ns \\;=\\; \\sqrt{s^{2}}\n\\;=\\;\n\\sqrt{\n\\frac{\n\\displaystyle \\sum_{i=1}^{n} (x_i - \\bar{x})^{2}\n}{\nn - 1\n}\n}\n\\]\nPopulation standard deviation\n\\[\n\\sigma\n=\n\\sqrt{\n\\frac{\n\\displaystyle \\sum_{i=1}^{N} (x_i - \\mu)^{2}\n}{\nN\n}\n}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#how-to-compare-variability",
    "href": "materials/statistics/session2.html#how-to-compare-variability",
    "title": "Statistics Session 02: Data Types",
    "section": "How to compare variability?",
    "text": "How to compare variability?\nIn which cases you can compare standard deviations?\n\n\n\nBox\nSample 1\nSample 2\n\n\n\n\nBox 1\n14.2\n18.2\n\n\nBox 2\n19.6\n17.9\n\n\nBox 3\n22.7\n18.1\n\n\nBox 4\n13.1\n18.1\n\n\nBox 5\n20.9\n18.2\n\n\nMean\n18.1\n18.1\n\n\nStandard deviation\n4.23\n0.12\n\n\n\n\nVisual Comparison of Variability\n\n\n\n\n\n\n\n\n\n\n\nWhen variability is bad?\n\nWhen consistency and quality control are important\n\nProduct weights, drug dosages, machine precision\n\nDelivery times, service response times\n\nFinancial risk (greater uncertainty)\n\n\n\nWhen variability is good?\n\nBiological diversity and adaptability\n\nMarketing segmentation & A/B testing\n\nCreativity and innovation\n\nInvestments seeking higher upside\n\nIdentifying top performers (sports, hiring)\n\n\n\nWhen variability is neutral?\n\nNatural randomness (weather, height, sampling variation)\n\n\n\n\n\n\n\nCaution\n\n\n\nVariability is not inherently good or bad‚Äîits value depends on what you are trying to achieve.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#why-n-1",
    "href": "materials/statistics/session2.html#why-n-1",
    "title": "Statistics Session 02: Data Types",
    "section": "Why \\(N-1\\)?",
    "text": "Why \\(N-1\\)?\nA degree of freedom (df) is an independent piece of information that can vary freely.\nWhenever we estimate a parameter from the sample, we introduce a constraint.\nEach constraint removes one degree of freedom.\nThe universal rule is:\n\\[\ndf = n - \\text{(number of estimated parameters)}.\n\\]\nDuring the mean estimation, since we have one constraint we remove only 1 hence\n\\[\ndf = n - 1\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#coefficient-of-variation-normalization",
    "href": "materials/statistics/session2.html#coefficient-of-variation-normalization",
    "title": "Statistics Session 02: Data Types",
    "section": "Coefficient of Variation (Normalization)",
    "text": "Coefficient of Variation (Normalization)\nThe coefficient of variation (CV) is a measure of relative variability.\nIt allows us to compare the variability of two datasets even when their units or scales differ.\n\nData\n\n\n\nDate\nNike\nGoogle\n\n\n\n\nSeptember 14, 2012\n48.32\n709.68\n\n\nOctober 15, 2012\n47.81\n740.98\n\n\nNovember 15, 2012\n45.42\n647.26\n\n\nDecember 14, 2012\n48.46\n701.96\n\n\nJanuary 15, 2013\n53.64\n724.93\n\n\nFebruary 15, 2013\n54.95\n792.89\n\n\nMean\n49.77\n719.62\n\n\nStandard deviation\n3.70\n47.96\n\n\n\n\n\nFormula for the Coefficient of Variation\n\\[\n\\text{CV} = \\frac{s}{\\bar{x}} \\times 100\n\\]\nWhere:\n\n\\(s\\) = standard deviation\n\n\\(\\bar{x}\\) = mean\n\nCV is expressed as a percentage\n\n\nNike\n\\[\n\\text{CV}_{\\text{Nike}}\n= \\frac{3.70}{49.77} \\times 100\n\\approx 7.4\\%\n\\]\nGoogle\n\\[\n\\text{CV}_{\\text{Google}}\n= \\frac{47.96}{719.62} \\times 100\n\\approx 6.7\\%\n\\]\nAlthough Google‚Äôs stock prices vary by a much larger dollar amount, its relative variability is smaller.\n\nNike CV = 7.4%\n\nGoogle CV = 6.7%\n\nThis means that, compared to its average price, Google‚Äôs stock price is more stable than Nike‚Äôs.\nThe CV helps us compare variability across different scales and units‚Äîsomething raw standard deviations alone cannot do.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#z-score",
    "href": "materials/statistics/session2.html#z-score",
    "title": "Statistics Session 02: Data Types",
    "section": "Z-score",
    "text": "Z-score\nNumber of standard deviations that particular value is farther from the mean of its population or sample:\nPopulation:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nSample:\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\]\n=STANDARDIZE(x, mean, standard_deviation)\nSuppose we have:\n\nx = 540\nmean = 776.3\nstandard deviation = 385.1\n\nIf a data point has a z-score of 0, that means it lies exactly at the mean. If a data point has a positive score (z-score &gt; 0), that means it lies above the mean. If a data point has negative score (z-score&lt;0), that means it lies below the mean.\n\nRule of Thumb for identifying outliers: Data values that have z-scores above +3 and below -3 can be categorized as outliers.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#empirical-rule",
    "href": "materials/statistics/session2.html#empirical-rule",
    "title": "Statistics Session 02: Data Types",
    "section": "Empirical Rule",
    "text": "Empirical Rule\nFor a normally distributed variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the empirical rule states:\n\nWithin 1 standard deviation \\((\\mu \\pm 1\\sigma)\\): about \\(68\\%\\) of the data\nWithin 2 standard deviations \\((\\mu \\pm 2\\sigma)\\): about \\(95\\%\\) of the data\nWithin 3 standard deviations \\(\\mu \\pm 3\\sigma\\): about \\(99.7\\%\\) of the data\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\nDaily Steps on Fitness Trackers\nDaily step counts for a person over several months often form a normal-like distribution.\nAssuming:\n\nAverage steps per day: \\(\\mu = 8{,}000\\)\n\nStandard deviation: \\(\\sigma = 1{,}500\\)\n\nThen:\n\n68% of days fall within: \\([6500,\\ 9500]\\)\n95% of days fall within: \\([5000,\\ 11000]\\)\n99.7% of days fall within: \\([3500,\\ 12500]\\)\n\nFitness coaches use this rule to identify unusually inactive or exceptionally active days.\n\n\nSAT / GRE / University Exam Scores\nLarge standardized tests tend to be approximately normal.\nSuppose:\n\nMean SAT Math score: \\(\\mu = 520\\)\n\nStandard deviation: \\(\\sigma = 100\\)\n\nThen:\n\n68% score between: \\(520 \\pm 100 = [420,\\ 620]\\)\n95% score between: \\(520 \\pm 200 = [320,\\ 720]\\)\n99.7% score between: \\(520 \\pm 300 = [220,\\ 820]\\)\n\nAdmissions departments use this distribution to benchmark typical, strong, or exceptional performance.\n\n\n\nSummarized Examples\n\n\n\n\n\n\n\n\n\n\n\nScenario\nMean (\\(\\mu\\))\nStd (\\(\\sigma\\))\n68% Range\n95% Range\n99.7% Range\n\n\n\n\nBody Temperature (¬∞C)\n37¬∞C\n0.3¬∞C\n36.7‚Äì37.3¬∞C\n36.4‚Äì37.6¬∞C\n36.1‚Äì37.9¬∞C\n\n\nSAT Math Score\n520\n100\n420‚Äì620\n320‚Äì720\n220‚Äì820\n\n\nSteps per Day\n8000\n1500\n6500‚Äì9500\n5000‚Äì11000\n3500‚Äì12500\n\n\nRod Length (mm)\n100\n2\n98‚Äì102\n96‚Äì104\n94‚Äì106\n\n\nReaction Time (sec)\n0.25\n0.04\n0.21‚Äì0.29\n0.17‚Äì0.33\n0.13‚Äì0.37",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#chebyshevs-theorem",
    "href": "materials/statistics/session2.html#chebyshevs-theorem",
    "title": "Statistics Session 02: Data Types",
    "section": "Chebyshev‚Äôs Theorem",
    "text": "Chebyshev‚Äôs Theorem\nChebyshev‚Äôs Theorem applies to any distribution, not only symmetric or bell-shaped ones.\nIt states that for any value of \\((z &gt; 1)\\), at least the following percentage of observations lie within\n( z ) standard deviations of the mean:\n\\[\n\\left( 1 - \\frac{1}{z^2} \\right) \\times 100\n\\]\n\n\n\n\n\n\n\n\nZ-Score Band\nChebyshev Minimum\nInterpretation\n\n\n\n\n\\(|Z| &lt; 1.5\\)\n\\(1 - \\frac{1}{2.25} = 0.556 \\rightarrow 55.6\\%\\)\nAt least \\(55.6\\%\\) of data have z-scores between ‚Äì1.5 and +1.5\n\n\n\\(|Z| &lt; 2\\)\n\\(1 - \\frac{1}{4} = 0.75 \\rightarrow 75\\%\\)\nAt least \\(75\\%\\) fall within ‚Äì2 ‚â§ Z ‚â§ +2\n\n\n\\(|Z| &lt; 3\\)\n\\(1 - \\frac{1}{9} = 0.889 \\rightarrow 88.9\\%\\)\nAt least \\(88.9\\%\\) fall within ‚Äì3 ‚â§ Z ‚â§ +3\n\n\n\\(|Z| &lt; 4\\)\n\\(1 - \\frac{1}{16} = 0.9375\\rightarrow 93.75\\%\\)\nAt least \\(93.75\\%\\) fall within ‚Äì4 ‚â§ Z ‚â§ +4\n\n\n\nSuppose monthly customer spending is highly skewed, with a few customers making extremely large purchases.\nBecause the distribution is not normal, we cannot rely on the empirical rule ‚Äî but Chebyshev‚Äôs theorem still applies when using z-scores.\nWe measure:\n\nMean spending: \\(\\mu = 120\\) USD\n\nStandard deviation: \\(\\sigma = 90\\) USD\n\nWe want to know what proportion of customers have z-scores between ‚Äì2 and +2, i.e., fall within two standard deviations of the mean.\nUsing Chebyshev:\n\\[\nP(|Z| &lt; 2) \\ge 1 - \\frac{1}{4} = 0.75\n\\]\nSo at least 75% of customers must have z-scores:\n\\[\n-2 \\le Z \\le 2\n\\]\nThis corresponds to spending between:\n\\[\nx = \\mu \\pm 2\\sigma = 120 \\pm 180\n\\]\nThus:\n\nLower bound: 120 - 180 = -60 (interpreted as 0 in real life)\n\nUpper bound: 120 + 180 = 300\n\nMeaning:\n\n\n\n\n\n\nTip\n\n\n\nAt least 75% of all customers must spend between 0 and 300 USD, even though the distribution is highly skewed and unpredictable.\n\n\nThis is the power of expressing Chebyshev in z-score terms:",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#chebyshev-vs-empirical-rule",
    "href": "materials/statistics/session2.html#chebyshev-vs-empirical-rule",
    "title": "Statistics Session 02: Data Types",
    "section": "Chebyshev vs Empirical Rule",
    "text": "Chebyshev vs Empirical Rule\n\n\n\n\n\n\n\n\nGoal\nNormal Data\nAny Data\n\n\n\n\nWant precise percentages (68‚Äì95‚Äì99.7)?\nUse z-scores + Empirical Rule\nNot valid\n\n\nWant a minimum guarantee?\nOptional\nUse z-scores + Chebyshev\n\n\nData is skewed or irregular?\nNot valid\nChebyshev works\n\n\nDistribution unknown?\nNot valid\nChebyshev works",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#grouped-data",
    "href": "materials/statistics/session2.html#grouped-data",
    "title": "Statistics Session 02: Data Types",
    "section": "Grouped Data",
    "text": "Grouped Data\nBelow table represents a grouped frequency distribution of respondents by age category. Each row corresponds to an age interval, and the second column shows the frequency (f·µ¢), meaning the number of people who fall within that interval. The total frequency is 100, indicating the entire sample size.\n\n\n\nAge Group\nNumber of Respondents (f·µ¢)\n\n\n\n\n20 to under 30\n20\n\n\n30 to under 40\n41\n\n\n40 to under 50\n19\n\n\n50 to under 60\n9\n\n\n60 to under 70\n11\n\n\nTotal\n100\n\n\n\n\nMean of the Grouped Data\n\\[\n\\bar{x} \\approx\n\\frac{\n\\sum_{i=1}^{k} (f_i m_i)\n}{\nn\n}\n\\]\n\n\\(f_i\\) = the frequency for class \\(i\\)\n\n\\(m_i\\) = the midpoint for class \\(i\\)\n\n\\(n = \\sum_{i=1}^{k} f_i\\) (the total number of observations)\n\n\\(k\\) = number of classes\n\n\nStep 1: Midpoints\n\n\\(\\frac{20+30}{2}=25\\)\n\n\n\n\nAge Group\nClass Midpoint (m·µ¢)\nFrequency (f·µ¢)\n\n\n\n\n20 to under 30\n25\n20\n\n\n30 to under 40\n35\n41\n\n\n40 to under 50\n45\n19\n\n\n50 to under 60\n55\n9\n\n\n60 to under 70\n65\n11\n\n\nTotal\n\n100\n\n\n\n\n\n\nStep 2: Cumulative\n\n\n\nAge Group\nFrequency (f·µ¢)\nCumulative Frequency (F·µ¢)\n\n\n\n\n20 to under 30\n20\n20\n\n\n30 to under 40\n41\n61\n\n\n40 to under 50\n19\n80\n\n\n50 to under 60\n9\n89\n\n\n60 to under 70\n11\n100\n\n\nTotal\n100\n100\n\n\n\n\n\nStep 3: Relative Frequency Table\n\n\n\nAge Group\nFrequency (f·µ¢)\nRelative Frequency (f·µ¢ / 100)\n\n\n\n\n20 to under 30\n20\n0.20\n\n\n30 to under 40\n41\n0.41\n\n\n40 to under 50\n19\n0.19\n\n\n50 to under 60\n9\n0.09\n\n\n60 to under 70\n11\n0.11\n\n\nTotal\n100\n1.00\n\n\n\n\n\nStep 4: Percentage Frequencies Table\n\n\n\nAge Group\nFrequency (f·µ¢)\nPercentage (%)\n\n\n\n\n20 to under 30\n20\n20\n\n\n30 to under 40\n41\n41\n\n\n40 to under 50\n19\n19\n\n\n50 to under 60\n9\n9\n\n\n60 to under 70\n11\n11\n\n\nTotal\n100\n100\n\n\n\n\n\nStep 5: Visualization\n\n\n\n\n\n\n\n\n\n\n\nGrouped SD\n\\[\ns^2 \\approx \\frac{\\sum_{i=1}^k f_i (m_i - \\bar{x})^2}{n - 1}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge Group\nClass Midpoint \\(m_i\\)\nFrequency \\(f_i\\)\n\\(\\bar{x}\\)\n\\(m_i - \\bar{x}\\)\n\\((m_i - \\bar{x})^2\\)\n\\((m_i - \\bar{x})^2 f_i\\)\n\n\n\n\n20‚Äì30\n25\n20\n40\n-15\n225\n4,500\n\n\n30‚Äì40\n35\n41\n40\n-5\n25\n1,025\n\n\n40‚Äì50\n45\n19\n40\n5\n25\n475\n\n\n50‚Äì60\n55\n9\n40\n15\n225\n2,025\n\n\n60‚Äì70\n65\n11\n40\n25\n625\n6,875\n\n\nTotal\n\n100\n\n\n\n14,900",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#relative-positions-percentile",
    "href": "materials/statistics/session2.html#relative-positions-percentile",
    "title": "Statistics Session 02: Data Types",
    "section": "Relative Positions | Percentile",
    "text": "Relative Positions | Percentile\nPercentiles help us describe where a particular value lies within the distribution of a dataset.\n\nPercentile: an approximate measure of the percentage of values in the dataset that fall below a given value.\nThe \\(p\\)-th percentile is the value such that at least \\(p\\) percent of the observations lie below it.\nDo not confuse percentages and percentiles ‚Äî they describe different concepts.\nThe median is the 50th percentile, because half of the values lie below it.\n\n\\[\ni = \\frac{p}{100}(n)\n\\]\nWhere:\n\n\\(p\\) = the desired percentile\n\n\\(n\\) = the total number of observations\n\n\\(i\\) = the index (position) of the percentile in the ordered dataset\n\nInterpreting the Index \\(i\\)\n\nIf \\(i\\) is not a whole number, round \\(i\\) up to the next whole number: The value at that position is the percentile.\nIf \\(i\\) is a whole number, the percentile is the midpoint between the value at position \\(i\\) and the value at position \\(i + 1\\).\n\nIn this example, we will calculate several percentiles using the index formula:\n\\[\ni = \\frac{p}{100}(n)\n\\]\n\nPercentile Rank\n\n\n\nName\nScore\nName\nScore\n\n\n\n\nLori\n726\nMarc\n690\n\n\nChris\n643\nKendra\n747\n\n\nJohn\n777\nPeter\n618\n\n\nSam\n695\nJordan\n701\n\n\nKatie\n796\nTeresa\n660\n\n\nCarole\n743\nDavid\n716\n\n\nIrina\n642\nGia\n742\n\n\nLisa\n819\nRyan\n682\n\n\n\n\n\\[ \\text{Percentile Rank} = \\frac{\\text{Number of values below x}+0.5}{\\text{Total nuber of values}}100 \\]\nThere are 14 scores below John.\n\\[\\frac{14 + 0.5}{16} \\times 100 = 90.625\\]\nThus, John is in approximately the 91st percentile. This means he scored better than about 91% of all individuals in the dataset.\n‚Äì\nSuppose we have the following ordered dataset of \\(n = 12\\) values:\n2, 4, 5, 7, 8, 9, 10, 12, 14, 15, 18, 21\nWe will try find:\n\nthe 25th percentile (\\(P_{25}\\))\nthe 50th percentile (\\(P_{50}\\), the median)\nthe 75th percentile (\\(P_{75}\\))\n\n\n\n\nFinding the 25th Percentile (\\(P_{25}\\))\nStep 1: Compute the index\n\\[\ni = \\frac{25}{100}(12) = 3\n\\]\nStep 2: Since \\(i = 3\\) is a whole number, we take the midpoint between the 3rd and 4th values.\n\n3rd value = 5\n\n4th value = 7\n\nThus, \\[\nP_{25} = \\frac{5 + 7}{2} = 6\n\\]\n\n\n\nFinding the 50th Percentile (\\(P_{50}\\)), the Median\nStep 1: Compute the index\n\\[\ni = \\frac{50}{100}(12) = 6\n\\]\nStep 2: Index is a whole number ‚Üí midpoint between 6th and 7th values.\n\n6th value = 9\n\n7th value = 11\n\nThus, \\[\nP_{50} = \\frac{9 + 11}{2} = 10\n\\]\n\n\n\nFinding the 75th Percentile (\\(P_{75}\\))\nStep 1: Compute the index\n\\[\ni = \\frac{75}{100}(12) = 9\n\\]\nStep 2: Whole number ‚Üí midpoint between 9th and 10th values.\n\n9th value = 14\n\n10th value = 18\n\nThus:\n\\[\nP_{75} = \\frac{14 + 18}{2} = 16\n\\]\n\n\n\nSummary\n\n\\(P_{25} = 6\\)\n\n\\(P_{50} = 10\\)\n\n\\(P_{75} = 16\\)\n\nPercentiles locate values relative to the distribution and help us describe how data spreads.\n\n\nQuartiles\nQuartiles divide the data into four equal parts. They are special percentiles:\n\n1st quartile (\\(Q_1\\)) = 25th percentile\n\n2nd quartile (\\(Q_2\\)) = 50th percentile (median)\n\n3rd quartile (\\(Q_3\\)) = 75th percentile\n\n\nThe Interquartile Range (IQR) measures the spread of the middle 50% of the data:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\n\n\nBoxplot Example\n\n\n\n\n\n\n\n\n\n\n\nExample Dataset\nConsider the following ordered dataset with 12 data points:\n2, 4, 5, 7, 8, 9, 10, 12, 14, 15, 18, 21\n\n\n\nStep 1: Find the Median (\\(Q_2\\))\nBecause \\(n = 12\\) is even:\n\n6th value = 9\n\n7th value = 10\n\n\\[\nQ_2 = \\frac{9 + 10}{2} = 9.5\n\\]\n\n\n\nStep 2: Find \\(Q_1\\) (25th percentile)\n\\(Q_1\\) is the median of the lower half:\nLower half: 2, 4, 5, 7, 8, 9\nMedian of lower half:\n\n3rd value = 5\n\n4th value = 7\n\n\\[\nQ_1 = \\frac{5 + 7}{2} = 6\n\\]\n\n\n\nStep 3: Find \\(Q_3\\) (75th percentile)\n\\(Q_3\\) is the median of the upper half:\nUpper half: 10, 12, 14, 15, 18, 21\nMedian of upper half:\n\n3rd value = 14\n\n4th value = 15\n\n\\[\nQ_3 = \\frac{14 + 15}{2} = 14.5\n\\]\n\n\n\n\nStep 4: Compute the IQR\n\\[\n\\text{IQR} = Q_3 - Q_1 = 14.5 - 6 = 8.5\n\\]\n\n\nSummary of Quartiles\n\n\n\nQuartile\nValue\n\n\n\n\n\\(Q_1\\)\n6\n\n\n\\(Q_2\\)\n9.5\n\n\n\\(Q_3\\)\n14.5\n\n\nIQR\n8.5\n\n\n\n\n\n\nAssingment in Percentiles\nWe are given the populations (in millions) of 12 U.S. cities.\nWe will:\n\nCalculate the 60th percentile of the population data\n\nFind the percentile rank of the population of Dallas (1.20 million)\n\n\n\nData Table\n\n\n\nCity\nPopulation (millions)\n\n\n\n\nNew York, NY\n8.18\n\n\nLos Angeles, CA\n3.79\n\n\nChicago, IL\n2.70\n\n\nHouston, TX\n2.10\n\n\nPhiladelphia, PA\n1.53\n\n\nPhoenix, AZ\n1.45\n\n\nSan Antonio, TX\n1.33\n\n\nSan Diego, CA\n1.31\n\n\nDallas, TX\n1.20\n\n\nSan Jose, CA\n0.95\n\n\nJacksonville, FL\n0.82\n\n\nIndianapolis, IN\n0.82",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#measures-of-association",
    "href": "materials/statistics/session2.html#measures-of-association",
    "title": "Statistics Session 02: Data Types",
    "section": "Measures of Association",
    "text": "Measures of Association\nSo far we were working on one dimasional.\nIn statistics, measures of association are numerical indices that quantify the strength and sometimes direction of a relationship between two or more variables. They help you understand how strongly variables are related (not necessarily causally), and the appropriate measure depends on the type of data (continuous, ordinal, categorical, binary, etc.)\nIn the scope of bootcamp we are going to concetrate on two measures:\n\nCovariance\nCorrelation\n\n\n\n\n\n\n\nNote\n\n\n\nDuring the Python module we are going to cover Chi-Squered test as well\n\n\n\nCovariance\n\nCovariance describes the direction of the linear relationship between two variables.\n\nA positive covariance means the variables tend to increase together.\n\nA negative covariance means one variable increases as the other decreases.\n\nThe magnitude of covariance does not indicate the strength of association.\n\nTherefore, covariance is useful for describing direction, but not strength.\n\n\\[\ns_{xy} =\n\\frac{\n\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n}{\nn -1\n}\n\\]\nWhere:\n\n\\(x_i\\), \\(y_i\\) are paired observations\n\n\\(\\bar{x}\\) = mean of the \\(x\\) values\n\n\\(\\bar{y}\\) = mean of the \\(y\\) values\n\n\\(n\\) = sample size\n\nCovariance uses the product of deviations to identify whether\n\\(x\\) and \\(y\\) move in the same direction (positive) or opposite directions (negative).\n\n\nCorrelation\nCorrelation standardizes covariance so that it always falls between ‚Äì1.0 and +1.0. It tells us both the direction and strength of a linear relationship between two variables.\n\\[\nr_{xy} = \\frac{s_{xy}}{s_x s_y}\n\\]\nExample:\n\\[\nr_{xy}\n= \\frac{12.8}{(1.79)(7.77)}\n= 0.920\n\\]\n\nThe values of \\(r_{xy}\\) range from ‚Äì1.0 to +1.0.\n\nWhen \\(r_{xy}\\) is positive, the linear relationship between \\(x\\) and \\(y\\) is positive:\nas \\(x\\) increases, \\(y\\) tends to increase.\n\nWhen \\(r_{xy}\\) is negative, the linear relationship between \\(x\\) and \\(y\\) is negative:\nas \\(x\\) increases, \\(y\\) tends to decrease.\n\nWhen \\(r_{xy} = 0\\), there is no linear relationship between the variables.\n\nA value of \\(r_{xy} = 0.920\\) indicates a strong positive linear relationship.\n\n\nHours of Study vs Exam Grade\nLet‚Äôs calculate both correlation and covariance between Hours of Study and Exam Grade step by step.\nThe table below shows the number of hours each student studied and their corresponding exam grade.\n\n\n\nStudent\nHours of Study\nExam Grade\n\n\n\n\n1\n3\n86\n\n\n2\n7\n98\n\n\n3\n5\n92\n\n\n4\n4\n83\n\n\n5\n3\n78\n\n\n6\n2\n79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance and Correlation: Full Manual Calculations\nWe use the dataset:\n\n\n\nStudent\nHours (\\(x_i\\))\nGrade (\\(y_i\\))\n\n\n\n\n1\n3\n86\n\n\n2\n7\n98\n\n\n3\n5\n92\n\n\n4\n4\n83\n\n\n5\n3\n78\n\n\n6\n2\n79\n\n\n\nNumber of observations:\n\\[ n = 6 \\]\n\n\n\nStep 1: Compute the Means\nHours:\n\\[ \\bar{x} = \\frac{3 + 7 + 5 + 4 + 3 + 2}{6} = \\frac{24}{6} = 4 \\]\nGrades:\n\\[ \\bar{y} = \\frac{86 + 98 + 92 + 83 + 78 + 79}{6} = \\frac{516}{6} = 86 \\]\n\n\n\nStep 2: Covariance Table\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(y_i\\)\n\\(x_i - \\bar{x}\\)\n\\(y_i - \\bar{y}\\)\n\\((x_i - \\bar{x})(y_i - \\bar{y})\\)\n\n\n\n\n3\n86\n-1\n0\n0\n\n\n7\n98\n3\n12\n36\n\n\n5\n92\n1\n6\n6\n\n\n4\n83\n0\n-3\n0\n\n\n3\n78\n-1\n-8\n8\n\n\n2\n79\n-2\n-7\n14\n\n\nSum\n\n\n\n64\n\n\n\n\n\n\nStep 3: Sample Covariance\nFormula:\n\\[\ns_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\]\nPlug in values:\n\\[\ns_{xy} = \\frac{64}{6 - 1} = \\frac{64}{5} = 12.8\n\\]\n\n\n\nStep 4: Compute Standard Deviations\nHours (\\(x\\)):\n\n\n\n\\(x_i\\)\n\\(x_i - \\bar{x}\\)\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n3\n-1\n1\n\n\n7\n3\n9\n\n\n5\n1\n1\n\n\n4\n0\n0\n\n\n3\n-1\n1\n\n\n2\n-2\n4\n\n\nSum\n\n16\n\n\n\nVariance:\n\\[\ns_x^2 = \\frac{16}{5} = 3.2\n\\]\nStd dev:\n\\[\ns_x = \\sqrt{3.2} \\approx 1.79\n\\]\n\nGrades (\\(y\\)):\n\n\n\n\\(y_i\\)\n\\(y_i - \\bar{y}\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n86\n0\n0\n\n\n98\n12\n144\n\n\n92\n6\n36\n\n\n83\n-3\n9\n\n\n78\n-8\n64\n\n\n79\n-7\n49\n\n\nSum\n\n302\n\n\n\nVariance:\n\\[\ns_y^2 = \\frac{302}{5} = 60.4\n\\]\nStd dev:\n\\[\ns_y = \\sqrt{60.4} \\approx 7.77\n\\]\n\n\n\nStep 5: Correlation\n\\[\nr_{xy} = \\frac{s_{xy}}{s_x s_y}\n\\]\nSubstitute:\n\\[\nr_{xy} = \\frac{12.8}{(1.79)(7.77)} = 0.9207\n\\]\nThis matches the Python result.\n\n\n\nFinal Values\n\nCovariance: \\(s_{xy} = 12.8\\)\nCorrelation: \\(r_{xy} = 0.9207\\)\n\n\n\n\nScatter Plot With Values\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Graphically\nBelow image shoes the correlation visually given different values\nr = 1, -1, 0.6, -0.4, 0\n\n\n\n\n\n\n\n\n\n\n\nCorrelation VS Causation\nHigh correlation does not imply causation:\n\nReverse Causality\nSample Selection\n\n\nThe Ice Cream and Drowning Season\n\nEvery summer, ice cream sales rise sharply.\nEvery summer, drowning incidents also rise sharply.\n\nThe two curves look almost identical for many countries.\n\n\n\n\n\n\nWarningReality\n\n\n\nBoth are driven by a third factor \\(\\rightarrow\\) a hot weather.\nPeople buy more ice cream, and people also go swimming more. Swimming increases exposure to drowning risk.\n\\[\\downarrow\\]\nThe common cause was hot weather\n\n\n\n\n\n\n\n\n\n\n\nReference: Tyler Vigen. Spurious Correlations. Hachette Books, 2015.\n\n\nThe Firefighters and Fire Damage Pattern\n\nLarge fires attract more firefighters.\n\nLarge fires also cause more property damage.\n\nThe two lines rise together almost perfectly.\n\n\n\n\n\n\nWarningReality\n\n\n\n\nFire size is the true driver. Bigger fires require more firefighters and naturally cause more damage.\n\nDeploying more firefighters does not increase damage ‚Äî the causality is reversed.\n\n\\[\\downarrow\\]\nThe common cause was fire severity\n\n\n\n\n\n\n\n\n\n\n\nReference: Gelman, Andrew & Jennifer Hill. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press, 2006.\n\n\n\nThe Salary and Coffee Consumption Illusion\n\nEmployees with higher salaries tend to drink more coffee.\n\nThe relationship appears almost linear.\n\nThis may tempt someone to conclude that drinking more coffee leads to higher pay.\n\n\n\n\n\n\nWarningReality\n\n\n\n\nSenior employees have both higher salaries and stronger coffee habits due to culture, workload, and meeting routines.\n\nCoffee does not cause higher salaries ‚Äî job level does.\n\n\\[\\downarrow\\] The common cause was job seniority\n\n\n\n\n\n\n\n\n\n\n\nReferences: Davenport, Thomas H., and Jeanne Harris. Competing on Analytics. Harvard Business Press, 2007.\n\n\nThe Shoe Size and Reading Score Example\n\nChildren with larger shoe sizes show higher reading scores.\n\nThe scatterplot forms a convincing positive trend.\n\nAt first glance, it seems like bigger feet lead to better reading ability.\n\n\n\n\n\n\nWarningReality\n\n\n\n\nOlder children have both bigger shoes and higher reading ability.\n\nAge is the confounding factor that explains the pattern.\n\n\\[\\downarrow\\]\nThe common cause was age\n\n\n\n\n\n\n\n\n\n\n\nReference Gravetter, Frederick & Larry Wallnau. Statistics for the Behavioral Sciences. Cengage Learning.\n\n\nSmoking and Lung Cancer Incidence\n\nSmoking rates show a steep rise across mid-20th century.\nLung cancer incidence follows with a lag of several years.\n\nThis trend appears across countries, genders, and time periods.\n\n\n\n\n\n\nWarningReality\n\n\n\n\nSmoking introduces carcinogenic compounds that mutate lung tissue.\n\nThe causal mechanism is biological and supported by decades of medical research.\n\n\\[\\downarrow\\] The mechanism was carcinogenic exposure",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session2.html#usefull-materials",
    "href": "materials/statistics/session2.html#usefull-materials",
    "title": "Statistics Session 02: Data Types",
    "section": "Usefull materials",
    "text": "Usefull materials\n\nBusiness Statistics 2 Chapter 3 and Chapter 4\nHere",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 02: Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#well-known-distributions",
    "href": "materials/statistics/slides/session3.html#well-known-distributions",
    "title": "Continuous Distributions",
    "section": "Well Known distributions",
    "text": "Well Known distributions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#topics",
    "href": "materials/statistics/slides/session3.html#topics",
    "title": "Continuous Distributions",
    "section": "Topics",
    "text": "Topics\n\nProbabilistic Distributions\nContinuous Distributions\nNormal Distribution\nUniform Distribution\nExponential Distribution",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#what-is-a-probabilistic-distribution",
    "href": "materials/statistics/slides/session3.html#what-is-a-probabilistic-distribution",
    "title": "Continuous Distributions",
    "section": "What Is a Probabilistic Distribution?",
    "text": "What Is a Probabilistic Distribution?\nIn Data Analytics, we rarely know outcomes with certainty.\nInstead of saying: ‚ÄúTomorrow‚Äôs sales will be exactly 120 units‚Äù\nWe say: ‚ÄúSales will most likely be around 120, but could reasonably vary‚Äù",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#what-a-distribution-describes",
    "href": "materials/statistics/slides/session3.html#what-a-distribution-describes",
    "title": "Continuous Distributions",
    "section": "What a Distribution Describes",
    "text": "What a Distribution Describes\nA probabilistic distribution answers:\n\nWhat values can a variable take?\nHow likely is each value (or range of values)?\nHow is uncertainty spread across those values?",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#from-raw-data-to-distribution",
    "href": "materials/statistics/slides/session3.html#from-raw-data-to-distribution",
    "title": "Continuous Distributions",
    "section": "From Raw Data to Distribution",
    "text": "From Raw Data to Distribution\nWhen we observe data repeatedly: - Customer purchases - Session durations - Delivery times\nPatterns emerge.\nA distribution summarizes these patterns as a model, not a table.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#random-variables",
    "href": "materials/statistics/slides/session3.html#random-variables",
    "title": "Continuous Distributions",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a numerical description of uncertainty.\nExamples:\n\nNumber of purchases today\nTime until customer churns\nEmail opened or not",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#discrete-vs-continuous",
    "href": "materials/statistics/slides/session3.html#discrete-vs-continuous",
    "title": "Continuous Distributions",
    "section": "Discrete vs Continuous",
    "text": "Discrete vs Continuous\n\nDiscrete: counted outcomes\n\nPurchases\nComplaints\nClick / no click\n\nContinuous: measured outcomes\n\nRevenue\nTime\nWeight\nDistance",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#continuous-distributions",
    "href": "materials/statistics/slides/session3.html#continuous-distributions",
    "title": "Continuous Distributions",
    "section": "Continuous Distributions",
    "text": "Continuous Distributions\nMost business variables are measured, not counted. Examples:\n\nRevenue\nCost\nTime\nDuration",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#continuous-core-rule",
    "href": "materials/statistics/slides/session3.html#continuous-core-rule",
    "title": "Continuous Distributions",
    "section": "Continuous: Core Rule",
    "text": "Continuous: Core Rule\n\n\n\n\n\n\n\nRemember\n\n\nFor a continuous random variable \\(X\\): \\[\nP(X = x) = 0\n\\] Only intervals have probability: \\[\nP(a \\le X \\le b)\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#example-human-heights",
    "href": "materials/statistics/slides/session3.html#example-human-heights",
    "title": "Continuous Distributions",
    "section": "Example: Human Heights",
    "text": "Example: Human Heights\nAssume population height:\n\nMean ‚âà 170 cm\nStandard deviation ‚âà 8 cm\n\nWe observe people one by one and build a distribution.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#small-sample-noisy-picture",
    "href": "materials/statistics/slides/session3.html#small-sample-noisy-picture",
    "title": "Continuous Distributions",
    "section": "Small Sample ‚Üí Noisy Picture",
    "text": "Small Sample ‚Üí Noisy Picture",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#larger-sample-structure-emerges",
    "href": "materials/statistics/slides/session3.html#larger-sample-structure-emerges",
    "title": "Continuous Distributions",
    "section": "Larger Sample ‚Üí Structure Emerges",
    "text": "Larger Sample ‚Üí Structure Emerges",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#probability-density-function-pdf",
    "href": "materials/statistics/slides/session3.html#probability-density-function-pdf",
    "title": "Continuous Distributions",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\n\nPDF describes density, not probability\nTotal area under the curve equals 1\n\\[\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#expected-value-and-variance",
    "href": "materials/statistics/slides/session3.html#expected-value-and-variance",
    "title": "Continuous Distributions",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nExpected Value (Mean):\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\\]\nVariance (Spread):\n\\[\nVar(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)\\,dx\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#definition",
    "href": "materials/statistics/slides/session3.html#definition",
    "title": "Continuous Distributions",
    "section": "Definition",
    "text": "Definition\nA Normal Distribution is:\n\nSymmetric\nBell-shaped\nFully defined by \\(\\mu\\) and \\(\\sigma^2\\)\n\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#normal-pdf",
    "href": "materials/statistics/slides/session3.html#normal-pdf",
    "title": "Continuous Distributions",
    "section": "Normal PDF",
    "text": "Normal PDF\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\n\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#normal-distribution-visualization",
    "href": "materials/statistics/slides/session3.html#normal-distribution-visualization",
    "title": "Continuous Distributions",
    "section": "Normal Distribution: Visualization",
    "text": "Normal Distribution: Visualization",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#standard-normal-distribution",
    "href": "materials/statistics/slides/session3.html#standard-normal-distribution",
    "title": "Continuous Distributions",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\nSpecial case where:\n\\[\n\\mu = 0,\\quad \\sigma = 1\n\\]\n\\[\nZ = \\frac{X-\\mu}{\\sigma}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#normal-vs-gaussian",
    "href": "materials/statistics/slides/session3.html#normal-vs-gaussian",
    "title": "Continuous Distributions",
    "section": "Normal vs Gaussian",
    "text": "Normal vs Gaussian\n\n\n\n\n\n\nNote\n\n\nNormal and Gaussian distributions are the same thing.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#definition-1",
    "href": "materials/statistics/slides/session3.html#definition-1",
    "title": "Continuous Distributions",
    "section": "Definition",
    "text": "Definition\n\\[\nX \\sim \\text{Uniform}(a,b)\n\\]\n\\[\nf(x) = \\frac{1}{b-a}, \\quad a \\le x \\le b\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#uniform-mean-and-variance",
    "href": "materials/statistics/slides/session3.html#uniform-mean-and-variance",
    "title": "Continuous Distributions",
    "section": "Uniform Mean and Variance",
    "text": "Uniform Mean and Variance\n\\[\nE[X] = \\frac{a+b}{2}\n\\]\n\\[\nVar(X) = \\frac{(b-a)^2}{12}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#uniform-visualization",
    "href": "materials/statistics/slides/session3.html#uniform-visualization",
    "title": "Continuous Distributions",
    "section": "Uniform Visualization",
    "text": "Uniform Visualization",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#definition-2",
    "href": "materials/statistics/slides/session3.html#definition-2",
    "title": "Continuous Distributions",
    "section": "Definition",
    "text": "Definition\nExponential distribution models waiting time until an event. \\[\nX \\sim \\text{Exp}(\\lambda)\n\\]\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#exponential-mean-and-variance",
    "href": "materials/statistics/slides/session3.html#exponential-mean-and-variance",
    "title": "Continuous Distributions",
    "section": "Exponential Mean and Variance",
    "text": "Exponential Mean and Variance\n\\[\nE[X] = \\frac{1}{\\lambda}\n\\]\n\\[\nVar(X) = \\frac{1}{\\lambda^2}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#exponential-visualization",
    "href": "materials/statistics/slides/session3.html#exponential-visualization",
    "title": "Continuous Distributions",
    "section": "Exponential Visualization",
    "text": "Exponential Visualization",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#spreadsheet-summary",
    "href": "materials/statistics/slides/session3.html#spreadsheet-summary",
    "title": "Continuous Distributions",
    "section": "Spreadsheet Summary",
    "text": "Spreadsheet Summary\nNormal: =NORM.INV(RAND(),Œº,œÉ), =NORM.DIST(x,Œº,œÉ,TRUE)\nUniform: =RAND()*(b-a)+a\nExponential: =-LN(1-RAND())/Œª",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session3.html#section",
    "href": "materials/statistics/slides/session3.html#section",
    "title": "Continuous Distributions",
    "section": "",
    "text": "Distributions model uncertainty\nContinuous variables require PDFs\nNormal, Uniform, Exponential model different behaviors\nCorrect distribution choice drives correct decisions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Continuous Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#population-vs-sample",
    "href": "materials/statistics/slides/session5.html#population-vs-sample",
    "title": "Sampling",
    "section": "Population vs Sample",
    "text": "Population vs Sample\nA population is the full group we want to understand.\nA sample is a smaller, representative subset used to infer population behavior.\n\nSampling allows us to study a few to understand the many.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#why-not-measure-everyone",
    "href": "materials/statistics/slides/session5.html#why-not-measure-everyone",
    "title": "Sampling",
    "section": "Why Not Measure Everyone?",
    "text": "Why Not Measure Everyone?\nMeasuring every individual is often:\n\nToo expensive\n\nToo slow\n\nLogistically impossible\n\nSometimes destructive\n\n\n\nA properly selected sample provides reliable insight with far less effort.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#why-businesses-prefer-sampling",
    "href": "materials/statistics/slides/session5.html#why-businesses-prefer-sampling",
    "title": "Sampling",
    "section": "Why Businesses Prefer Sampling",
    "text": "Why Businesses Prefer Sampling\nSampling helps organizations:\n\nReduce cost and effort\n\nMake faster decisions\n\nHandle large, continuous data streams\n\nAvoid unnecessary measurements\n\n\n\nSampling is a scientific tool, not a shortcut.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#risks-of-sampling",
    "href": "materials/statistics/slides/session5.html#risks-of-sampling",
    "title": "Sampling",
    "section": "Risks of Sampling",
    "text": "Risks of Sampling\nSampling introduces uncertainty because sample values rarely match population values.\n\nLarger samples reduce error\n\nGood design prevents bias\n\nStatistics helps quantify uncertainty",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#types-of-sampling",
    "href": "materials/statistics/slides/session5.html#types-of-sampling",
    "title": "Sampling",
    "section": "Types of Sampling",
    "text": "Types of Sampling\n\nProbability sampling: known chance of selection\n\nNon-probability sampling: selection probability unknown\n\nWe focus mainly on probability-based methods.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#simple-random-sampling",
    "href": "materials/statistics/slides/session5.html#simple-random-sampling",
    "title": "Sampling",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nEvery population member has an equal chance of selection.\n\nSimple Random",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#systematic-sampling-1",
    "href": "materials/statistics/slides/session5.html#systematic-sampling-1",
    "title": "Sampling",
    "section": "Systematic Sampling | 1",
    "text": "Systematic Sampling | 1\nSelect every k-th member of a population list.\nThe sampling constant:\n\\[\nk = \\frac{N}{n}\n\\]\n\nN: population size\n\nn: sample size\n\n\nSystematic",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#systematic-sampling-2",
    "href": "materials/statistics/slides/session5.html#systematic-sampling-2",
    "title": "Sampling",
    "section": "Systematic Sampling | 2",
    "text": "Systematic Sampling | 2\nSystematic sampling is easier to implement than a pure random sample.\n\nRisk: If the population has a repeating pattern aligned with \\(k\\), bias may occur.\nFix: Randomize the starting point before selecting every k-th observation.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#stratified-sampling-1",
    "href": "materials/statistics/slides/session5.html#stratified-sampling-1",
    "title": "Sampling",
    "section": "Stratified Sampling | 1",
    "text": "Stratified Sampling | 1\nUsed when population subgroups differ in ways important to the analysis.\nThe population is divided into strata, and each stratum is sampled proportionally.\nThis prevents over- or under-representation, especially when natural differences exist (e.g., junior vs senior engineers).\n\nStratified",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#stratified-sampling-2",
    "href": "materials/statistics/slides/session5.html#stratified-sampling-2",
    "title": "Sampling",
    "section": "Stratified Sampling | 2",
    "text": "Stratified Sampling | 2\nExample proportional allocation (sample size 200):\n\n\n\nStratum\nPopulation\nPop %\nSample\nCalculation\n\n\n\n\nJunior\n1200\n40%\n80\n\\(0.40 \\times 200\\)\n\n\nMid-Level\n900\n30%\n60\n\\(0.30 \\times 200\\)\n\n\nSenior\n450\n15%\n30\n\\(0.15 \\times 200\\)\n\n\nManagement\n450\n15%\n30\n\\(0.15 \\times 200\\)\n\n\nTotal\n3000\n100%\n200\n‚Äî",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#cluster-sampling-1",
    "href": "materials/statistics/slides/session5.html#cluster-sampling-1",
    "title": "Sampling",
    "section": "Cluster Sampling | 1",
    "text": "Cluster Sampling | 1\nTypical clusters: classrooms, neighborhoods, branch offices, cities.\nStratified vs Cluster\n\nStrata are homogeneous (e.g., only juniors).\n\nClusters are mini-populations,",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#cluster-sampling-2",
    "href": "materials/statistics/slides/session5.html#cluster-sampling-2",
    "title": "Sampling",
    "section": "Cluster Sampling | 2",
    "text": "Cluster Sampling | 2\n\nCluser",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#bootstrap-resampling",
    "href": "materials/statistics/slides/session5.html#bootstrap-resampling",
    "title": "Sampling",
    "section": "Bootstrap Resampling",
    "text": "Bootstrap Resampling\nResampling with replacement to estimate:\n\nVariability\n\nStandard errors\n\nConfidence around model metrics\n\nDoes not require strict distributional assumptions.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#bootstrap-resampling-1",
    "href": "materials/statistics/slides/session5.html#bootstrap-resampling-1",
    "title": "Sampling",
    "section": "Bootstrap Resampling",
    "text": "Bootstrap Resampling\n\nBootstrap",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#probability-sampling-comparison",
    "href": "materials/statistics/slides/session5.html#probability-sampling-comparison",
    "title": "Sampling",
    "section": "Probability Sampling Comparison",
    "text": "Probability Sampling Comparison\nA visual summary comparing simple random, systematic, stratified, and cluster sampling.\n\nComparison",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#non-probability-sampling",
    "href": "materials/statistics/slides/session5.html#non-probability-sampling",
    "title": "Sampling",
    "section": "Non-Probability Sampling",
    "text": "Non-Probability Sampling\nUsed when probability-based design is not possible.\n\nConvenience sampling: choose whoever is easiest to reach\n\nInternet polls: fast but biased\n\n\n\nNot reliable for inference or population estimates",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#sampling-error-vs-non-sampling-error",
    "href": "materials/statistics/slides/session5.html#sampling-error-vs-non-sampling-error",
    "title": "Sampling",
    "section": "Sampling Error vs Non-Sampling Error",
    "text": "Sampling Error vs Non-Sampling Error\nSampling Error: Difference between sample statistic and true parameter:\n\\[\n\\text{Sampling Error} = \\bar{x} - \\mu\n\\]\nNon-Sampling Error: Survey bias, measurement mistakes, data entry errors, non-response issues.\nCannot be fixed by increasing sample size.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#correction-factor",
    "href": "materials/statistics/slides/session5.html#correction-factor",
    "title": "Sampling",
    "section": "Correction Factor",
    "text": "Correction Factor\nWhen the proportion of the sample size is \\(&gt; 5%\\) we need to apply correction factor.\nCorrection factor\n\\[\\sqrt{\\frac{N - n}{N - 1}}\\]\n\nN: population size\nn: sample size\n\n\nthe higher the sample size the smaller FPC",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#correction-factor-effect",
    "href": "materials/statistics/slides/session5.html#correction-factor-effect",
    "title": "Sampling",
    "section": "Correction Factor Effect",
    "text": "Correction Factor Effect\nLet‚Äôs assume our population size is N=100\n\n\n\n\n\n\n\n\n\nSample Size (n)\nStandard Error\nFinite Correction Factor\nStandard Error with FPC\n\n\n\n\n40\n0.111\n0.778\n0.086\n\n\n60\n0.090\n0.636\n0.057\n\n\n80\n0.078\n0.449\n0.035\n\n\n100\n0.070\n0\n0",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#finite-population-correction",
    "href": "materials/statistics/slides/session5.html#finite-population-correction",
    "title": "Sampling",
    "section": "Finite Population Correction",
    "text": "Finite Population Correction\nWhen sampling more than 5% of a finite population:\n\\[\n\\sigma_{\\bar{x}} =\n\\frac{\\sigma}{\\sqrt{n}}\n\\sqrt{\\frac{N - n}{N - 1}}\n\\]\nThis correction adjusts the standard error to reflect reduced variability when sampling without replacement.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#introduction",
    "href": "materials/statistics/slides/session5.html#introduction",
    "title": "Sampling",
    "section": "Introduction",
    "text": "Introduction\nOne of the most important roles statistics plays is to:\n\ngather information from a sample\nuse that information to make statements about a population\nquantify uncertainty with confidence intervals\n\n\nConfidence Intervals help measure how certain we are about population parameters based on 1 sample.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#point-estimates",
    "href": "materials/statistics/slides/session5.html#point-estimates",
    "title": "Sampling",
    "section": "Point Estimates",
    "text": "Point Estimates\nA point estimate is a single value used to estimate a population parameter.\n\n\nsample mean \\(\\bar{x}\\)\nsample proportion \\(\\hat{p}\\)\n\n\n\nThey provide only a single best guess of the true population value.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confidence-interval",
    "href": "materials/statistics/slides/session5.html#confidence-interval",
    "title": "Sampling",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nA confidence interval for the mean is an interval estimate around a sample mean that provides a range within which the true population mean is expected to lie.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confidence-level",
    "href": "materials/statistics/slides/session5.html#confidence-level",
    "title": "Sampling",
    "section": "Confidence Level",
    "text": "Confidence Level\nA confidence level is the probability that the interval constructed from sample data will contain the population parameter of interest.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confidence-interval-intuition",
    "href": "materials/statistics/slides/session5.html#confidence-interval-intuition",
    "title": "Sampling",
    "section": "Confidence Interval Intuition",
    "text": "Confidence Interval Intuition\nThe purpose of generating a confidence interval is to provide an estimate for the true population mean by combining:\n\nthe sample mean \\(\\bar{x}\\)\n\nthe critical \\(z\\)-value\n\nthe standard error \\(\\sigma_{\\bar{x}}\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confidence-levels",
    "href": "materials/statistics/slides/session5.html#confidence-levels",
    "title": "Sampling",
    "section": "Confidence Levels",
    "text": "Confidence Levels\n\n\n\n\n\n\n\n\n\n\nConfidence Level\nŒ± (Significance Level)\nŒ±/2 (Each Tail)\nLower zŒ±/2\nUpper zŒ±/2\n\n\n\n\n90%\n0.10\n0.05\n-1.645\n1.645\n\n\n95%\n0.05\n0.025\n-1.960\n1.960\n\n\n99%\n0.01\n0.005\n-2.576\n2.576",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#case-study",
    "href": "materials/statistics/slides/session5.html#case-study",
    "title": "Sampling",
    "section": "Case Study",
    "text": "Case Study\nFor ABC Company we want to construct a CI for the avarage order size, based on sample mean of $129.2 with a 90% confidence level.\nwe will need:\n\n\nthe sample size: \\(\\bar{x}\\)\n\nthe population standard deviation \\(\\sigma\\)\n\n\n\nSuppose the sample mean was based on the 32 orders (\\(n=32\\)).\nLet‚Äôs also assumethe population standard deviation is equal to \\(\\sigma=40.602\\)\n\n\n\\[\n\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{\\$ 40.602}{\\sqrt{32}} = \\$7.173\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#margin-of-error-moe",
    "href": "materials/statistics/slides/session5.html#margin-of-error-moe",
    "title": "Sampling",
    "section": "Margin of Error (MoE)",
    "text": "Margin of Error (MoE)\n\\[MoE_{\\bar{x}} = z_{\\frac{\\alpha}{2}}\\sigma_{\\bar{x}}\\]\n\nConfidence Levels:\n\n\n\\[UCL = \\bar{x} + MoE\\]\n\n\\[LCL = \\bar{x} - MoE\\]\n\n\\[ \\Downarrow \\]\n\\[MoE_{90} = 1.645 \\cdot 7.176 = 11.80\\]\n\n\n\\[UCL = 129.2 + 11.8 = 141\\]\n\n\\[LCL = 129.2 - 11.8 = 117\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confindence-intervals",
    "href": "materials/statistics/slides/session5.html#confindence-intervals",
    "title": "Sampling",
    "section": "Confindence Intervals",
    "text": "Confindence Intervals\n\n\n\n\n\n\n\n\n\n\n\nCL\n\\(z_{Œ±/2}\\)\nSE\nMoE\nLCL\nUCL\n\n\n\n\n90%\n1.645\n7.173\n11.80\n117\n141\n\n\n95%\n1.960\n7.173\n14.06\n115\n143\n\n\n99%\n2.576\n7.174\n18.47\n110\n147",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confindence-intervals-multiple-samples",
    "href": "materials/statistics/slides/session5.html#confindence-intervals-multiple-samples",
    "title": "Sampling",
    "section": "Confindence Intervals | Multiple Samples",
    "text": "Confindence Intervals | Multiple Samples\nLet‚Äôs say the true population mean is 125\n\n\n\n\n\n\n\n\n\n\nSample\nSample Mean\nMargin of Error\nLower Limit\nUpper Limit\n\n\n\n\n1\n129.20\n11.80\n117.40\n141.00\n\n\n2\n132.00\n11.80\n120.20\n143.80\n\n\n3\n117.50\n11.80\n105.70\n129.30\n\n\n4\n128.20\n11.80\n116.40\n140.00\n\n\n5\n108.80\n11.80\n97.00\n120.60\n\n\n6\n130.10\n11.80\n118.30\n141.90\n\n\n7\n117.90\n11.80\n106.10\n129.70\n\n\n8\n120.10\n11.80\n108.30\n131.90\n\n\n9\n133.80\n11.80\n122.00\n145.60\n\n\n10\n119.00\n11.80\n107.20\n130.80",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#confindence-intervals-visual-interpretation",
    "href": "materials/statistics/slides/session5.html#confindence-intervals-visual-interpretation",
    "title": "Sampling",
    "section": "Confindence Intervals | Visual Interpretation",
    "text": "Confindence Intervals | Visual Interpretation",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#sample-sd",
    "href": "materials/statistics/slides/session5.html#sample-sd",
    "title": "Sampling",
    "section": "Sample SD",
    "text": "Sample SD\nThe sample standard deviation can always be computed directly from the data:\n\\[\ns = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n - 1}}\n\\]\n\nWhen \\(\\sigma\\) is unknown, we cannot rely on the normal distribution to calculate a confidence interval for the mean.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#t-distribution",
    "href": "materials/statistics/slides/session5.html#t-distribution",
    "title": "Sampling",
    "section": "t-distribution",
    "text": "t-distribution\nThe overall structure of the Confidence Interval formula remains the same:\n\n\\(\\sigma \\Rightarrow s\\)\n\nnormal distribution \\(\\Rightarrow\\) t-distribution\nz-table \\(\\Rightarrow\\) t-tabe",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#t-distribution-1",
    "href": "materials/statistics/slides/session5.html#t-distribution-1",
    "title": "Sampling",
    "section": "t-distribution",
    "text": "t-distribution",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#calculating-the-confidence-intervals",
    "href": "materials/statistics/slides/session5.html#calculating-the-confidence-intervals",
    "title": "Sampling",
    "section": "Calculating the Confidence Intervals",
    "text": "Calculating the Confidence Intervals\n\nCalculate Sample Mean\nCalcualate the Standard Error\nCalculate the Margin of Error\nCaluclate the Confidence Inervals\n\n\nNote the Confidence Level/Significance Level (\\(\\alpha\\)) must be given. Let‚Äôs assume that the \\(\\alpha = 0.05\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#weekly-vistors",
    "href": "materials/statistics/slides/session5.html#weekly-vistors",
    "title": "Sampling",
    "section": "Weekly Vistors",
    "text": "Weekly Vistors\nThe shop owner expects to have more than 90 visiters per week for the sustainable develepment.\n\n\n\nWeek\nVisitors\n\n\n\n\n1\n116\n\n\n2\n83\n\n\n3\n89\n\n\n4\n87\n\n\n5\n81\n\n\n6\n109\n\n\n7\n114\n\n\n8\n123\n\n\n9\n102\n\n\n10\n131\n\n\n11\n96\n\n\n12\n74\n\n\n13\n109\n\n\n14\n106\n\n\n15\n118\n\n\n16\n78\n\n\n17\n91\n\n\n18\n98",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#section",
    "href": "materials/statistics/slides/session5.html#section",
    "title": "Sampling",
    "section": "",
    "text": "In order to calculate the sample mean and the sample standard deviation we can use excel:\n\nDF: COUNT(B2:B19)-1 = 17\nMean: AVERAGE(B2:B19) = 100.3\nStandard Deviation | Sample: STDEV.S(B2:B19) = 16.6\nCritical t-score: T.INV.2T(alpha, df) = T.INV.2T(0.05,17) = 2.11\nMargin of Error: CONFIDENCE.T(alpha, sd,n) = CONFIDENCE.T(0.05, 16.6,18) = 8.25",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#margin-of-error",
    "href": "materials/statistics/slides/session5.html#margin-of-error",
    "title": "Sampling",
    "section": "Margin of Error",
    "text": "Margin of Error\n\\[\n\\hat{\\sigma}_{\\bar{x}} = \\frac{s}{\\sqrt{n}} = 3.92\n\\]\nConfidence Levels:\n\n\n\\[UCL = \\bar{x} + MoE\\]\n\n\\[LCL = \\bar{x} - MoE\\]\n\n\\[ \\Downarrow \\]\n\n\n\\[UCL = 100.3 + 8.25 = 108.55\\]\n\n\\[LCL = 100.3 - 8.25 = 92.05\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#section-1",
    "href": "materials/statistics/slides/session5.html#section-1",
    "title": "Sampling",
    "section": "",
    "text": "So far, we have focused on calculating a confidence interval and margin of error for a population when we know:\n\nThe confidence level\n\nThe sample size\n\nThe standard deviation\n\nNow we will try to reverse the process.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#goal",
    "href": "materials/statistics/slides/session5.html#goal",
    "title": "Sampling",
    "section": "Goal",
    "text": "Goal\nInstead of computing the margin of error from the sample size, we determine the sample size needed to achieve a desired margin of error, given:\n\nThe confidence level\n\nThe population standard deviatio",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#case-study-1",
    "href": "materials/statistics/slides/session5.html#case-study-1",
    "title": "Sampling",
    "section": "Case Study",
    "text": "Case Study\nA major telecom operator in 2024 needed an accurate estimate of average monthly mobile data usage. With heavy-streaming customers generating most of the traffic, the operator aimed to set plans that balance cost and capacity.\nTo achieve this at a 95% confidence level with a \\(\\pm2\\) GB margin of error, the operator calculated the sample size required to estimate the population mean efficiently and reliably.\nBased on internal analytics, the population standard deviation is believed to be \\(\\sigma = 8\\) GB.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session5.html#calculating-sample-size",
    "href": "materials/statistics/slides/session5.html#calculating-sample-size",
    "title": "Sampling",
    "section": "Calculating Sample Size",
    "text": "Calculating Sample Size\n\\[MoE_\\bar{x} = z_{\\frac{\\alpha}{2}} \\sigma_{\\bar{x}} = z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\Rightarrow  \\sqrt{n} = \\frac{z_{\\frac{\\alpha}{2}}\\sigma}{MoE}\\]\n\\[\nn = \\left( \\frac{z_{\\alpha/2}\\, 2\\sigma}{\\text{MoE}} \\right)^2\n\\]\n\\[\nn = \\left( \\frac{z_{\\alpha/2}\\, 2\\sigma}{\\text{MoE}} \\right)^2\n\\]\nRemember for a 95% confidence level critical z-value would be \\(z_{\\alpha/2} = 1.96\\)\nCalculate and round up:\n\\[\nn = \\left( \\frac{1.96 \\cdot 8}{2} \\right)^2 = 61.46 = 62\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#agenda",
    "href": "materials/statistics/slides/intro.html#agenda",
    "title": "Course Intro",
    "section": "Agenda",
    "text": "Agenda\n\n\n\nA little bit about myself\nWhat is Data Analytics?\nWho is a Data Analyist?\nProgram Overview\n\nQ&A",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#what-is-data-analytics",
    "href": "materials/statistics/slides/intro.html#what-is-data-analytics",
    "title": "Course Intro",
    "section": "What is Data Analytics?",
    "text": "What is Data Analytics?\n\n‚Ä¶ turning data into understanding and action.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#business-questions",
    "href": "materials/statistics/slides/intro.html#business-questions",
    "title": "Course Intro",
    "section": "Business Questions‚Ä¶",
    "text": "Business Questions‚Ä¶\n\nWhy are customers leaving?\nWhich products generate the most revenue?\nWhich marketing channel performs best?\nWhere are operational inefficiencies?\nWhat drives customer behavior?",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#types-of-work",
    "href": "materials/statistics/slides/intro.html#types-of-work",
    "title": "Course Intro",
    "section": "Types of Work‚Ä¶",
    "text": "Types of Work‚Ä¶\n\nCleaning data\n\nExploring data\n\nVisualizing patterns\n\nRunning analyses\n\nCommunicating insights",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#how-companies-use-analytics",
    "href": "materials/statistics/slides/intro.html#how-companies-use-analytics",
    "title": "Course Intro",
    "section": "How Companies Use Analytics",
    "text": "How Companies Use Analytics\nAnalytics supports decisions across the entire business:\n\n\nMarketing: ROI, segments, attribution\n\nSales: funnel, forecasting\n\nFinance: profitability, risk\n\nOperations: optimization, efficiency\n\nProduct: customer journeys, A/B tests\n\n\n\nAnalytics isn‚Äôt optional.\nIt‚Äôs how companies stay competitive.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#decision-making-with-analytics",
    "href": "materials/statistics/slides/intro.html#decision-making-with-analytics",
    "title": "Course Intro",
    "section": "Decision-Making with Analytics",
    "text": "Decision-Making with Analytics\nThe cycle:\n\nDefine the business problem\n\nIdentify and gather data\n\nClean and structure it\n\nAnalyze patterns\n\nBuild dashboards & reports\n\nRecommend actions\n\nMonitor results and iterate",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#types-of-analytics",
    "href": "materials/statistics/slides/intro.html#types-of-analytics",
    "title": "Course Intro",
    "section": "Types of Analytics",
    "text": "Types of Analytics\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescriptive\nDiagnostic\nPredictive\nPrescriptive\n\n\n\n\nPurpose\nLearn what happened.\nLearn why something happened.\nLearn what is likely to happen.\nLearn what to do.\n\n\nOutput\nStatic reports with KPIs.\nReports with drill-down, slicing, and dicing capabilities.\nForecasts.\nActionable recommendations.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#technical-skills",
    "href": "materials/statistics/slides/intro.html#technical-skills",
    "title": "Course Intro",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nSQL\n\nPython\n\nExcel\n\nStatistics\n\nVisualization tools (Tableau, Power BI)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#business",
    "href": "materials/statistics/slides/intro.html#business",
    "title": "Course Intro",
    "section": "Business",
    "text": "Business\n\nProblem formulation\n\nInterpretation\n\nData storytelling\n\nCommunication",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#about-bootcamp",
    "href": "materials/statistics/slides/intro.html#about-bootcamp",
    "title": "Course Intro",
    "section": "About Bootcamp",
    "text": "About Bootcamp\n\n\nInstructor: Karen Hovhannisyan\n Duration: 6 Months\nModules: 6\nTech Stack: PostgreSQL ¬∑ Python ¬∑ Tableau ¬∑ Docker ¬∑ Git/GitHub\nTools: VScode ¬∑ Pgadmin ¬∑ Terminal ¬∑ Excel",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#goal",
    "href": "materials/statistics/slides/intro.html#goal",
    "title": "Course Intro",
    "section": "Goal",
    "text": "Goal\nPrepare analytical thinkers , not just tool users\n\n\n\nEquipping participants with a complete, functional, and standardized analytics environment from coding to version control, databases, and visualization tools.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#learning-journey",
    "href": "materials/statistics/slides/intro.html#learning-journey",
    "title": "Course Intro",
    "section": "Learning Journey",
    "text": "Learning Journey",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#who-this-program-is-for",
    "href": "materials/statistics/slides/intro.html#who-this-program-is-for",
    "title": "Course Intro",
    "section": "Who This Program is For",
    "text": "Who This Program is For\n\nStudents\n\nProfessionals switching careers\n\nAnalysts wanting to upskill",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#module-1-statistical-thinking",
    "href": "materials/statistics/slides/intro.html#module-1-statistical-thinking",
    "title": "Course Intro",
    "section": "Module 1 | Statistical Thinking",
    "text": "Module 1 | Statistical Thinking\n\n\nOutcome: Build a foundational analytical mindset and statistical reasoning before moving to SQL and Python.\n\n\nDuration: 4 weeks\nFoundations: descriptive ‚Üí inferential, sampling, distributions\nHypotheses, p-values, confidence intervals\nMini storytelling project",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#module-2-sql",
    "href": "materials/statistics/slides/intro.html#module-2-sql",
    "title": "Course Intro",
    "section": "Module 2 | SQL",
    "text": "Module 2 | SQL\nOutcome: Develop the ability to query, transform, and aggregate data efficiently, creating analytical datasets that form the foundation for Python-based analysis and dashboards.\n\n\nDuration: 6 weeks\nBasic queries\nJoins, CTEs, window functions\nStored procedures, UDFs, CUBE/ROLLUP, materialized views\nExport analytical tables ‚Üí CSV (for Tableau)\nMini storytelling project",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#module-3-python",
    "href": "materials/statistics/slides/intro.html#module-3-python",
    "title": "Course Intro",
    "section": "Module 3 | Python",
    "text": "Module 3 | Python\nOutcome: Strengthen analytical programming skills by automating data preparation, performing statistical tests, and applying predictive techniques to extract insights from SQL data.\n\n\nDuration: 7 weeks\nPython fundamentals, pandas, visualization\nA/B testing, regression, clustering (high-level)\nSQLAlchemy: read/write with PostgreSQL; ETL\nMini project",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#module-4-tableau",
    "href": "materials/statistics/slides/intro.html#module-4-tableau",
    "title": "Course Intro",
    "section": "Module 4 | Tableau",
    "text": "Module 4 | Tableau\nOutcome: Learn to visualize, interpret, and communicate insights effectively through interactive and dynamic dashboards that transform analytical results into compelling business narratives.\n\n\nDuration: 5 weeks\nVisual analytics, LODs, interactivity\nDesign principles & performance\nBuild public dashboards (Tableau Public)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#module-5-capstone",
    "href": "materials/statistics/slides/intro.html#module-5-capstone",
    "title": "Course Intro",
    "section": "Module 5 | Capstone",
    "text": "Module 5 | Capstone\nIntegrate all acquired skills to design and deliver a complete end-to-end analytics project. From raw data to visual storytelling,demonstrating readiness for real-world analytics roles.\n\n\nDuration: 3 weeks\nFull pipeline: Problem Definition ‚Üí SQL ‚Üí Python ‚Üí Tableau\nGitHub repo + Tableau Public link\nPortfolio\nFinal presentation",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#connected-the-dots",
    "href": "materials/statistics/slides/intro.html#connected-the-dots",
    "title": "Course Intro",
    "section": "Connected the dots",
    "text": "Connected the dots\n\nModule 1 ‚Äî Statistical Thinking: Build a foundational analytical mindset and statistical reasoning before moving to SQL and Python.\nModule 2 ‚Äî SQL: Develop the ability to query, transform, and aggregate data efficiently, creating analytical datasets that form the foundation for Python-based analysis and dashboards.\nModule 3 ‚Äî Python: Strengthen analytical programming skills by automating data preparation, performing statistical tests, and applying predictive techniques to extract insights from SQL data.\nModule 4 ‚Äî Tableau: Learn to visualize, interpret, and communicate insights effectively through interactive and dynamic dashboards that transform analytical results into compelling business narratives.\nModule 5 ‚Äî Capstone: Integrate all acquired skills to design and deliver a complete end-to-end analytics project‚Äîfrom raw data to visual storytelling‚Äîdemonstrating readiness for real-world analytics roles.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#course-logistics",
    "href": "materials/statistics/slides/intro.html#course-logistics",
    "title": "Course Intro",
    "section": "Course Logistics",
    "text": "Course Logistics\n\n\n\n\n\n\n\nLecture Notes\n\n\nAll the course materials you can find here\n\n\n\n\nThe material includes:\n\nSlides\nExtended Reading\nHomework\nProgram installation and setup",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/intro.html#qa",
    "href": "materials/statistics/slides/intro.html#qa",
    "title": "Course Intro",
    "section": "Q&A",
    "text": "Q&A",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Course Intro"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#topics",
    "href": "materials/statistics/slides/session4.html#topics",
    "title": "Discrete Distributions",
    "section": "Topics",
    "text": "Topics\n\nDiscrete Distributions\nPoisson Distribution\nBernuli Distribution\nCentral Limit Theorem | CLT",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#when-do-we-have-a-discrete-distributions",
    "href": "materials/statistics/slides/session4.html#when-do-we-have-a-discrete-distributions",
    "title": "Discrete Distributions",
    "section": "When do we have a Discrete Distributions?",
    "text": "When do we have a Discrete Distributions?\nA distribution is discrete if:\n\nThe random variable takes countable values\nProbabilities are assigned to exact outcomes\nThe total probability is obtained by summing, not integrating\n\nExamples of Discrete Outcomes\n\n\nNumber of purchases made today\n\nNumber of customers who churned this month\n\nNumber of defects in a production batch\n\nWhether a user clicked an ad (yes / no)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#discrete-random-variables",
    "href": "materials/statistics/slides/session4.html#discrete-random-variables",
    "title": "Discrete Distributions",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA discrete random variable is a numerical description of a countable outcome.\nTypical forms:\n\nBinary outcomes: \\(X \\in \\{0,1\\}\\)\nCounts: \\(X = 0,1,2,3,\\dots\\)\n\nExamples:\n\n\n\\(X = 1\\) if a customer churns, \\(0\\) otherwise\n\n\\(X\\) = number of purchases per user\n\n\\(X\\) = number of support tickets per day",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#probability-mass-function",
    "href": "materials/statistics/slides/session4.html#probability-mass-function",
    "title": "Discrete Distributions",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nDiscrete distributions are described by a Probability Mass Function (PMF).\nThe PMF answers the question:\n\nWhat is the probability that the random variable equals a specific value?\n\n\nMathematically:\n\\[\nP(X = x)\n\\]\nCore Properties of a PMF\n\\[\n\\sum_x P(X = x) = 1\n\\]\n\n\\(0 \\le P(X = x) \\le 1\\)\nProbabilities are assigned to exact values\nThe total probability sums to 1:",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#imagine",
    "href": "materials/statistics/slides/session4.html#imagine",
    "title": "Discrete Distributions",
    "section": "Imagine",
    "text": "Imagine\nImagine a retail store observing customer foot traffic.\nEvery 10 minutes, a random number of customers enter the store.\nOver many days, management notices:\n\nSome intervals have 1‚Äì2 customers\n\nSome have 5‚Äì6 customers\n\nOccasionally, none\n\n\nYet the average stays roughly the same.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#probability-mass-function-1",
    "href": "materials/statistics/slides/session4.html#probability-mass-function-1",
    "title": "Discrete Distributions",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\\[\nP(X = x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0,1,2,\\dots\n\\]\n\nThis formula gives the probability of seeing exactly \\(x\\) events",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#expected-value-and-variance",
    "href": "materials/statistics/slides/session4.html#expected-value-and-variance",
    "title": "Discrete Distributions",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\n\\[\nE[X] = \\lambda\n\\]\n\\[\nVar(X) = \\lambda\n\\]\n\nThe average number of events equals \\(\\lambda\\)\n\nThe uncertainty (variance) grows at the same rate\n\n\n\nLarge \\(\\lambda\\) ‚Üí frequent events\n\nSmall \\(\\lambda\\) ‚Üí rare events",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#business-applications-of-poisson-distribution",
    "href": "materials/statistics/slides/session4.html#business-applications-of-poisson-distribution",
    "title": "Discrete Distributions",
    "section": "Business Applications of Poisson Distribution",
    "text": "Business Applications of Poisson Distribution\nPoisson models are widely used in practice for:\n\ncustomer arrivals and foot traffic\n\ncall center volume estimation\n\nwebsite click and impression counts\n\nmanufacturing defects\n\nincident and outage reporting",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#poisson-process",
    "href": "materials/statistics/slides/session4.html#poisson-process",
    "title": "Discrete Distributions",
    "section": "Poisson Process",
    "text": "Poisson Process\n\nExponential distribution models time between events\nPoisson distribution models number of events\n\nThey describe the same process from different perspectives.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#visualization",
    "href": "materials/statistics/slides/session4.html#visualization",
    "title": "Discrete Distributions",
    "section": "Visualization",
    "text": "Visualization\n\nLeft plot ‚Üí PMF for \\(\\lambda = 3\\) and \\(\\lambda = 10\\)\n\nRight plot ‚Üí likelihood as a function of \\(\\lambda\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#two-possible-outcomes",
    "href": "materials/statistics/slides/session4.html#two-possible-outcomes",
    "title": "Discrete Distributions",
    "section": "Two Possible Outcomes",
    "text": "Two Possible Outcomes\n\nDid an event happen or not?\n\n\nA Bernoulli random variable has only two possible outcomes:\n\nsuccess\n\nfailure\n\nThese outcomes can be represented in multiple equivalent ways depending on context:\n\nyes / no\n\n1 / 0\n\nclick / no click\n\npurchase / no purchase\n\ntail / head (’≤’∏÷Ç’∑ / ’£’´÷Ä)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#mathematical-definition",
    "href": "materials/statistics/slides/session4.html#mathematical-definition",
    "title": "Discrete Distributions",
    "section": "Mathematical Definition",
    "text": "Mathematical Definition\nA random variable \\(X\\) follows a Bernoulli distribution if:\n\\[\nX \\sim \\text{Bernoulli}(p)\n\\]\nwhere:\n\n\\(p \\in [0,1]\\) is the probability of success",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#what-is-a-bernoulli-trial",
    "href": "materials/statistics/slides/session4.html#what-is-a-bernoulli-trial",
    "title": "Discrete Distributions",
    "section": "What Is a Bernoulli Trial",
    "text": "What Is a Bernoulli Trial\nA Bernoulli trial is a single experiment with:\n\nexactly one attempt\nexactly two possible outcomes\na fixed probability of success\n\nEach trial is assumed to be independent.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#examples-of-bernoulli-trials",
    "href": "materials/statistics/slides/session4.html#examples-of-bernoulli-trials",
    "title": "Discrete Distributions",
    "section": "Examples of Bernoulli Trials",
    "text": "Examples of Bernoulli Trials\nCommon real-world Bernoulli trials include:\n\nDid a user click an advertisement?\n\nDid a customer complete a purchase?\n\nDid a transaction fail?\n\nDid a device respond to a health check?\n\nEach produces a binary outcome.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#real-world-narrative",
    "href": "materials/statistics/slides/session4.html#real-world-narrative",
    "title": "Discrete Distributions",
    "section": "Real-World Narrative",
    "text": "Real-World Narrative\nConsider an online store tracking customer conversions.\nFor each user session:\n\nPurchase ‚Üí 1\n\nNo purchase ‚Üí 0\n\nAcross thousands of sessions:\n\nSome users convert\n\nMost users do not\n\nEach session is evaluated independently, with the same underlying probability of conversion.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#business-use-cases",
    "href": "materials/statistics/slides/session4.html#business-use-cases",
    "title": "Discrete Distributions",
    "section": "Business Use Cases",
    "text": "Business Use Cases\nTypical Bernoulli use cases in analytics:\n\nad click behavior\n\nconversion events\n\nemail engagement\n\nfraud detection flags\n\nchurn indicators\n\nBernoulli is the building block for many advanced models.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#probability-mass-function-pmf",
    "href": "materials/statistics/slides/session4.html#probability-mass-function-pmf",
    "title": "Discrete Distributions",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nBecause Bernoulli is a discrete distribution, it is described by a Probability Mass Function (PMF):\n\\[\nP(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0,1\\}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#pmf-interpretation",
    "href": "materials/statistics/slides/session4.html#pmf-interpretation",
    "title": "Discrete Distributions",
    "section": "PMF Interpretation",
    "text": "PMF Interpretation\nThis formula yields exactly two probabilities:\n\n\\(P(X = 1) = p\\)\n\n\\(P(X = 0) = 1 - p\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#expected-value-and-variance-1",
    "href": "materials/statistics/slides/session4.html#expected-value-and-variance-1",
    "title": "Discrete Distributions",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nFor a Bernoulli random variable:\n\\[\nE[X] = p\n\\]\n\\[\nVar(X) = p(1-p)\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#interpretation-of-moments",
    "href": "materials/statistics/slides/session4.html#interpretation-of-moments",
    "title": "Discrete Distributions",
    "section": "Interpretation of Moments",
    "text": "Interpretation of Moments\n\nExpected value equals the probability of success\nVariance is largest at \\(p = 0.5\\)\nVariance shrinks as outcomes become more certain\n\n\n\nLarge \\(p\\) ‚Üí success likely\n\nSmall \\(p\\) ‚Üí success rare",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#concrete-examples",
    "href": "materials/statistics/slides/session4.html#concrete-examples",
    "title": "Discrete Distributions",
    "section": "Concrete Examples",
    "text": "Concrete Examples\n\n\\(p = 0.02\\) ‚Üí 2% conversion rate\n\n\\(p = 0.35\\) ‚Üí 35% email open rate\n\n\\(p = 0.90\\) ‚Üí highly reliable system",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#business-context",
    "href": "materials/statistics/slides/session4.html#business-context",
    "title": "Discrete Distributions",
    "section": "Business Context",
    "text": "Business Context\nAn e-commerce company records whether each visitor completes a purchase.\nEach session is encoded as:\n\n1 ‚Üí purchase\n\n0 ‚Üí no purchase\n\nObserved Data\n{.smaller}\n\n\n\nSession\nPurchase\n\n\n\n\n1\n0\n\n\n2\n1\n\n\n3\n0\n\n\n4\n0\n\n\n5\n1\n\n\n6\n0\n\n\n7\n1\n\n\n8\n0\n\n\n9\n0\n\n\n10\n1",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#summary-statistics",
    "href": "materials/statistics/slides/session4.html#summary-statistics",
    "title": "Discrete Distributions",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nLet \\(X\\) be the purchase indicator per session.\n\nNumber of sessions: \\(n = 10\\)\n\nTotal purchases: \\(S = 4\\)\n\nSample mean:\n\\[\n\\bar{x} = \\frac{4}{10} = 0.4\n\\]\nEstimated probability:\n\\[\n\\hat{p} = 0.4\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#defining-the-random-variable",
    "href": "materials/statistics/slides/session4.html#defining-the-random-variable",
    "title": "Discrete Distributions",
    "section": "Defining the Random Variable",
    "text": "Defining the Random Variable\n\\[\nX =\n\\begin{cases}\n1 & \\text{if a purchase occurs} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\n\nA Bernoulli model is an appropriate first representation of this process.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#final-intuition-summary",
    "href": "materials/statistics/slides/session4.html#final-intuition-summary",
    "title": "Discrete Distributions",
    "section": "Final Intuition Summary",
    "text": "Final Intuition Summary\n\nUse Uniform for ‚Äúrandom within limits.‚Äù\nUse Exponential for ‚Äútime until next event.‚Äù\nUse Poisson for ‚Äúnumber of events per interval.‚Äù\nUse Bernoulli for ‚Äúone yes/no outcome‚Äù.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#the-magic-of-the-normal-distribution",
    "href": "materials/statistics/slides/session4.html#the-magic-of-the-normal-distribution",
    "title": "Discrete Distributions",
    "section": "The Magic of the Normal Distribution",
    "text": "The Magic of the Normal Distribution\nThe Normal distribution appears everywhere in data.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#what-the-central-limit-theorem-says",
    "href": "materials/statistics/slides/session4.html#what-the-central-limit-theorem-says",
    "title": "Discrete Distributions",
    "section": "What the Central Limit Theorem Says",
    "text": "What the Central Limit Theorem Says\nThe Central Limit Theorem states:\n\nWhen we take the mean of many independent random variables\nregardless of the shape of the original distribution\nprovided they have finite mean and variance\nthe distribution of the sample mean becomes approximately Normal as the sample size grows",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#in-other-words",
    "href": "materials/statistics/slides/session4.html#in-other-words",
    "title": "Discrete Distributions",
    "section": "in other words‚Ä¶",
    "text": "in other words‚Ä¶\n\n.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#intuition-behind-the-clt",
    "href": "materials/statistics/slides/session4.html#intuition-behind-the-clt",
    "title": "Discrete Distributions",
    "section": "Intuition Behind the CLT",
    "text": "Intuition Behind the CLT\n\nReal-world data often come from skewed or irregular distributions.\nBut when we average many small effects, the result tends toward a bell shape.\nThis ‚Äúaveraging effect‚Äù is why the Normal distribution is so common.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#why-this-matters-in-analytics",
    "href": "materials/statistics/slides/session4.html#why-this-matters-in-analytics",
    "title": "Discrete Distributions",
    "section": "Why This Matters in Analytics",
    "text": "Why This Matters in Analytics\nThe CLT allows us to:\n\nbuild confidence intervals\nperform hypothesis tests\nuse Normal-based statistical tools\napproximate distributions of sums and averages\n\nEven when the original data are not Normal.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#non-normal-data-example",
    "href": "materials/statistics/slides/session4.html#non-normal-data-example",
    "title": "Discrete Distributions",
    "section": "Non-Normal Data Example",
    "text": "Non-Normal Data Example\nThink about:\n\n\nwaiting times (often skewed)\nrevenue per customer (heavy-tailed)\ncustomer arrival counts (discrete)\n\nIndividually, these can be far from Normal.\nWhen we repeatedly take many samples and compute means:\n\nthe distribution of the means becomes more symmetric\nit approaches the characteristic bell curve\nthis happens regardless of the original shape",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#simulation-visualizing-clt",
    "href": "materials/statistics/slides/session4.html#simulation-visualizing-clt",
    "title": "Discrete Distributions",
    "section": "Simulation: Visualizing CLT",
    "text": "Simulation: Visualizing CLT",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#what-the-simulation-shows",
    "href": "materials/statistics/slides/session4.html#what-the-simulation-shows",
    "title": "Discrete Distributions",
    "section": "What the Simulation Shows",
    "text": "What the Simulation Shows\n\nFor small n, the distribution of sample means is skewed.\nAs n increases:\n\nthe distribution becomes more symmetric\nit approaches a bell shape\neven though the original data were not Normal",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#conditions-for-clt",
    "href": "materials/statistics/slides/session4.html#conditions-for-clt",
    "title": "Discrete Distributions",
    "section": "Conditions for CLT",
    "text": "Conditions for CLT\nBefore applying CLT:\n\nsamples should be independent\nsample size should be sufficiently large\nunderlying distribution must have finite variance\n\nThese ensure the sample mean behaves approximately Normal.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#practical-rules-of-thumb-heuristics-in-analytics",
    "href": "materials/statistics/slides/session4.html#practical-rules-of-thumb-heuristics-in-analytics",
    "title": "Discrete Distributions",
    "section": "Practical Rules of Thumb (heuristics in analytics)",
    "text": "Practical Rules of Thumb (heuristics in analytics)\n\nIf original data are nearly Normal ‚Üí small n is ok\n\nIf data are skewed ‚Üí larger n is needed\n\nn ‚â• 30 often yields good Normal approximation",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session4.html#applying-clt-in-practice",
    "href": "materials/statistics/slides/session4.html#applying-clt-in-practice",
    "title": "Discrete Distributions",
    "section": "Applying CLT in Practice",
    "text": "Applying CLT in Practice\n\nconfidence interval construction\n\nhypothesis testing\n\nA/B testing rules\n\nregression inference\n\nThese methods assume approximate Normality of averages.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Discrete Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/index.html",
    "href": "materials/statistics/index.html",
    "title": "Statistics",
    "section": "",
    "text": "Statistics Session 01: Data Layers and Bias in Data\n\n\nWhat is Data Analytics, Data Layers, Types of Biases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Session 02: Data Types\n\n\nData Types, Central Tendancy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Session 03: Probabilistic Distributions\n\n\nContinues Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Session 04: Probabilistic Distributions\n\n\nDiscrete Probability Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Session 05: Sampling\n\n\nSampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Session 06: Inferential Statistics\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html",
    "href": "materials/statistics/session4.html",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "",
    "text": "Discrete Distributions\nPoisson Distribution\nBernuli Distribution\nCentral Limit Theorem | CLT",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#topics",
    "href": "materials/statistics/session4.html#topics",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "",
    "text": "Discrete Distributions\nPoisson Distribution\nBernuli Distribution\nCentral Limit Theorem | CLT",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#discrete-distributions",
    "href": "materials/statistics/session4.html#discrete-distributions",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Discrete Distributions",
    "text": "Discrete Distributions\nUp to this point, we focused on continuous distributions, where variables can take infinitely many values within a range.\nNow we move to discrete distributions, which model situations where outcomes are countable.\nThis shift is important because many business problems are naturally discrete.\nA distribution is discrete if:\n\nThe random variable takes countable values\nProbabilities are assigned to exact outcomes\nThe total probability is obtained by summing, not integrating\n\nExamples of discrete outcomes:\n\nNumber of purchases made today\n\nNumber of customers who churned this month\n\nNumber of defects in a production batch\n\nWhether a user clicked an ad (yes / no)\n\n\nDiscrete Random Variables\nA discrete random variable is a numerical description of a countable outcome.\nTypical forms:\n\nBinary outcomes: \\(X \\in \\{0,1\\}\\)\nCounts: \\(X = 0,1,2,3,\\dots\\)\n\nExamples:\n\n\\(X = 1\\) if a customer churns, \\(0\\) otherwise\n\n\\(X\\) = number of purchases per user\n\n\\(X\\) = number of support tickets per day\n\n\n\nProbability Mass Function\nDiscrete distributions are described by a Probability Mass Function (PMF).\nThe PMF answers the question:\n\nWhat is the probability that the random variable equals a specific value?\n\nMathematically:\n\\[\nP(X = x)\n\\]\nCore Properties of a PMF\n\n\\(0 \\le P(X = x) \\le 1\\)\nProbabilities are assigned to exact values\nThe total probability sums to 1:\n\n\\[\n\\sum_x P(X = x) = 1\n\\]\nDiscrete vs Continuous\n\n\n\nAspect\nDiscrete\nContinuous\n\n\n\n\nPossible values\nCountable\nInfinite\n\n\nProbability at a point\nMeaningful\nAlways 0\n\n\nMathematical tool\nSum\nIntegral\n\n\nFunction\nPMF\nPDF\n\n\n\n\n\n\n\n\n\nWarningDo Not Confuse Them\n\n\n\nIt is critical to clearly distinguish between PMF and PDF.\n\nPMF (Probability Mass Function):\nUsed for discrete random variables. It assigns probability to exact values: \\[\nP(X = x)\n\\]\nPDF (Probability Density Function)\nUsed for continuous random variables. It does not give probabilities at exact points: \\[\nP(X = x) = 0\n\\]\n\nKey differences:\n\nPMF values are actual probabilities\nPDF values are densities, not probabilities\nPMFs sum to 1\nPDFs integrate to 1\n\nUsing a PDF when the data is discrete (or vice versa) leads to incorrect interpretations and wrong conclusions.\n\n\n\n\nWhy Descrete Destributions are Essential?\nDiscrete distributions are essential when modeling:\n\nDecisions (yes / no)\nEvent counts\nConversion funnels\nOperational metrics\nRisk events\n\nThey allow analysts to:\n\nQuantify uncertainty in decisions\nEstimate probabilities of rare events\nOptimize policies and thresholds",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#poisson-distribution",
    "href": "materials/statistics/session4.html#poisson-distribution",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe Poisson Distribution models a very common business question:\n\nHow many times does an event occur within a fixed time (or space) interval?\n\nExamples of such intervals:\n\nper hour\n\nper day\n\nper kilometer\n\nper page view session\n\nA random variable \\(X\\) follows a Poisson distribution if:\n\\[\nX \\sim \\text{Poisson}(\\lambda)\n\\]\nwhere:\n\\(\\lambda &gt; 0\\) is the average number of events per interval\nThe parameter \\(\\lambda\\) captures the typical intensity of events.\nIt answers:\n\n‚ÄúOn average, how many events should I expect in this interval?‚Äù\n\nImportantly:\n\nEvents are independent\nThe average rate is stable\nEvents occur randomly, not in bursts or schedules\n\n\nReal-World Narrative\nImagine a retail store observing customer foot traffic.\nEvery 10 minutes, a random number of customers enter the store.\nOver many days, management notices:\n\nSome intervals have 1‚Äì2 customers\n\nSome have 5‚Äì6 customers\n\nOccasionally, none\n\nYet the average stays roughly the same.\nTypical Poisson use cases in retail and digital analytics:\n\nnumber of customers entering per hour\n\nnumber of purchases per minute in an online shop\n\nnumber of returns processed in each 30-minute window\n\nnumber of carts abandoned per hour\n\nIf these events:\n\nhappen independently\noccur at a stable average rate\n\n‚Ä¶then the count of events per interval follows a Poisson distribution.\n\n\nProbability Mass Function\nThe PMF of a Poisson random variable is:\n\\[\nP(X = x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0,1,2,\\dots\n\\]\n\n\\(x\\) is an exact count\n\nThis formula gives the probability of seeing exactly \\(x\\) events\n\n\n\nExpected Value and Variance\nFor a Poisson distribution both Variance and Expected Value\n\\[\nE[X] = \\lambda\n\\]\n\\[\nVar(X) = \\lambda\n\\]\n\nThe average number of events equals \\(\\lambda\\)\n\nThe uncertainty (variance) grows at the same rate\n\nThis makes Poisson models easy to interpret and diagnose.\n\n\nBusiness Interpretation of \\(\\lambda\\)\n\nLarge \\(\\lambda\\) ‚Üí frequent events\n\nSmall \\(\\lambda\\) ‚Üí rare events\n\nConcrete examples:\n\n\\(\\lambda = 2\\) ‚Üí about 2 calls per hour\n\n\\(\\lambda = 15\\) ‚Üí about 15 website visits per minute\n\n\\(\\lambda = 0.2\\) ‚Üí a rare failure, once every 5 intervals\n\n\n\nBusiness Applications of Poisson Distribution\nPoisson models are widely used in practice for:\n\ncustomer arrivals and foot traffic\n\ncall center volume estimation\n\nwebsite click and impression counts\n\nmanufacturing defects\n\nincident and outage reporting\n\nThese models help translate random counts into actionable forecasts.\n\n\nSpreadsheet Demonstration\nAssume \\(\\lambda\\) is stored in cell B1.\n\nPMF for \\(x\\): =POISSON.DIST(x, B1, FALSE)\nCDF (at most \\(x\\) events): =POISSON.DIST(x, B1, TRUE)\n\nTypical spreadsheet questions:\n\n‚ÄúWhat is the probability of more than 10 calls this hour?‚Äù\n\n‚ÄúWhat is the chance we see zero purchases in 5 minutes?‚Äù\n\nThese are answered directly using the Poisson CDF.\n\n\n\n\n\n\nTip\n\n\n\nReference for POISSON.DIST in Distributions Spreadsheets:\n\n\n\n\nRelationship to Continuous Models\n\nExponential distribution models time between events\nPoisson distribution models number of events\n\nThey describe the same process from different perspectives.\n\n\nVisualization\n\nLeft plot ‚Üí PMF for \\(\\lambda = 3\\) and \\(\\lambda = 10\\)\n\nRight plot ‚Üí likelihood as a function of \\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nHigher \\(\\lambda\\) means more customers entering per time interval.\n\nLower \\(\\lambda\\) means slower foot traffic.\n\nIn our observed retail data:\n\nTotal customers: \\(S = 23\\)\n\nNumber of intervals: \\(n = 6\\)\nMLE: \\(\\hat{\\lambda} = 23/6 = 3.83\\) customers per interval\n\nMeaning\n\non average, about 3.8 customers arrive every 10 minutes,\n\nwhich translates to roughly 23 customers per hour.\n\nThis insight directly supports:\n\nstaff scheduling\n\ncheckout lane allocation\n\npeak-hour planning\n\ndemand forecasting\n\ncapacity optimization\n\nPoisson models turn randomness into operational clarity.\n\n\nCase Study\nIn practice, ‚Äúfitting a Poisson model‚Äù means:\n\nconfirming the data matches the Poisson story\n\nestimating \\(\\lambda\\)\n\nchecking whether the fitted model matches the observed counts\n\n\nBusiness Context\nA retail store records the number of customers entering the store every 10 minutes during a weekday afternoon (14:00‚Äì16:00).\n\nTime window length is fixed\n\nEvents are counts\n\nGoal: model arrivals and estimate \\(\\lambda\\)\n\n\n\nObserved Data (Raw Counts)\n\n\n\nInterval\nTime Window\nCustomers\n\n\n\n\n1\n14:00‚Äì14:10\n4\n\n\n2\n14:10‚Äì14:20\n3\n\n\n3\n14:20‚Äì14:30\n5\n\n\n4\n14:30‚Äì14:40\n2\n\n\n5\n14:40‚Äì14:50\n6\n\n\n6\n14:50‚Äì15:00\n4\n\n\n7\n15:00‚Äì15:10\n3\n\n\n8\n15:10‚Äì15:20\n5\n\n\n9\n15:20‚Äì15:30\n4\n\n\n10\n15:30‚Äì15:40\n6\n\n\n11\n15:40‚Äì15:50\n2\n\n\n12\n15:50‚Äì16:00\n4\n\n\n\n\n\nSummary Statistics\nLet \\(X\\) = number of customers per 10-minute interval.\n\nNumber of intervals: \\(n = 12\\)\n\nTotal customers: \\(S = 48\\)\n\nSample mean:\n\\[\n\\bar{x} = \\frac{48}{12} = 4\n\\]\n\nThis strongly suggests a candidate model:\n\\[\nX \\sim \\text{Poisson}(\\lambda = 4)\n\\]\n\n\nDefine the Random Variable\nLet:\n\\[\nX = \\text{number of customers entering the store in a 10-minute interval}\n\\]\nKey properties:\n\n\\(X\\) is a count\n\\(X \\in \\{0,1,2,\\dots\\}\\)\nInterval length is fixed\n\nThis definition already aligns with the Poisson framework.\nBefore computing anything, we check the story, not formulas.\n\nFixed interval length ‚Üí yes (10 minutes)\nCounting events ‚Üí yes (customers)\nIndependence ‚Üí reasonable in short windows\nStable average rate ‚Üí plausible for this time range\n\nThis tells us:\n\nA Poisson model is a reasonable first approximation.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#bernoulli-distribution",
    "href": "materials/statistics/session4.html#bernoulli-distribution",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nThe Bernoulli Distribution models the most fundamental probabilistic question in data analytics and business decision-making:\n\nDid an event happen or not?\n\nThis is the simplest form of uncertainty we encounter in practice.\nThere are only two possible outcomes for a Bernoulli random variable:\n\nsuccess\n\nfailure\n\nThese outcomes can be represented in many equivalent ways depending on the business context:\n\nyes / no\n\n1 / 0\n\nclick / no click\n\npurchase / no purchase\n\ntail / head (’≤’∏÷Ç’∑ / ’£’´÷Ä)\n\nA random variable \\(X\\) follows a Bernoulli distribution if:\n\\[\nX \\sim \\text{Bernoulli}(p)\n\\]\nwhere:\n\n\\(p \\in [0,1]\\) is the probability of success\n\nUnlike the Poisson distribution, which models how many times an event occurs over an interval, the Bernoulli distribution focuses on a single decision, attempt, or event.\nIt answers the question:\n\n‚ÄúDid the event occur this time?‚Äù",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#what-is-a-bernoulli-trial",
    "href": "materials/statistics/session4.html#what-is-a-bernoulli-trial",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "What Is a Bernoulli Trial",
    "text": "What Is a Bernoulli Trial\nA Bernoulli trial is a single experiment characterized by:\n\nexactly one attempt\nexactly two possible outcomes\na fixed probability of success\n\nEach trial is assumed to be independent of the others.\nThis structure appears constantly in real-world data.\nExamples of Bernoulli trials include:\n\nDid a user click an advertisement?\n\nDid a customer complete a purchase?\n\nDid a transaction fail?\n\nDid a device respond to a health check?\n\nEach experiment produces a single binary outcome.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#real-world-narrative-1",
    "href": "materials/statistics/session4.html#real-world-narrative-1",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Real-World Narrative",
    "text": "Real-World Narrative\nConsider an online store tracking customer conversions.\nFor each user session, the outcome is recorded as:\n\nPurchase ‚Üí 1\n\nNo purchase ‚Üí 0\n\nAcross thousands of sessions:\n\nSome users convert\n\nMost users do not\n\nHowever, each session is evaluated independently, with the same underlying probability of conversion.\nThis is precisely the setting modeled by a Bernoulli distribution.\nTypical Bernoulli use cases in business analytics include:\n\nad click behavior (clicked / not clicked)\n\nconversion events (purchase / no purchase)\n\nemail engagement (opened / ignored)\n\nfraud detection flags (fraud / legitimate)\n\nchurn indicators (churned / stayed)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#probability-mass-function-pmf",
    "href": "materials/statistics/session4.html#probability-mass-function-pmf",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nBecause the Bernoulli distribution is discrete, it is described using a Probability Mass Function (PMF).\nThe PMF of a Bernoulli random variable is:\n\\[\nP(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0,1\\}\n\\]\nThis formula yields two probabilities:\n\n\\(P(X = 1) = p\\)\n\n\\(P(X = 0) = 1 - p\\)\n\nThese are the only possible outcomes, and their probabilities always sum to 1.\n\n\n\n\n\n\nWarningPMF vs PDF\n\n\n\n\nBernoulli is a discrete distribution, so we use a PMF\nContinuous distributions (Normal, Exponential) use a PDF\nPMFs assign probability to exact outcomes\nPDFs describe density, not probability at a single point\n\nNever mix these concepts.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#expected-value-and-variance-1",
    "href": "materials/statistics/session4.html#expected-value-and-variance-1",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nFor a Bernoulli random variable:\n\\[\nE[X] = p\n\\]\n\\[\nVar(X) = p(1-p)\n\\]\nInterpretation:\n\nThe expected value equals the probability of success\nVariance is largest when \\(p = 0.5\\), reflecting maximum uncertainty\nVariance decreases as outcomes become more predictable",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#business-interpretation-of-p",
    "href": "materials/statistics/session4.html#business-interpretation-of-p",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Business Interpretation of \\(p\\)",
    "text": "Business Interpretation of \\(p\\)\nThe parameter \\(p\\) is often one of the most important quantities in analytics.\nIt represents a rate, probability, or KPI.\n\nLarge \\(p\\) ‚Üí success is likely\n\nSmall \\(p\\) ‚Üí success is rare\n\nConcrete examples:\n\n\\(p = 0.02\\) ‚Üí 2% conversion rate\n\n\\(p = 0.35\\) ‚Üí 35% email open rate\n\n\\(p = 0.90\\) ‚Üí highly reliable system\n\nBernoulli models appear everywhere in practice:\n\nconversion modeling\n\nchurn indicators\n\nA/B testing outcomes\n\nfraud detection flags\n\nquality control pass/fail checks\n\nThey form the building block for more advanced statistical models.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#spreadsheet-demonstration-1",
    "href": "materials/statistics/session4.html#spreadsheet-demonstration-1",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Spreadsheet Demonstration",
    "text": "Spreadsheet Demonstration\nAssume the probability \\(p\\) is stored in cell B1.\n\nProbability of success: =B1\nProbability of failure: =1-B1\nSimulate a Bernoulli outcome: =IF(RAND()&lt;B1,1,0)\nEstimate \\(p\\) from observed data: =AVERAGE(range)\nVariance: =B1*(1-B1)\n\nTypical spreadsheet questions include:\n\n‚ÄúWhat fraction of users converted?‚Äù\n\n‚ÄúHow volatile is this conversion rate?‚Äù\n\n\nCase Study\n\nBusiness Context\nAn e-commerce company records whether each visitor completes a purchase during a session.\nEach session is encoded as:\n\n1 ‚Üí purchase\n\n0 ‚Üí no purchase\n\n\n\nObserved Data\n{.smaller}\n\n\n\nSession\nPurchase\n\n\n\n\n1\n0\n\n\n2\n1\n\n\n3\n0\n\n\n4\n0\n\n\n5\n1\n\n\n6\n0\n\n\n7\n1\n\n\n8\n0\n\n\n9\n0\n\n\n10\n1\n\n\n\n\n\nSummary Statistics\nLet \\(X\\) denote the purchase indicator per session.\n\nNumber of sessions: \\(n = 10\\)\n\nTotal purchases: \\(S = 4\\)\n\nThe sample mean is:\n\\[\n\\bar{x} = \\frac{4}{10} = 0.4\n\\]\nThis provides a natural estimate of the probability of purchase:\n\\[\n\\hat{p} = 0.4\n\\]\n\n\n\nDefining the Random Variable\nThe Bernoulli random variable can be written explicitly as:\n\\[\nX =\n\\begin{cases}\n1 & \\text{if a purchase occurs} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nKey properties:\n\nbinary outcome\n\nindependent trials\n\nfixed probability per session\n\nBefore applying any formulas, we validate the modeling story:\n\nsingle decision ‚Üí yes\n\ntwo outcomes ‚Üí yes\n\nstable probability ‚Üí reasonable\n\nConclusion:\n\nA Bernoulli model is an appropriate first representation of this process.\n\nBernoulli distributions transform binary behavior into measurable probabilities, forming the foundation of modern data analytics and statistical modeling.\n\n\n\n\n\n\nTip\n\n\n\nReference to the Distributions Spreadsheets:",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#final-intuition-summary",
    "href": "materials/statistics/session4.html#final-intuition-summary",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Final Intuition Summary",
    "text": "Final Intuition Summary\n\nUse Uniform for ‚Äúrandom within limits.‚Äù\nUse Exponential for ‚Äútime until next event.‚Äù\nUse Poisson for ‚Äúnumber of events per interval.‚Äù\nUse Bernoulli for ‚Äúone yes/no outcome‚Äù.\n\nTogether, these four distributions cover the majority of basic retail/business modeling scenarios:\n\nfoot traffic analysis,\ncheckout optimization,\nconversion measurement,\nforecasting purchase behavior\nand customer journey analytics.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session4.html#central-limit-theorem-clt",
    "href": "materials/statistics/session4.html#central-limit-theorem-clt",
    "title": "Statistics Session 04: Probabilistic Distributions",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\nThe normal distribution is kind of magical in that we see it a lot in nature. But there‚Äôs a reason for that, and that reason makes it super useful for statistics as well. The Central Limit Theorem is the basis for a lot of statistics and the good news is that it is a pretty simple concept. That magic is called The Central Limit Theorem (CLT).\n\n\n‚ÄúEven if you‚Äôre not normal, the average is normal‚Äù\n\n\nBig Idea\nThe Central Limit Theorem states that when we take many independent samples from a population and compute the average of each sample, the distribution of those averages:\n\napproaches a Normal distribution\nas the sample size increases\nregardless of the shape of the original distribution\n(provided the variance is finite)\n\nIn symbols, if:\n\\[\nX_1, X_2, ..., X_n\n\\] are independent with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the sample mean: \\[\n\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\n\\] has an approximate Normal distribution for large \\(n\\): \\[\n\\bar{X} \\approx \\mathcal{N}\\!\\left(\\mu,\\,\\frac{\\sigma^2}{n}\\right)\n\\]\n\n\n\nWhy This Matters\nIn analytics, we rarely care about one single observation;\nInstead we often care about:\n\nsample means\n\ntotals\n\nproportions\n\ndifferences of means\n\nEven if the raw data come from a non-Normal distribution, the CLT tells us the distribution of these statistics will be approximately Normal when the sample size is sufficiently large.\nThis underpins many common tools, including:\n\nconfidence intervals\n\nhypothesis tests\n\nregressions\n\nA/B testing\n\ncontrol charts\n\n\n\nIntuition with an Example\nImagine you repeatedly sample from a skewed distribution ‚Äî say, an exponential distribution (like waiting times).\nEven though individual values are skewed, the distribution of average values will look more symmetric and bell-shaped when you increase the sample size.\nThis ‚Äúaveraging effect‚Äù is why Normal distributions are so prevalent in real data.\n\n\n\nSimulation of CLT\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the Simulation Shows\n\nFor small sample sizes, the distribution of sample means is skewed.\nAs sample size increases:\n\nthe distribution becomes more symmetric\nit takes on the bell-shaped form\nit becomes closer to Normal\n\n\nThis happens even though the original population was not Normal.\n\n\n\nConditions for the CLT to Work\nThe CLT applies when:\n\nsamples are independent\nthe sample size is large enough\nthe population has finite variance\n\nIn practice, a sample size of 30 or more is often enough for many distributions.\n\n\n\nPractical Importance\nThe CLT justifies:\n\nusing Normal distribution tools for inference\napproximating distributions of sums/means\nconstructing confidence intervals\nperforming hypothesis tests\n\nEven for non-Normal raw data, aggregating information leads to Normal behavior.\n\n\n\nSummary\n\nThe Central Limit Theorem explains the widespread appearance of Normal distributions.\nIt applies to sample means (and sums) of independent observations.\nIt works even when individual data are not Normal.\nIt is a cornerstone of statistical inference.\n\n\n‚ÄúEven if you‚Äôre not normal, the average is normal.‚Äù",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 04: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Slides",
    "section": "",
    "text": "Intro\n\n\n\n\n\nSession 1\nSession 2\nSession 3\nSession 4\n\n\n\n\n\nSession 1\nSession 2\nSession 3\nSession 4\nSession 5\nSession 6\n\n\n\n\n\nSession 1\nSession 2\nSession 3\nSession 4\nSession 5\nSession 6\nSession 7\n\n\n\n\n\nSession 1\nSession 2\nSession 3\nSession 4\n\n\n\n\n\nSession 1\nSession 2\nSession 3"
  },
  {
    "objectID": "index.html#intro",
    "href": "index.html#intro",
    "title": "Slides",
    "section": "",
    "text": "Intro"
  },
  {
    "objectID": "index.html#statistical-thinking",
    "href": "index.html#statistical-thinking",
    "title": "Slides",
    "section": "",
    "text": "Session 1\nSession 2\nSession 3\nSession 4"
  },
  {
    "objectID": "index.html#sql",
    "href": "index.html#sql",
    "title": "Slides",
    "section": "",
    "text": "Session 1\nSession 2\nSession 3\nSession 4\nSession 5\nSession 6"
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Slides",
    "section": "",
    "text": "Session 1\nSession 2\nSession 3\nSession 4\nSession 5\nSession 6\nSession 7"
  },
  {
    "objectID": "index.html#tableau",
    "href": "index.html#tableau",
    "title": "Slides",
    "section": "",
    "text": "Session 1\nSession 2\nSession 3\nSession 4"
  },
  {
    "objectID": "index.html#capstone",
    "href": "index.html#capstone",
    "title": "Slides",
    "section": "",
    "text": "Session 1\nSession 2\nSession 3"
  },
  {
    "objectID": "materials/statistics/session6.html",
    "href": "materials/statistics/session6.html",
    "title": "Statistics Session 06: Inferential Statistics",
    "section": "",
    "text": "In the statistical world, a hypothesis is an assumption about a specific population parameter, such as a mean, a proportion, or a standard deviation.\nWe Data Analyists/Statisticians like to make an assumptions about the value of a population parameter, and then:\n\ncollect a sample from that population,\nmeasure the sample,\ndeclare in ascholarly manner, whether the sample supports the original assumption.\n\nThis, in a nutshell, is what hypothesis testing is all about.\n\n\n\nA recent Wall Street Journal article titled ‚ÄúDoes the Internet Make You Smarter or Dumber?‚Äù posed the possibility that online activities turn us into shallow thinkers. The article cited a statistic claiming that the average time an American spends looking at a Web page is 56 seconds. A researcher at a local university would like to test this claim using a hypothesis test.\n\n\nThe null hypothesis denoted by \\(H_0\\), represents the status quo and involves stating the belief that the population parameter is \\(\\le,=, \\ge\\) a specific value. The null hypothesis is believed to be true unless there is overwhelming evidence to the contrary.\n\n\n\nThe alternative hypothesis, denoted by \\(H_1\\), represents the opposite of the null hypothesis and is believed to be true if the null hypothesis is found to be false. The alternative hypothesis always states that the population parameter is \\(\\gt,\\ne, \\lt\\) a specific value.\nYou need to be careful how you state the null and alternative hypotheses. Your decision will depend on the nature of the test and the motivation of the person conducting it. Suppose the purpose of the test is to determine if the population mean is equal to a specific value, which is what we would want to test based on our previous Wall Street Journal article\n\n\n\n\n\n\nImportantAnalogy with the Legal System\n\n\n\nWe can never accept the Null Hypothesis!\nIn order to completely understand the process consider the analogy with the legal system.\nWe can either reject it or not reject it (fail to reject it)\nThe court system assumes a persion is innocent until proben guily, the hypothesis test is formulated as follows\n\n\\(H_0:\\) the defendant is innocent (status quo)\n\\(H_1:\\) the defendant is guilty\n\nThe court system might have two conclusions:\n\nReject the Null Hypothesis \\(\\rightarrow\\) the defendant is guilty\nFail to reject the Null Hypothesis \\(\\rightarrow\\) the defendant is guilty\n\n\n\n\n\n\n\n\nIdentify the null and alternative hypotheses\nSet a value for the significance level \\(\\alpha\\)\nDetermine the appropriate critical value\nCalculate the appropriate test statistic\nCompare the test statistics with the critical score\nState your conclusion\n\n\n\nYou need to be careful how you state the null and alternative hypotheses. Your decision will depend on the nature of the test and the motivation of the person conducting it. Suppose the purpose of the test is to determine if the population mean is equal to a specific value, which is what we would want to test based on our previous Wall Street Journal article. We would then assign this statement as the null hypothesis, which results in the following equation:\nIn this example the Internet users spend an average timeof 56 seconds on a Web page. Thus the status quo would be:\n\\[\nH_0: \\mu = 56 \\text{ seconds (status quo)}\n\\]\n\\[\nH_1: \\mu \\ne  56 \\text{ seconds}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Tailed Test\nLeft-Tailed Test\nRight-Tailed Test\n\n\n\n\nNull\n\\[H_0: \\mu = 56\\]\n\\[H_0: \\mu \\ge 56\\]\n\\[H_0: \\mu \\le 56\\]\n\n\nAlternative\n\\[H_1: \\mu \\ne 56\\]\n\\[H_1: \\mu &lt; 56\\]\n\\[H_1: \\mu &gt; 56\\]\n\n\n\n\n\n\n\nThe level of significance represents the probability of making a Type I error. A Type I error occurs when we reject the null hypothesis but it is actually true. In the scope of the program we will set \\(\\alpha = 0.05\\) which is a common value used in hypothesis testing.\n\nIn the scope the above example Type I Error we conclude that the true average time spent on a webpage is not 56 seconds, even though in reality it is 56 seconds.\n\n\n\n\nAlpha (Œ±)\nTail\nCritical z-Score\nCritical t-Score (df = 20)\n\n\n\n\n0.01\nOne\n2.33\n2.528\n\n\n0.01\nTwo\n2.575\n2.845\n\n\n0.02\nOne\n2.05\n2.312\n\n\n0.02\nTwo\n2.33\n2.528\n\n\n0.05\nOne\n1.645\n1.725\n\n\n0.05\nTwo\n1.96\n2.086\n\n\n0.10\nOne\n1.28\n1.325\n\n\n0.10\nTwo\n1.645\n1.725\n\n\n\nIn practical terms:\n\nThe researcher conducts a sample study.\nThe sample mean differs enough from 56 seconds to fall outside the acceptance region.\nThe test rejects the null hypothesis at the 5%level.\nBut in truth, Americans do spend exactly 56 seconds on average.\n\n\n\n\n\n\n\n\n\n\n\nNoteIn case of z-test\n\n\n\nWhen Variance is known\n\ntest statistic would be \\(z_{\\bar{x}}\\)\n\ncritical value would be \\(z_{\\alpha}\\)\n\n\\[\nz_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{\\sigma / \\sqrt{n}}\n\\]\nGiven\n\nClaimed mean: \\(\\mu_{H_0} = 56\\) seconds\n\nSample mean: \\(\\bar{x} = 62\\) seconds\n\nPopulation standard deviation (historical estimate): \\(\\sigma = 18\\) seconds\n\nSample size: \\(n = 45\\)\n\n\\[\nz_{\\bar{x}}\n= \\frac{\\bar{x} - \\mu_{H_0}}{\\sigma / \\sqrt{n}}\n= \\frac{62 - 56}{18 / \\sqrt{45}}\n= \\frac{6}{2.683}\n= 2.24\n\\]\nA test statistic of 2.24 lies well into the rejection region (far from 0).\n\nFor a two-tailed test at \\(\\alpha = 0.05\\), the critical values are \\(\\pm 1.96\\).\n\nSince 2.24 &gt; 1.96, we reject \\(H_0\\).\n\nInterpretation:\nThere is statistically significant evidence that the true average time spent on a webpage is different from 56 seconds.\n\n\n\n\n\n\n\n\nNoteIn case of t-test\n\n\n\nWhen Variance is Unknown\n\ntest statistic would be \\(t_{\\bar{x}}\\)\n\ncritical value would be \\(t_{\\alpha}\\) (from the t-table)\n\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s / \\sqrt{n}}\n\\]\nGiven\n\nClaimed mean: \\(\\mu_{H_0} = 56\\) seconds\n\nSample mean: \\(\\bar{x} = 62\\) seconds\n\nSample standard deviation: \\(s = 20\\) seconds\n\nSample size: \\(n = 30\\)\n\nDegrees of freedom: \\(df = n - 1 = 29\\)\n\nTest Statistic\n\\[\nt_{\\bar{x}}\n= \\frac{62 - 56}{20 / \\sqrt{30}}\n= \\frac{6}{3.651}\n= 1.64\n\\]\nCritical Value\nFor a two-tailed test at \\(\\alpha = 0.05\\) with \\(df = 29\\):\n\\[f\nt_{\\alpha/2, \\, df} = t_{0.025,29} = 2.045\n\\]\nDecision\nSince 1.64 &lt; 2.045, we fail to reject \\(H_0\\). There is not enough statistical evidence to conclude that the true average time spent on a webpage is different from 56 seconds. The sample mean is slightly higher, but not far enough from 56 to be statistically significant given the sample size and sample variability.\n\n\nIn the scope of this program we are going to conduct two hypothesis testing\n\nHypothesis Testing for a Single Population\nHypothesis Testing for two Samples (A/b testing) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Type\nHypotheses\nCondition\nConclusion\n\n\n\n\nTwo-tail\n\\(H_0: \\mu = \\mu_0\\)\n\\(\\lvert z_x \\rvert &gt; z_{\\alpha/2}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)\n\\(\\lvert z_x \\rvert \\le z_{\\alpha/2}\\)\nDo not reject \\(H_0\\)\n\n\nOne-tail\n\\(H_0: \\mu \\le \\mu_0\\)\n\\(z_x &gt; z_\\alpha\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\)\n\\(z_x \\le z_\\alpha\\)\nDo not reject \\(H_0\\)\n\n\nOne-tail\n\\(H_0: \\mu \\ge \\mu_0\\)\n\\(z_x &lt; -z_\\alpha\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &lt; \\mu_0\\)\n\\(z_x \\ge -z_\\alpha\\)\nDo not reject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Type\nHypotheses\nCondition\nConclusion\n\n\n\n\nTwo-tail\n\\(H_0: \\mu = \\mu_0\\)\n\\(\\lvert t_{\\bar{x}} \\rvert &gt; t_{\\alpha/2,\\;df}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)\n\\(\\lvert t_{\\bar{x}} \\rvert \\le t_{\\alpha/2,\\;df}\\)\nDo not reject \\(H_0\\)\n\n\nRight-tail\n\\(H_0: \\mu \\le \\mu_0\\)\n\\(t_{\\bar{x}} &gt; t_{\\alpha,\\;df}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\)\n\\(t_{\\bar{x}} \\le t_{\\alpha,\\;df}\\)\nDo not reject \\(H_0\\)\n\n\nLeft-tail\n\\(H_0: \\mu \\ge \\mu_0\\)\n\\(t_{\\bar{x}} &lt; -t_{\\alpha,\\;df}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &lt; \\mu_0\\)\n\\(t_{\\bar{x}} \\ge -t_{\\alpha,\\;df}\\)\nDo not reject \\(H_0\\)\n\n\n\n\n\n\nIt is mor convenient the use p-value apporach, as it helps us to rememeber the decission rule easily.\nIf the p-value is less than \\(\\alpha\\), there is little chance of observing the sample mean from the population on which it is based if the null hypothesis were actually true. We therefore reject the null hypothesis under this condition.\n\n\n\nCondition\nConclusion\n\n\n\n\n\\(p\\text{-Value} \\ge \\alpha\\)\nDo not reject \\(H_0\\)\n\n\n\\(p\\text{-Value} &lt; \\alpha\\)\nReject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also be guilty of Type II error, which occurs when the null hypothesis is really false and we fail to reject it. The probability of a Type II error is known as \\(\\beta\\).\n\nWhen we decide to reject the null hypothesis, there is always the chance, with probability equal to \\(\\alpha\\), that we are wrong! In the Nissan Leaf example, a is the probability of concluding that the average driving distance does exceed 100 miles when, in fact, it does not exceed 100 miles.\n\n\n\n\n\n\n\n\n\nTipUrban Traffic Analytics: Average Speed in Yerevan\n\n\n\nUrban mobility reports often claim that the average weekday driving speed in Yerevan is 15 km/h. A transportation analyst wants to test this claim using recent GPS data from ride-sharing vehicles.\nThis example is ideal for explaining all three hypothesis-test formulations:\n\nTwo-Tailed: Is the true average speed different from 15 km/h?\nRight-Tailed: Are drivers going faster than 15 km/h?\nLeft-Tailed: Are drivers going slower than 15 km/h?\n\n\n\n\n\nWe want to test whether the true average weekday driving speed in Yerevan differs from the commonly stated value of 15 km/h.\n\n\n\\[H_0: \\mu = 15\\]\n\\[H_1: \\mu \\ne 15\\]\nSince the population standard deviation is unknown, a one-sample t-test is appropriate.\nGiven\n\nSample size: \\(n = 1000\\)\nSample mean: \\(\\bar{x} = 16.2 \\text{ km/h}\\)\nSample standard deviation: \\(s = 5.1 \\text{ km/h}\\)\nNull hypothesis mean: \\(\\mu_{H_0} = 15\\)\nSignificance level: \\(\\alpha = 0.05\\)\nDegrees of freedom: \\(df = 999\\)\n\nThe test statistic is:\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s/\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\nInterpretation of the Two-Tailed Test\n\nThe computed test statistic is: \\(t_{\\bar{x}} = 7.44\\)\nFor a two-tailed test with \\(\\alpha = 0.05\\) and \\(df = 999\\) \\(\\rightarrow\\) \\(t_{\\alpha/2, df} = \\pm 1.96\\) :\np-value: \\(p = 2 \\cdot P(T &gt; 7.44)\\approx 1 \\times 10^{-10}\\)\n\nDecision\n\\(|7.44| &gt; 1.96 \\rightarrow\\) \\(p &lt; 0.05\\)\nWe reject the null hypothesis. There is very strong evidence that the true average weekday driving speed in Yerevan is not equal to 15 km/h.\n\n\n\n\n\nWe want to test whether the true average weekday driving speed in Yerevan is greater than\nthe stated value of 15 km/h.\n\n\n\\[H_0: \\mu \\le 15\\]\n\\[H_1: \\mu &gt; 15\\]\nGiven\n\nSample size: \\(n = 1000\\)\nSample mean: \\(\\bar{x} = 16.2\\) km/h\nSample standard deviation: \\(s = 5.1\\) km/h\nNull hypothesis mean: \\(\\mu_{H_0}=15\\)\nSignificance level: \\(\\alpha=0.05\\)\nDegrees of freedom: \\(df = 999\\)\n\nThe test statistic is:\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s/\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest statistic: \\(t_{\\bar{x}} = 7.44\\)\nCritical value: \\(t_{\\alpha, 999} = 1.645\\)\np-value: \\(p \\approx 5\\times 10^{-11}\\)\n\nDecision\nSince \\(7.44 &gt; 1.645\\) and \\(p &lt; 0.05\\), we reject \\(H_0\\).\nConclusion:\nThere is extremely strong evidence that drivers in Yerevan drive faster than 15 km/h on average.\n\n\n\nWe want to test whether the true average weekday driving speed in Yerevan is less than\nthe stated value of 15 km/h.\n\n\n\\[H_0: \\mu \\ge 15\\]\n\\[H_1: \\mu &lt; 15\\]\nGiven\n\nSample size: \\(n = 1000\\)\nSample mean: \\(\\bar{x} = 16.2\\) km/h\nSample standard deviation: \\(s = 5.1\\) km/h\nNull hypothesis mean: \\(\\mu_{H_0} = 15\\)\nSignificance level: \\(\\alpha = 0.05\\)\nDegrees of freedom: \\(df = 999\\)\n\nThe test statistic is:\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s/\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest statistic: \\(t_{\\bar{x}} = 7.44\\)\n\nCritical value: \\(t_{\\alpha, 999} = -1.645\\)\n\np-value: \\(p = P(T &lt; 7.44) \\approx 1.00\\)\n\nDecision:\nSince \\(7.44\\) is not less than \\(-1.645\\) and \\(p &gt; 0.05\\), we do NOT reject \\(H_0\\).\nConclusion:\nThere is no evidence that drivers in Yerevan are slower than 15 km/h. In fact, the sample strongly indicates the opposite.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 06: Inferential Statistics"
    ]
  },
  {
    "objectID": "materials/statistics/session6.html#hypothesis-testing",
    "href": "materials/statistics/session6.html#hypothesis-testing",
    "title": "Statistics Session 06: Inferential Statistics",
    "section": "",
    "text": "In the statistical world, a hypothesis is an assumption about a specific population parameter, such as a mean, a proportion, or a standard deviation.\nWe Data Analyists/Statisticians like to make an assumptions about the value of a population parameter, and then:\n\ncollect a sample from that population,\nmeasure the sample,\ndeclare in ascholarly manner, whether the sample supports the original assumption.\n\nThis, in a nutshell, is what hypothesis testing is all about.\n\n\n\nA recent Wall Street Journal article titled ‚ÄúDoes the Internet Make You Smarter or Dumber?‚Äù posed the possibility that online activities turn us into shallow thinkers. The article cited a statistic claiming that the average time an American spends looking at a Web page is 56 seconds. A researcher at a local university would like to test this claim using a hypothesis test.\n\n\nThe null hypothesis denoted by \\(H_0\\), represents the status quo and involves stating the belief that the population parameter is \\(\\le,=, \\ge\\) a specific value. The null hypothesis is believed to be true unless there is overwhelming evidence to the contrary.\n\n\n\nThe alternative hypothesis, denoted by \\(H_1\\), represents the opposite of the null hypothesis and is believed to be true if the null hypothesis is found to be false. The alternative hypothesis always states that the population parameter is \\(\\gt,\\ne, \\lt\\) a specific value.\nYou need to be careful how you state the null and alternative hypotheses. Your decision will depend on the nature of the test and the motivation of the person conducting it. Suppose the purpose of the test is to determine if the population mean is equal to a specific value, which is what we would want to test based on our previous Wall Street Journal article\n\n\n\n\n\n\nImportantAnalogy with the Legal System\n\n\n\nWe can never accept the Null Hypothesis!\nIn order to completely understand the process consider the analogy with the legal system.\nWe can either reject it or not reject it (fail to reject it)\nThe court system assumes a persion is innocent until proben guily, the hypothesis test is formulated as follows\n\n\\(H_0:\\) the defendant is innocent (status quo)\n\\(H_1:\\) the defendant is guilty\n\nThe court system might have two conclusions:\n\nReject the Null Hypothesis \\(\\rightarrow\\) the defendant is guilty\nFail to reject the Null Hypothesis \\(\\rightarrow\\) the defendant is guilty\n\n\n\n\n\n\n\n\nIdentify the null and alternative hypotheses\nSet a value for the significance level \\(\\alpha\\)\nDetermine the appropriate critical value\nCalculate the appropriate test statistic\nCompare the test statistics with the critical score\nState your conclusion\n\n\n\nYou need to be careful how you state the null and alternative hypotheses. Your decision will depend on the nature of the test and the motivation of the person conducting it. Suppose the purpose of the test is to determine if the population mean is equal to a specific value, which is what we would want to test based on our previous Wall Street Journal article. We would then assign this statement as the null hypothesis, which results in the following equation:\nIn this example the Internet users spend an average timeof 56 seconds on a Web page. Thus the status quo would be:\n\\[\nH_0: \\mu = 56 \\text{ seconds (status quo)}\n\\]\n\\[\nH_1: \\mu \\ne  56 \\text{ seconds}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Tailed Test\nLeft-Tailed Test\nRight-Tailed Test\n\n\n\n\nNull\n\\[H_0: \\mu = 56\\]\n\\[H_0: \\mu \\ge 56\\]\n\\[H_0: \\mu \\le 56\\]\n\n\nAlternative\n\\[H_1: \\mu \\ne 56\\]\n\\[H_1: \\mu &lt; 56\\]\n\\[H_1: \\mu &gt; 56\\]\n\n\n\n\n\n\n\nThe level of significance represents the probability of making a Type I error. A Type I error occurs when we reject the null hypothesis but it is actually true. In the scope of the program we will set \\(\\alpha = 0.05\\) which is a common value used in hypothesis testing.\n\nIn the scope the above example Type I Error we conclude that the true average time spent on a webpage is not 56 seconds, even though in reality it is 56 seconds.\n\n\n\n\nAlpha (Œ±)\nTail\nCritical z-Score\nCritical t-Score (df = 20)\n\n\n\n\n0.01\nOne\n2.33\n2.528\n\n\n0.01\nTwo\n2.575\n2.845\n\n\n0.02\nOne\n2.05\n2.312\n\n\n0.02\nTwo\n2.33\n2.528\n\n\n0.05\nOne\n1.645\n1.725\n\n\n0.05\nTwo\n1.96\n2.086\n\n\n0.10\nOne\n1.28\n1.325\n\n\n0.10\nTwo\n1.645\n1.725\n\n\n\nIn practical terms:\n\nThe researcher conducts a sample study.\nThe sample mean differs enough from 56 seconds to fall outside the acceptance region.\nThe test rejects the null hypothesis at the 5%level.\nBut in truth, Americans do spend exactly 56 seconds on average.\n\n\n\n\n\n\n\n\n\n\n\nNoteIn case of z-test\n\n\n\nWhen Variance is known\n\ntest statistic would be \\(z_{\\bar{x}}\\)\n\ncritical value would be \\(z_{\\alpha}\\)\n\n\\[\nz_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{\\sigma / \\sqrt{n}}\n\\]\nGiven\n\nClaimed mean: \\(\\mu_{H_0} = 56\\) seconds\n\nSample mean: \\(\\bar{x} = 62\\) seconds\n\nPopulation standard deviation (historical estimate): \\(\\sigma = 18\\) seconds\n\nSample size: \\(n = 45\\)\n\n\\[\nz_{\\bar{x}}\n= \\frac{\\bar{x} - \\mu_{H_0}}{\\sigma / \\sqrt{n}}\n= \\frac{62 - 56}{18 / \\sqrt{45}}\n= \\frac{6}{2.683}\n= 2.24\n\\]\nA test statistic of 2.24 lies well into the rejection region (far from 0).\n\nFor a two-tailed test at \\(\\alpha = 0.05\\), the critical values are \\(\\pm 1.96\\).\n\nSince 2.24 &gt; 1.96, we reject \\(H_0\\).\n\nInterpretation:\nThere is statistically significant evidence that the true average time spent on a webpage is different from 56 seconds.\n\n\n\n\n\n\n\n\nNoteIn case of t-test\n\n\n\nWhen Variance is Unknown\n\ntest statistic would be \\(t_{\\bar{x}}\\)\n\ncritical value would be \\(t_{\\alpha}\\) (from the t-table)\n\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s / \\sqrt{n}}\n\\]\nGiven\n\nClaimed mean: \\(\\mu_{H_0} = 56\\) seconds\n\nSample mean: \\(\\bar{x} = 62\\) seconds\n\nSample standard deviation: \\(s = 20\\) seconds\n\nSample size: \\(n = 30\\)\n\nDegrees of freedom: \\(df = n - 1 = 29\\)\n\nTest Statistic\n\\[\nt_{\\bar{x}}\n= \\frac{62 - 56}{20 / \\sqrt{30}}\n= \\frac{6}{3.651}\n= 1.64\n\\]\nCritical Value\nFor a two-tailed test at \\(\\alpha = 0.05\\) with \\(df = 29\\):\n\\[f\nt_{\\alpha/2, \\, df} = t_{0.025,29} = 2.045\n\\]\nDecision\nSince 1.64 &lt; 2.045, we fail to reject \\(H_0\\). There is not enough statistical evidence to conclude that the true average time spent on a webpage is different from 56 seconds. The sample mean is slightly higher, but not far enough from 56 to be statistically significant given the sample size and sample variability.\n\n\nIn the scope of this program we are going to conduct two hypothesis testing\n\nHypothesis Testing for a Single Population\nHypothesis Testing for two Samples (A/b testing) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Type\nHypotheses\nCondition\nConclusion\n\n\n\n\nTwo-tail\n\\(H_0: \\mu = \\mu_0\\)\n\\(\\lvert z_x \\rvert &gt; z_{\\alpha/2}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)\n\\(\\lvert z_x \\rvert \\le z_{\\alpha/2}\\)\nDo not reject \\(H_0\\)\n\n\nOne-tail\n\\(H_0: \\mu \\le \\mu_0\\)\n\\(z_x &gt; z_\\alpha\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\)\n\\(z_x \\le z_\\alpha\\)\nDo not reject \\(H_0\\)\n\n\nOne-tail\n\\(H_0: \\mu \\ge \\mu_0\\)\n\\(z_x &lt; -z_\\alpha\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &lt; \\mu_0\\)\n\\(z_x \\ge -z_\\alpha\\)\nDo not reject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Type\nHypotheses\nCondition\nConclusion\n\n\n\n\nTwo-tail\n\\(H_0: \\mu = \\mu_0\\)\n\\(\\lvert t_{\\bar{x}} \\rvert &gt; t_{\\alpha/2,\\;df}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)\n\\(\\lvert t_{\\bar{x}} \\rvert \\le t_{\\alpha/2,\\;df}\\)\nDo not reject \\(H_0\\)\n\n\nRight-tail\n\\(H_0: \\mu \\le \\mu_0\\)\n\\(t_{\\bar{x}} &gt; t_{\\alpha,\\;df}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\)\n\\(t_{\\bar{x}} \\le t_{\\alpha,\\;df}\\)\nDo not reject \\(H_0\\)\n\n\nLeft-tail\n\\(H_0: \\mu \\ge \\mu_0\\)\n\\(t_{\\bar{x}} &lt; -t_{\\alpha,\\;df}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &lt; \\mu_0\\)\n\\(t_{\\bar{x}} \\ge -t_{\\alpha,\\;df}\\)\nDo not reject \\(H_0\\)\n\n\n\n\n\n\nIt is mor convenient the use p-value apporach, as it helps us to rememeber the decission rule easily.\nIf the p-value is less than \\(\\alpha\\), there is little chance of observing the sample mean from the population on which it is based if the null hypothesis were actually true. We therefore reject the null hypothesis under this condition.\n\n\n\nCondition\nConclusion\n\n\n\n\n\\(p\\text{-Value} \\ge \\alpha\\)\nDo not reject \\(H_0\\)\n\n\n\\(p\\text{-Value} &lt; \\alpha\\)\nReject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also be guilty of Type II error, which occurs when the null hypothesis is really false and we fail to reject it. The probability of a Type II error is known as \\(\\beta\\).\n\nWhen we decide to reject the null hypothesis, there is always the chance, with probability equal to \\(\\alpha\\), that we are wrong! In the Nissan Leaf example, a is the probability of concluding that the average driving distance does exceed 100 miles when, in fact, it does not exceed 100 miles.\n\n\n\n\n\n\n\n\n\nTipUrban Traffic Analytics: Average Speed in Yerevan\n\n\n\nUrban mobility reports often claim that the average weekday driving speed in Yerevan is 15 km/h. A transportation analyst wants to test this claim using recent GPS data from ride-sharing vehicles.\nThis example is ideal for explaining all three hypothesis-test formulations:\n\nTwo-Tailed: Is the true average speed different from 15 km/h?\nRight-Tailed: Are drivers going faster than 15 km/h?\nLeft-Tailed: Are drivers going slower than 15 km/h?\n\n\n\n\n\nWe want to test whether the true average weekday driving speed in Yerevan differs from the commonly stated value of 15 km/h.\n\n\n\\[H_0: \\mu = 15\\]\n\\[H_1: \\mu \\ne 15\\]\nSince the population standard deviation is unknown, a one-sample t-test is appropriate.\nGiven\n\nSample size: \\(n = 1000\\)\nSample mean: \\(\\bar{x} = 16.2 \\text{ km/h}\\)\nSample standard deviation: \\(s = 5.1 \\text{ km/h}\\)\nNull hypothesis mean: \\(\\mu_{H_0} = 15\\)\nSignificance level: \\(\\alpha = 0.05\\)\nDegrees of freedom: \\(df = 999\\)\n\nThe test statistic is:\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s/\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\nInterpretation of the Two-Tailed Test\n\nThe computed test statistic is: \\(t_{\\bar{x}} = 7.44\\)\nFor a two-tailed test with \\(\\alpha = 0.05\\) and \\(df = 999\\) \\(\\rightarrow\\) \\(t_{\\alpha/2, df} = \\pm 1.96\\) :\np-value: \\(p = 2 \\cdot P(T &gt; 7.44)\\approx 1 \\times 10^{-10}\\)\n\nDecision\n\\(|7.44| &gt; 1.96 \\rightarrow\\) \\(p &lt; 0.05\\)\nWe reject the null hypothesis. There is very strong evidence that the true average weekday driving speed in Yerevan is not equal to 15 km/h.\n\n\n\n\n\nWe want to test whether the true average weekday driving speed in Yerevan is greater than\nthe stated value of 15 km/h.\n\n\n\\[H_0: \\mu \\le 15\\]\n\\[H_1: \\mu &gt; 15\\]\nGiven\n\nSample size: \\(n = 1000\\)\nSample mean: \\(\\bar{x} = 16.2\\) km/h\nSample standard deviation: \\(s = 5.1\\) km/h\nNull hypothesis mean: \\(\\mu_{H_0}=15\\)\nSignificance level: \\(\\alpha=0.05\\)\nDegrees of freedom: \\(df = 999\\)\n\nThe test statistic is:\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s/\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest statistic: \\(t_{\\bar{x}} = 7.44\\)\nCritical value: \\(t_{\\alpha, 999} = 1.645\\)\np-value: \\(p \\approx 5\\times 10^{-11}\\)\n\nDecision\nSince \\(7.44 &gt; 1.645\\) and \\(p &lt; 0.05\\), we reject \\(H_0\\).\nConclusion:\nThere is extremely strong evidence that drivers in Yerevan drive faster than 15 km/h on average.\n\n\n\nWe want to test whether the true average weekday driving speed in Yerevan is less than\nthe stated value of 15 km/h.\n\n\n\\[H_0: \\mu \\ge 15\\]\n\\[H_1: \\mu &lt; 15\\]\nGiven\n\nSample size: \\(n = 1000\\)\nSample mean: \\(\\bar{x} = 16.2\\) km/h\nSample standard deviation: \\(s = 5.1\\) km/h\nNull hypothesis mean: \\(\\mu_{H_0} = 15\\)\nSignificance level: \\(\\alpha = 0.05\\)\nDegrees of freedom: \\(df = 999\\)\n\nThe test statistic is:\n\\[\nt_{\\bar{x}} = \\frac{\\bar{x} - \\mu_{H_0}}{s/\\sqrt{n}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest statistic: \\(t_{\\bar{x}} = 7.44\\)\n\nCritical value: \\(t_{\\alpha, 999} = -1.645\\)\n\np-value: \\(p = P(T &lt; 7.44) \\approx 1.00\\)\n\nDecision:\nSince \\(7.44\\) is not less than \\(-1.645\\) and \\(p &gt; 0.05\\), we do NOT reject \\(H_0\\).\nConclusion:\nThere is no evidence that drivers in Yerevan are slower than 15 km/h. In fact, the sample strongly indicates the opposite.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 06: Inferential Statistics"
    ]
  },
  {
    "objectID": "materials/statistics/session6.html#comparing-two-population-means",
    "href": "materials/statistics/session6.html#comparing-two-population-means",
    "title": "Statistics Session 06: Inferential Statistics",
    "section": "Comparing Two Population Means",
    "text": "Comparing Two Population Means\nThe sampling distribution for the difference in means is the result of subtracting the sampling distribution for the mean of one population from the sampling distribution for the mean of a second population.\n\n\nTwo-Sample t-Test (A/B Test)\nDoes Variant B change average daily engagement?\nA telecom company is testing two versions of its mobile self-care app:\n\nVersion A (control) ‚Äî current user interface\n\nVersion B (treatment) ‚Äî redesigned dashboard\n\nThe team wants to know whether average daily user engagement (minutes/day) differs between the two versions.\n\nHypotheses\n\\[H_0: \\mu_A = \\mu_B\\]\n\\[H_1: \\mu_A \\ne \\mu_B\\]\nSince population SD is unknown for both groups, we use a two-sample Welch t-test.\n\n\nGiven\n\nVersion A: \\(n_A = 800\\), \\(\\bar{x}_A = 12.5\\), \\(s_A = 6.2\\)\nVersion B: \\(n_B = 850\\), \\(\\bar{x}_B = 13.4\\), \\(s_B = 6.8\\)\n\n\n\nTest Statistic\n\\[\nt = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}\n\\]\nDegrees of freedom use the Welch approximation.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nTest statistic: \\(t = -2.93\\)\nCritical values: \\(t_{\\alpha/2, df} \\approx \\pm 1.96\\)\np-value: \\(p = 0.0034\\)\n\n\n\nDecision\nBecause \\(|t| &gt; 1.96\\) and \\(p &lt; 0.05\\), we reject \\(H_0\\).\n\n\nConclusion\nVersion B produces a statistically significant difference in average daily engagement compared to Version A.\n\n\n\nTwo Sample t-Test | Right-Tailed Test\nDoes Variant B increase engagement?\nNow the team wants a directional test:\nDoes Version B strictly increase user engagement?\n\nHypotheses\n\\[H_0: \\mu_B \\le \\mu_A\\]\n\\[H_1: \\mu_B &gt; \\mu_A\\]\n\n\nGiven\nSame data as Example 1:\n\n\\(n_A = 800\\), \\(\\bar{x}_A = 12.5\\), \\(s_A = 6.2\\)\n\n\\(n_B = 850\\), \\(\\bar{x}_B = 13.4\\), \\(s_B = 6.8\\)\n\n\n\nTest Statistic\n\\[\nt = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nInterpretation (Right-Tailed Test)\n\nTest statistic: \\(t = -2.93\\)\nCritical value: \\(t_{0.95, df} = 1.645\\)\np-value: \\(p = 0.998\\)\n\n\n\nDecision\nWe fail to reject \\(H_0\\).\n\n\nConclusion\nThere is no evidence that Version B increases user engagement.\n\n\n\nTwo Sample t-Test | Left-Tailed Test\nDoes Variant B reduce app load time?\nLoad time is a negative metric: smaller = better.\n\nHypotheses\n\\[H_0: \\mu_B \\ge \\mu_A\\]\n\\[H_1: \\mu_B &lt; \\mu_A\\]\n\n\nGiven\n\nVersion A: \\(\\bar{x}_A = 3.2\\) sec, \\(s_A = 1.4\\), \\(n_A = 600\\)\n\nVersion B: \\(\\bar{x}_B = 2.9\\) sec, \\(s_B = 1.3\\), \\(n_B = 620\\)\n\n\n\nTest Statistic\n\\[\nt = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{s_A^2/n_A + sB^2/n_B}}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nInterpretation (Left-Tailed Test)\n\nTest statistic: \\(t = 4.37\\)\nCritical value: \\(t_{0.05, df} = -1.645\\)\np-value: \\(p = 1.00\\)\n\n\n\nDecision\nWe fail to reject \\(H_0\\).\n\n\nConclusion\nThere is no evidence that Version B reduces app load time.\n\n\n\nCombined Summary Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Type\nScenario\nHypotheses\nTest Statistic\nCritical Value(s)\np-Value\nDecision\nInterpretation\n\n\n\n\nTwo-Tailed\n¬†Difference between App A and App B\n\\(H_0:\\mu_A=\\mu_B\\)  \\(H_1:\\mu_A\\ne\\mu_B\\)\n\\(t=-2.93\\)\n\\(\\pm 1.96\\)\n\\(p=0.003\\)\nReject\nEngagement is significantly different between A and B.\n\n\nRight-Tailed\nDoes B increase engagement vs A?\n\\(H_0:\\mu_B\\le\\mu_A\\)  \\(H_1:\\mu_B&gt;\\mu_A\\)\n\\(t=-2.93\\)\n\\(t_{0.95}=1.645\\)\n\\(p=0.99\\)\nFail to reject\nNo evidence that Version B increases engagement.\n\n\nLeft-Tailed\nDoes B reduce app load time vs A?\n\\(H_0:\\mu_B\\ge\\mu_A\\)  \\(H_1:\\mu_B&lt;\\mu_A\\)\n\\(t=4.37\\)\n\\(t_{0.05}=-1.645\\)\n\\(p=1.00\\)\nFail to rejectS\nNo evidence that Version B reduces load time.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 06: Inferential Statistics"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html",
    "href": "materials/statistics/session1.html",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "",
    "text": "This chapter introduces the foundations of data analytics and statistical thinking.\nStudents explore how data becomes insight, what types of analytics exist, and how organizations move through the data lifecycle‚Äîfrom collection to decision-making.\n\nBy the end of the class, you should be able to describe what data analytics is, recognize the main forms of analytics, and identify how data-related roles interact within a company.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#overview",
    "href": "materials/statistics/session1.html#overview",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "",
    "text": "This chapter introduces the foundations of data analytics and statistical thinking.\nStudents explore how data becomes insight, what types of analytics exist, and how organizations move through the data lifecycle‚Äîfrom collection to decision-making.\n\nBy the end of the class, you should be able to describe what data analytics is, recognize the main forms of analytics, and identify how data-related roles interact within a company.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#what-is-data-analytics",
    "href": "materials/statistics/session1.html#what-is-data-analytics",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "What Is Data Analytics?",
    "text": "What Is Data Analytics?\nData analytics is the practice of examining data systematically to discover useful information, reach conclusions, and support decision-making.\nIn practice, it bridges raw data and business strategy. The process typically follows five stages:\n\nData Generation ‚Äì transactions, sensors, user interactions, or surveys produce raw data.\n\nCollection and Storage ‚Äì data is gathered and kept in databases, data warehouses, or cloud systems.\n\nProcessing and Integration ‚Äì data is cleaned, formatted, and connected across sources.\n\nAnalysis and Modeling ‚Äì statistical or machine-learning methods reveal patterns and predict outcomes.\n\nCommunication and Action ‚Äì visualizations, dashboards, and reports communicate insights for business decisions.\n\nThis ‚Äúdata-to-insight‚Äù flow is iterative; every analysis generates new questions that feed the next cycle.\n\nBusiness Applications of Data Analytics\nAnalytics supports decision-making across many domains:\n\nTelecommunications: predicting churn, optimizing network performance, or tailoring offers.\n\nRetail: analyzing baskets, forecasting demand, or optimizing pricing.\n\nFinance: assessing credit risk or detecting fraud.\n\nMarketing: evaluating campaign effectiveness and designing experiments (A/B tests).\n\nHealthcare: identifying risk factors or optimizing treatment outcomes.\n\nThe unifying theme is that analytics translates data into value by informing actions.\n\n\n\nTypes of Analytics\nDifferent analytical approaches answer different questions.\n\n\n\n\n\n\n\n\n\nType\nCentral Question\nTypical Methods\nExample\n\n\n\n\nDescriptive\nWhat happened?\nAggregation, visualization\nMonthly sales by region\n\n\nDiagnostic\nWhy did it happen?\nCorrelation, segmentation\nAnalyzing the effect of pricing on sales\n\n\nPredictive\nWhat might happen?\nRegression, classification, forecasting\nPredicting churn probability\n\n\nPrescriptive\nWhat should we do?\nOptimization, simulation\nRecommending personalized offers\n\n\n\n\n\nExplanation with Lego\n\n\n\n\n\n\n\nTip\n\n\n\nTry fit your story according the above image.\n\n\n\n\nThe Data Lifecycle and Professional Roles\nThe data lifecycle describes how information moves through an organization and is transformed into insight.\n\nCapture / Collection: raw data is obtained from transactional systems, sensors, APIs, or external datasets.\n\nStorage / Management: engineers design and maintain databases, warehouses, or data lakes to store data efficiently and securely.\n\nProcessing / Transformation: data is cleaned, structured, and enriched to prepare it for analysis.\n\nAnalysis / Modeling: analytical teams explore data, test hypotheses, and build models that explain or predict outcomes.\n\nCommunication / Decision: results are shared through dashboards, visualizations, and reports to support business actions.\n\n\n\n\n\n\n\n\nRole\nPrimary Responsibilities\n\n\n\n\nData Engineer\nBuilds and maintains the data infrastructure, pipelines, and integrations that collect and store raw data.\n\n\nAnalytics Engineer\nBridges engineering and analysis by transforming raw data into clean, documented, and reusable datasets‚Äîoften using SQL, dbt, and data-modeling best practices. Ensures analysts and data scientists can work efficiently with trustworthy, well-structured data.\n\n\nData Analyst\nExplores datasets, performs aggregations, builds dashboards, and answers business questions through descriptive and diagnostic analysis.\n\n\nBusiness Analyst\nThe business analyst plays a similar role to the data analyst while bringing domain-specific knowledge to their work. A financial analyst, for example, is a type of business analyst who specializes in working with data from the finance industry.\n\n\nData Scientist\nApplies statistical and machine-learning techniques to uncover patterns, generate predictions, and perform experimentation.\n\n\nBusiness Stakeholder / Manager\nInterprets and acts on insights, ensuring that data-driven decisions translate into measurable outcomes.\n\n\n\nEach role contributes to a shared goal: establishing a reliable, end-to-end analytics pipeline that converts data into decisions.\n\n\n\n\n\n\nflowchart LR\nA[Capture / Collection] --&gt; B[Storage / Management]\nB --&gt; C[Processing / Transformation]\nC --&gt; D[Analysis / Modeling]\nD --&gt; E[Communication / Decision]\n\nclassDef stage fill:#f4f4f4,stroke:#555,stroke-width:1px,color:#000,font-weight:bold;\nclass A,B,C,D,E stage;\n\nsubgraph Roles [Key Roles]\n   DE[Data Engineer]\n   AE[Analytics Engineer]\n   DA[Data Analyst]\n   DS[Data Scientist]\n   BM[Business Manager]\nend\n\nDE -. supports .-&gt; A\nDE -. builds pipelines .-&gt; B\nAE -. transforms data .-&gt; C\nDA -. explores data .-&gt; D\nDS -. models data .-&gt; D\nBM -. acts on insight .-&gt; E\n\nstyle Roles fill:#f9f9f9,stroke:#ccc,stroke-width:1px,color:#333;",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#the-data-layers",
    "href": "materials/statistics/session1.html#the-data-layers",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "The Data Layers",
    "text": "The Data Layers\n\nDefinition of data\nData is used in its broadest sense: observations, measurements and facts (both quantitative and qualitative) that serve as information or evidence.\n\n\nZero-party data\nData intentionally or proactively shared by audiences, such as:\n\nResponses to polls, surveys or quizzes\nProfiling details added to online accounts or loyalty programmes\n\n\n\nFirst-party data\nProprietary data collected directly (with consent) via a company‚Äôs own channels. It typically captures behaviours and is a key marketing asset.\nCommon sources include:\n\nDigital interactions (website, apps)\nCustomer Relationship Management (CRM) systems\nContent engagement\nPoint-of-sale systems\nTransactions (accounting systems)\nInteractions with digital support and call centres\n\nExamples of first-party data products:\n\nSales performance metrics by region/country/category\nCRM extracts: contact info, purchase and interaction history for segmentation/personalization\nLoyalty/payment card purchase data showing behavioural patterns\nEmail marketing metrics: open rate, click-through rate, subscriber behaviour\n\n\n\nSecond-party data\nData not collected by the business itself but associated with its customers/audiences and obtained via a partnership or contractual agreement.\nExamples:\n\nRetail purchase data\nMarket research and survey data\nChannel partner/supplier data treated as first-party within that relationship\n\nTypical second-party sources in business:\n\nIn-store shopper research\nPanels/retail share data (e.g., Kantar/Nielsen/IRI/Mintel)\nBrand and communications tracking studies; ad testing; in-depth interviews\nWeb scraping (owned sites) and forums for sentiment, reviews, and competitive intelligence\n\n\n\nThird-party data\nData collected by another entity that doesn‚Äôt have a direct link to your customers/audiences, often aggregated and licensed for use. (Note: privacy regulations like GDPR have tightened access and usage.)\nTrusted third-party examples:\n\nSocial platforms‚Äô aggregated audience and behaviour insights\nExternal website analytics (e.g., Google Analytics) offering traffic, conversion, and interaction insights\nVendors (e.g., Experian/Acxiom/Dun & Bradstreet) providing profiles, segmentation, and targeting datasets\nOpen/public data (e.g., census aggregates)\n\n\nIn practice, most analytical teams draw from managed datasets in databases, platforms, or portals. These are often cleaned, contain business logic, and may blend multiple sources (e.g., CRM + ad-platform performance + third-party segments).",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#definition-of-insight",
    "href": "materials/statistics/session1.html#definition-of-insight",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "Definition of insight",
    "text": "Definition of insight\nAn actionable insight connects observations to what matters and what to do next:\n\nWhat? ‚Äî the observations from the data\n\nWhy? ‚Äî why it matters\n\nSo what? ‚Äî relevance and relative importance\n\nNow what? ‚Äî the recommended action\n\nKey reminders:\n\nA data point isn‚Äôt useful unless it links to why it matters.\nIt isn‚Äôt insightful unless it answers so what (why it‚Äôs relevant/important).\nIt isn‚Äôt actionable unless it leads to a feasible now what recommendation.\n\n\n‚Äì\n\nDefinition of Reporting\nReporting refers to the systematic process of turning raw data into structured, interpretable information that supports monitoring, decision-making, and accountability across an organization.\nIn other words Reporting is the practice of:\n\nCollecting data from one or more sources\nTransforming and aggregating it according to agreed business rules\nPresenting it in a consistent, repeatable format (tables, charts, dashboards, PDFs, spreadsheets)\n\n\n\n\n\n\n\nImportantkeyword\n\n\n\nthe key word is standardized\n\n\n\n\nDefinition of Storytelling\nStorytelling is the creation and telling of stories. In business settings, it is a form of communication that adapts structures from wider storytelling to persuade audiences to think, feel and act.\nData storytelling applies storytelling techniques to communicate insights, actions and ideas that come from data (rather than the raw data alone). It starts by understanding the data available within the broader goals, then draws out meaning from the background noise to guide the right conversations and decisions.\n\n\nData Reporting vs Data Story\nReporting focuses on standardized, repeatable outputs that explain the data point itself.\nStorytelling focuses on synthesizing evidence, reducing noise, and surfacing meaning and judgement to drive decisions and action. Visualization helps, but visualization alone is not a story.\n\n\n\n\n\n\n\n\nDimension\nData reporting\nData storytelling\n\n\n\n\nPrimary focus\nCommunicates the data\nCommunicates the insight and recommendations\n\n\nApproach\nStandardized\nCustomized to the situation and audience\n\n\nAudience effort\nRequires a data-literate audience\nDoes the hard work for the audience\n\n\nCore skills\nGood data visualization skills\nStrong critical thinking and creative communication\n\n\n\n\n\nSingle Source of Truth (SSOT)\nA Single Source of Truth (SSOT) is a practice in data management where one authoritative, trusted data source is designated for each data domain or metric.\nAll reports, dashboards, analyses, and decisions must rely on this source, ensuring consistency across the organization.\nIn simple terms:\none question \\(\\rightarrow\\) one correct answer, regardless of who asks it or where it is asked.\nWithout SSOT, organizations face inconsistent numbers, duplicated logic, and loss of trust in data. With SSOT, data becomes a strategic asset, not a source of conflict.\nKey benefits include:\n\nConsistency ‚Äì The same metric (e.g., revenue, churn, active users) has the same value everywhere.\nTrust ‚Äì Stakeholders trust data when numbers do not change between reports.\nEfficiency ‚Äì Analysts and engineers stop re-implementing the same logic repeatedly.\nFaster decision-making ‚Äì Less time spent reconciling discrepancies.\nGovernance & compliance ‚Äì Clear ownership and traceability of data definitions.\n\n\n\n\n\n\n\nNote\n\n\n\nWe are going to re-address this topics during the Capstone Project.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#videos",
    "href": "materials/statistics/session1.html#videos",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "Videos",
    "text": "Videos\n\nStatistical Thinking ‚Äì Data Understanding and Preparation: Watch here\n\nMaking Friends with ML: Watch here\n\nStorytelling with data Watch here",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#articles",
    "href": "materials/statistics/session1.html#articles",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "Articles",
    "text": "Articles\n\nChart Chooser\n\nWhy You Shouldn‚Äôt Use Pie Charts\n\nAbout the Types of Analytics\nBig Data\nSSOT",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#books-used-so-far",
    "href": "materials/statistics/session1.html#books-used-so-far",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "Books Used so far",
    "text": "Books Used so far\n\nThinking with Data ‚Äì by Max Shron. Download here\nStorytelling in Marketing - by Caroline Florence Downlaod here",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#assingment",
    "href": "materials/statistics/session1.html#assingment",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "Assingment",
    "text": "Assingment\nChoose an industry or organization that interests you and answer:\n\nWhat is a recurring business question that data could help answer?\n\nClearly state one repeated business problem or question.\nExplain briefly why this question matters to the organization.\n\nWhat data sources would be needed to address it?\n\nInternal data sources (e.g., transactions, customers, operations).\nExternal data sources (e.g., market data, competitors, environment).\nBriefly explain what each source provides.\n\nWhich type of analytics (descriptive, diagnostic, predictive, prescriptive) is most common today at your organization?\n\nIdentify the most commonly used analytics type.\nExplain how it is currently used (reports, dashboards, summaries).\n\nWhich type of analytics would apply?\n\nIdentify the most suitable analytics type for the business question.\nExplain why this type is more appropriate or valuable.\n\nWhat kind of decisions rely on data?\n\nStrategic decisions (long-term planning).\nTactical decisions (campaigns, resource allocation).\nOperational decisions (day-to-day actions).\n\nWhere do you see opportunities for improvement‚Äîbetter data, more automation, or clearer communication?\n\nData quality or availability improvements.\nAutomation of data pipelines or reporting.\nBetter visualization or communication of insights.\n\nSketch a simple flow diagram describing how data would move from source to insight.\n\nData sources.\nData collection and storage.\nData cleaning and processing.\nAnalysis and modeling.\nInsights and decision-making.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/session1.html#types-of-bias-in-data",
    "href": "materials/statistics/session1.html#types-of-bias-in-data",
    "title": "Statistics Session 01: Data Layers and Bias in Data",
    "section": "Types of Bias in Data",
    "text": "Types of Bias in Data\nUnderstanding bias is crucial in data analytics, as it affects the reliability and fairness of insights and models.\nBelow are the main types of bias with examples and ways to avoid them.\nAs junior data analyist it‚Äôs easy to fall into the trap of believing that the data is objective, that it‚Äôs the raw truth, and that it can‚Äôt be spun or misinterpreted. If you were to believe these things, however, you‚Äôd be wrong.\nData is produced either by humans or by machines and algorithms that have been created by humans.\nIn other words, data lends itself to human bias, which can lead to poor decision-making or the adoption of false beliefs. Fortunately, there‚Äôs a solution to this problem, and while it may not eliminate 100% of the analytical pitfalls of bias, it can help you see the truth you seek whenever you initiate a data analytics project.\nThis solution is data ethics, and one of its guiding principles is transparency around the way data is sampled, tested, and processed.\n\n\n\n\n\n\nNotedefinition\n\n\n\nBias can enter the data lifecycle at multiple stages.\nTo reason about it systematically, biases are grouped by where they originate and how they propagate downstream.\n\n\n\nData Coverage & Inclusion Biases\nBiases at this stage affect representation.\nIf present, all downstream analysis is structurally compromised.\nSellection Bias:\n\nSampling Bias\nExclusion Bias\nSurvivorship Bias\n\n\nSelection Bias\nHappens when certain groups are systematically excluded or included due to how data is selected.\nExample:\nA marketing campaign‚Äôs success is evaluated only on customers who opened emails, ignoring those who didn‚Äôt \\(\\rightarrow\\) engagement appears artificially high.\nHow to avoid:\n\nRandomize inclusion criteria; avoid convenience filtering (e.g., ‚Äúopeners only‚Äù).\n\nCompare included vs.¬†excluded groups; use propensity scores or re-weighting.\n\nExpand recruitment channels and reduce barriers to inclusion.\n\n\n\nreading\n\n\nSampling Bias\nOccurs when the data collected is not representative of the entire population.\nExample:\nA telecom company predicts churn using data only from urban customers, ignoring rural ones. The model will likely perform poorly for rural areas.\nHow to avoid:\n\nDefine the target population explicitly and sample across all key segments (e.g., urban/rural, device types).\n\nUse probability sampling where possible; if not, weight to population benchmarks.\n\nMonitor sample composition continuously and correct drift.\n\n\n\nreading\n\n\n\nSurvivorship Bias\nFocusing only on successful cases while ignoring failures.\nExample:\nAnalyzing only successful marketing campaigns inflates perceived effectiveness.\nHow to avoid:\n\nTrack full cohorts, including churned/inactive or failed tests.\n\nReport denominator/attrition; analyze exits explicitly.\n\nAvoid filtering by ‚Äúsurvived‚Äù outcomes in exploratory steps.\n\n\n\n\nreading\n\n\n\nExclusion Bias\nImportant variables are mistakenly left out during data collection or preprocessing.\nExample:\nAn e-commerce model excludes device_type (mobile vs.¬†desktop), missing behavior differences that affect conversion.\nHow to avoid:\n\nMap requirements with domain experts; maintain a \"must-have\" variable inventory.\n\nTrace feature lineage; run ablation tests to detect missing signal.\n\nIterate collection forms/ETL to capture omitted fields.\n\n\n\nreading\n\n\n\n\n\nData Collection & Measurement Biases\nBiases at this stage affect how data is recorded and reported.\nEven with correct population coverage, poor measurement distorts reality.\nMeasurement Biases:\n\nMeasurement Bias\n\nRecall Bias\n\nResponse Bias\n\nObserver Bias\n\n\n\nMeasurement Bias\nArises from inaccurate tools or methods used to collect data.\nExample:\nA survey app records 0 when users skip a question instead of missing, misleading analysts to think respondents selected zero.\nHow to avoid:\n\nStandardize definitions and validation rules; treat missing explicitly.\n\nCalibrate and test instruments; run overlap/parallel periods when switching tools.\n\nInclude data-quality checks (range, type, logic) in ETL.\n\n\nreading\n\n\nRecall Bias\nOccurs when participants don‚Äôt accurately remember past events.\nExample:\nWhen asked how many times they visited a store last month, respondents under/over-report due to memory errors.\nHow to avoid:\n\nShorten recall windows; use diaries or passive behavioral data where possible.\n\nAsk concrete, bounded questions (‚Äúin the last 7 days‚Äù).\n\nProvide anchors/examples to improve recall.\n\n\nreading\n\n\n\n\nResponse Bias\nParticipants give socially desirable or expected answers rather than truthful ones.\nExample:\nIn a satisfaction survey, customers rate service higher to appear polite.\nHow to avoid:\n\nUse neutral wording and anonymity; avoid leading questions.\n\nPrefer behavioral measures over attitudinal when possible.\n\nInclude validity checks (e.g., reverse-coded items).\n\n\nreading\n\n\n\n\nObserver Bias\nA researcher‚Äôs expectations influence data collection or interpretation.\nExample:\nAn analyst expecting a new ad to perform better focuses on positive feedback and downplays negatives.\nHow to avoid:\n\nBlind analysts to treatment where feasible; pre-register analysis plans.\n\nUse objective scoring rubrics and inter-rater reliability checks.\n\nAutomate extraction or labeling where appropriate.\n\n\nreading\n\n\n\n\n\n\nAnalysis & Modeling Biases\nBiases at this stage affect interpretation, reasoning, and model learning.\nThey often amplify earlier data issues.\nAnalysis Biases:\n\nConfirmation Bias\n\nAvailability Bias\n\nHistorical Bias\n\nAlgorithmic Bias\n\n\n\nConfirmation Bias\nTendency to favor data that confirms existing beliefs or hypotheses.\nExample:\nA data scientist believes discounts improve retention and analyzes only high-discount months.\nHow to avoid:\n\nWrite the research question and success criteria before analysis.\n\nDeliberately seek disconfirming evidence; run robustness checks.\n\nUse holdout periods and peer review.\n\n\n\nreading\n\n\n\n\nAvailability Bias\nRecent or vivid events are over-weighted in judgment.\nExample:\nAfter widely covered plane crashes, people overestimate crash risk.\nHow to avoid:\n\nUse base rates and long-run averages; automate risk calculations.\n\nSeek counter-examples and historical context before concluding.\n\n\nreading\n\n\n\n\nHistorical Bias\nOutdated or biased historical data perpetuates inequalities.\nExample:\nA credit model trained on years of biased lending data continues to disadvantage certain income groups.\nHow to avoid:\n\nAudit legacy datasets for representation and harmful proxies.\n\nRefresh training data; use time-aware validation.\n\nApply fairness constraints and monitor subgroup performance.\n\n\nreading\n\n\n\n\nAlgorithmic Bias\nAlgorithms learn or amplify biased patterns from data.\nExample:\nA hiring model trained on past decisions that favored men continues to favor male applicants.\nHow to avoid:\n\nRemove or regularize proxy features; conduct fairness and drift audits.\n\nEvaluate with subgroup metrics (e.g., TPR/FPR parity) and add constraints.\n\nRetrain with de-biased data; use post-processing where lawful.\n\n\nreading\n\n\n\n\n\nReporting & Communication Biases\nBiases at this stage affect how insights are presented and interpreted by decision-makers.\n\nReporting Bias\nSelective presentation of data or results that favor a narrative.\nExample:\nA company highlights higher click-through rate but hides the drop in customer satisfaction.\nHow to avoid:\n\nPredefine the reporting bundle (primary + guardrail metrics).\n\nShow uncertainty, denominators, and counter-metrics.\n\nPublish full results or an appendix; avoid cherry-picking.\n\n\nreading\n\n\n\n\nBias Summary\n\n\n\n\n\n\n\n\nBias Type\nKey Cause\nExample Context\n\n\n\n\nSelection Bias\nNon-random inclusion/exclusion\nOnly counting email openers\n\n\nSampling Bias\nNon-representative sample\nUsing only urban data for churn model\n\n\nSurvivorship Bias\nIgnoring failures or exits\nStudying only successful campaigns\n\n\nExclusion Bias\nMissing important variables\nOmitting device type in conversion model\n\n\nMeasurement Bias\nFaulty data collection or encoding\n0 recorded instead of missing\n\n\nRecall Bias\nInaccurate memory\nSelf-reported store visits\n\n\nResponse Bias\nSocial desirability\nOverrated satisfaction scores\n\n\nObserver Bias\nResearcher expectations\nAnalyst highlights positive feedback\n\n\nConfirmation Bias\nFavoring expected results\nIgnoring non-discount periods\n\n\nAvailability Bias\nRecency or vividness\nOverestimating crash risk after news\n\n\nHistorical Bias\nBiased legacy data\nCredit score discrimination\n\n\nAlgorithmic Bias\nModel amplifies biased patterns\nGender bias in hiring model\n\n\nReporting Bias\nSelective result presentation\nHiding negative KPIs",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 01: Data Layers and Bias in Data"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#hypothesis-testing-intuition",
    "href": "materials/statistics/slides/session6.html#hypothesis-testing-intuition",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Testing | Intuition",
    "text": "Hypothesis Testing | Intuition\nWe Data Analyists/Statisticians like to make an assumptions about the value of a population parameter, and then:\n\n\ncollect a sample from that population,\nmeasure the sample,\ndeclare in ascholarly manner, whether the sample supports the original assumption.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#stating-the-hypothesis",
    "href": "materials/statistics/slides/session6.html#stating-the-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Stating the Hypothesis",
    "text": "Stating the Hypothesis\nA recent Wall Street Journal article titled ‚ÄúDoes the Internet Make You Smarter or Dumber?‚Äù posed the possibility that online activities turn us into shallow thinkers.\nThe article cited a statistic claiming that the average time an American spends looking at a Web page is 56 seconds.\n\nA researcher at a local university would like to test this claim using a hypothesis test.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#null-hypothesis",
    "href": "materials/statistics/slides/session6.html#null-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Null Hypothesis",
    "text": "Null Hypothesis\nThe null hypothesis \\(H_0\\), represents the status quo and involves stating the belief that the population parameter is \\(\\le,=, \\ge\\) a specific value.\n\nThe null hypothesis is believed to be true unless there is overwhelming evidence to the contrary.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#alternative-hypothesis",
    "href": "materials/statistics/slides/session6.html#alternative-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Alternative hypothesis",
    "text": "Alternative hypothesis\nThe alternative hypothesis, \\(H_1\\), represents the opposite of the null hypothesis and is believed to be true if the null hypothesis is found to be false.\n\nThe alternative hypothesis always states that the population parameter is \\(\\gt,\\ne, \\lt\\) a specific value.\n\n\nYou need to be careful how you state the null and alternative hypotheses.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#analyogy-with-the-legal-system-1",
    "href": "materials/statistics/slides/session6.html#analyogy-with-the-legal-system-1",
    "title": "Hypothesis Testing",
    "section": "Analyogy with the Legal System | 1",
    "text": "Analyogy with the Legal System | 1\nWe can either reject it or not reject it (fail to reject it)\n\nThe court system assumes a persion is innocent until proven guily, the hypothesis test is formulated as follows:\n\n\\(H_0:\\) the defendant is innocent (status quo)\n\\(H_1:\\) the defendant is guilty",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#analyogy-with-the-legal-system-2",
    "href": "materials/statistics/slides/session6.html#analyogy-with-the-legal-system-2",
    "title": "Hypothesis Testing",
    "section": "Analyogy with the Legal System | 2",
    "text": "Analyogy with the Legal System | 2\nThe court system might have two conclusions:\n\nReject the Null Hypothesis \\(\\rightarrow\\) the defendant is guilty\nFail to reject the Null Hypothesis \\(\\rightarrow\\) the defendant is guilty",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#steps-in-hypothesis-testing",
    "href": "materials/statistics/slides/session6.html#steps-in-hypothesis-testing",
    "title": "Hypothesis Testing",
    "section": "Steps In Hypothesis Testing",
    "text": "Steps In Hypothesis Testing\n\nIdentify the null and alternative hypotheses\nSet a value for the significance level \\(\\alpha\\)\nDetermine the appropriate critical value\nCalculate the appropriate test statistic\nCompare the test statistics with the critical score\nState your conclusion",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#hypothesis-stamenet",
    "href": "materials/statistics/slides/session6.html#hypothesis-stamenet",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Stamenet",
    "text": "Hypothesis Stamenet\nIn this example the Internet users spend an average timeof 56 seconds on a Web page.\nStatus Quo\n\n\\[\nH_0: \\mu = 56 \\text{ seconds (status quo)}\n\\]\nAlternative:\n\n\n\\[\nH_1: \\mu \\ne  56 \\text{ seconds}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#all-the-combinations",
    "href": "materials/statistics/slides/session6.html#all-the-combinations",
    "title": "Hypothesis Testing",
    "section": "All the Combinations",
    "text": "All the Combinations\n\n\n\n\n\n\n\n\n\n\nTwo-Tailed Test\nLeft-Tailed Test\nRight-Tailed Test\n\n\n\n\nNull\n\\[H_0: \\mu = 56\\]\n\\[H_0: \\mu \\ge 56\\]\n\\[H_0: \\mu \\le 56\\]\n\n\nAlternative\n\\[H_1: \\mu \\ne 56\\]\n\\[H_1: \\mu &lt; 56\\]\n\\[H_1: \\mu &gt; 56\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#set-significance-level",
    "href": "materials/statistics/slides/session6.html#set-significance-level",
    "title": "Hypothesis Testing",
    "section": "Set Significance Level",
    "text": "Set Significance Level\nThe level of significance represents the probability of making a Type I error. A Type I error occurs when we reject the null hypothesis but it is actually true. The common Value: \\(\\alpha = 0.05\\)\n\n\nIn the scope the above example Type I Error we conclude that the true average time spent on a webpage is not 56 seconds, even though in reality it is 56 seconds.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#in-out-hypothesis-context",
    "href": "materials/statistics/slides/session6.html#in-out-hypothesis-context",
    "title": "Hypothesis Testing",
    "section": "In Out Hypothesis Context",
    "text": "In Out Hypothesis Context\n\nThe researcher conducts a sample study.\nThe sample mean differs enough from 56 seconds to fall outside the acceptance region.\nThe test rejects the null hypothesis at the 5%level.\nBut in truth, Americans do spend exactly 56 seconds on average.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#type-i-vs-type-ii",
    "href": "materials/statistics/slides/session6.html#type-i-vs-type-ii",
    "title": "Hypothesis Testing",
    "section": "Type I vs Type II",
    "text": "Type I vs Type II",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#determining-critical-values-z-test",
    "href": "materials/statistics/slides/session6.html#determining-critical-values-z-test",
    "title": "Hypothesis Testing",
    "section": "Determining Critical Values | z-test",
    "text": "Determining Critical Values | z-test\nWhen Variance is known\n\ntest statistic would be \\(z_{\\bar{x}}\\)\n\ncritical value would be \\(z_{\\alpha}\\)\n\n$$ z_{{x}} = \n$$\n\nGiven\n\nClaimed mean: \\(\\mu_{H_0} = 56\\) seconds\n\nSample mean: \\(\\bar{x} = 62\\) seconds\n\nPopulation standard deviation (historical estimate): \\(\\sigma = 18\\) seconds\n\nSample size: \\(n = 45\\)\n\n\\[\nz_{\\bar{x}}\n= \\frac{\\bar{x} - \\mu_{H_0}}{\\sigma / \\sqrt{n}}\n= \\frac{62 - 56}{18 / \\sqrt{45}}\n= \\frac{6}{2.683}\n= 2.24\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#interpretation-z-test",
    "href": "materials/statistics/slides/session6.html#interpretation-z-test",
    "title": "Hypothesis Testing",
    "section": "Interpretation | z-test",
    "text": "Interpretation | z-test\nA test statistic of 2.24 lies well into the rejection region (far from 0).\n\nFor a two-tailed test at \\(\\alpha = 0.05\\), the critical values are \\(\\pm 1.96\\).\n\nSince 2.24 &gt; 1.96, we reject \\(H_0\\).\n\nInterpretation:\nThere is statistically significant evidence that the true average time spent on a webpage is different from 56 seconds.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#graphical-representation",
    "href": "materials/statistics/slides/session6.html#graphical-representation",
    "title": "Hypothesis Testing",
    "section": "Graphical Representation",
    "text": "Graphical Representation",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#z-value",
    "href": "materials/statistics/slides/session6.html#z-value",
    "title": "Hypothesis Testing",
    "section": "Z Value",
    "text": "Z Value\n\n\n\n\n\n\n\n\n\nTest Type\nHypotheses\nCondition\nConclusion\n\n\n\n\nTwo-tail\n\\(H_0: \\mu = \\mu_0\\)\n\\(\\lvert z_x \\rvert &gt; z_{\\alpha/2}\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)\n\\(\\lvert z_x \\rvert \\le z_{\\alpha/2}\\)\nDo not reject \\(H_0\\)\n\n\nOne-tail\n\\(H_0: \\mu \\le \\mu_0\\)\n\\(z_x &gt; z_\\alpha\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\)\n\\(z_x \\le z_\\alpha\\)\nDo not reject \\(H_0\\)\n\n\nOne-tail\n\\(H_0: \\mu \\ge \\mu_0\\)\n\\(z_x &lt; -z_\\alpha\\)\nReject \\(H_0\\)\n\n\n\n\\(H_1: \\mu &lt; \\mu_0\\)\n\\(z_x \\ge -z_\\alpha\\)\nDo not reject \\(H_0\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#p-value",
    "href": "materials/statistics/slides/session6.html#p-value",
    "title": "Hypothesis Testing",
    "section": "P Value",
    "text": "P Value\nIt is mor convenient the use p-value apporach, as it helps us to rememeber the decission rule easily.\nIf the p-value is less than \\(\\alpha\\), there is little chance of observing the sample mean from the population on which it is based if the null hypothesis were actually true. We therefore reject the null hypothesis under this condition.\n\n\n\nCondition\nConclusion\n\n\n\n\n\\(p\\text{-Value} \\ge \\alpha\\)\nDo not reject \\(H_0\\)\n\n\n\\(p\\text{-Value} &lt; \\alpha\\)\nReject \\(H_0\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session6.html#visuall-representation",
    "href": "materials/statistics/slides/session6.html#visuall-representation",
    "title": "Hypothesis Testing",
    "section": "Visuall Representation",
    "text": "Visuall Representation",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#overview",
    "href": "materials/statistics/slides/session1.html#overview",
    "title": "Descriptive Stats",
    "section": "Overview",
    "text": "Overview\nThis module introduces the foundations of data analytics and statistical thinking.\nStudents explore how data becomes insight, what types of analytics exist, and how organizations move through the data lifecycle‚Äîfrom collection to decision-making.\n\nBy the end of the class, you should be able to describe what data analytics is, recognize the main forms of analytics, and identify how data-related roles interact within a company.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#what-is-data-analytics",
    "href": "materials/statistics/slides/session1.html#what-is-data-analytics",
    "title": "Descriptive Stats",
    "section": "What Is Data Analytics?",
    "text": "What Is Data Analytics?\nData analytics is the practice of examining data systematically to discover useful information, reach conclusions, and support decision-making.\n\nIn practice, it bridges raw data and business strategy.\n\nData Generation ‚Äì transactions, sensors, user interactions, or surveys produce raw data.\n\nCollection and Storage ‚Äì data is gathered and kept in databases, data warehouses, or cloud systems.\n\nProcessing and Integration ‚Äì data is cleaned, formatted, and connected across sources.\n\nAnalysis and Modeling ‚Äì statistical or machine-learning methods reveal patterns and predict outcomes.\n\nCommunication and Action ‚Äì visualizations, dashboards, and reports communicate insights for business decisions.\n\n\n\n\n\n\n\n\nImportant\n\n\nThis data-to-insight flow is iterative; every analysis generates new questions that feed the next cycle.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#business-applications-of-data-analytics",
    "href": "materials/statistics/slides/session1.html#business-applications-of-data-analytics",
    "title": "Descriptive Stats",
    "section": "Business Applications of Data Analytics",
    "text": "Business Applications of Data Analytics\nAnalytics supports decision-making across many domains:\n\nTelecommunications: predicting churn, optimizing network performance, or tailoring offers.\n\nRetail: analyzing baskets, forecasting demand, or optimizing pricing.\n\nFinance: assessing credit risk or detecting fraud.\n\nMarketing: evaluating campaign effectiveness and designing experiments (A/B tests).\n\nHealthcare: identifying risk factors or optimizing treatment outcomes.\n\nThe unifying theme is that analytics translates data into value by informing actions.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#types-of-analytics",
    "href": "materials/statistics/slides/session1.html#types-of-analytics",
    "title": "Descriptive Stats",
    "section": "Types of Analytics",
    "text": "Types of Analytics\nDifferent analytical approaches answer different questions.\n\n\n\n\n\n\n\n\n\nType\nCentral Question\nTypical Methods\nExample\n\n\n\n\nDescriptive\nWhat happened?\nAggregation, visualization\nMonthly sales by region\n\n\nDiagnostic\nWhy did it happen?\nCorrelation, segmentation\nAnalyzing the effect of pricing on sales\n\n\nPredictive\nWhat might happen?\nRegression, classification, forecasting\nPredicting churn probability\n\n\nPrescriptive\nWhat should we do?\nOptimization, simulation\nRecommending personalized offers",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#explanation-with-lego",
    "href": "materials/statistics/slides/session1.html#explanation-with-lego",
    "title": "Descriptive Stats",
    "section": "Explanation with Lego",
    "text": "Explanation with Lego",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#the-data-lifecycle",
    "href": "materials/statistics/slides/session1.html#the-data-lifecycle",
    "title": "Descriptive Stats",
    "section": "The Data Lifecycle",
    "text": "The Data Lifecycle\nThe data lifecycle describes how information moves through an organization and is transformed into insight.\n\nCapture / Collection: raw data is obtained from transactional systems, sensors, APIs, or external datasets.\n\nStorage / Management: engineers design and maintain databases, warehouses, or data lakes to store data efficiently and securely.\n\nProcessing / Transformation: data is cleaned, structured, and enriched to prepare it for analysis.\n\nAnalysis / Modeling: analytical teams explore data, test hypotheses, and build models that explain or predict outcomes.\n\nCommunication / Decision: results are shared through dashboards, visualizations, and reports to support business actions.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#professional-roles",
    "href": "materials/statistics/slides/session1.html#professional-roles",
    "title": "Descriptive Stats",
    "section": "Professional Roles",
    "text": "Professional Roles\n\n\n\n\n\n\n\nRole\nPrimary Responsibilities\n\n\n\n\nData Engineer\nBuilds and maintains the data infrastructure, pipelines, and integrations that collect and store raw data.\n\n\nAnalytics Engineer\nBridges engineering and analysis by transforming raw data into clean, documented, and reusable datasets‚Äîoften using SQL, dbt, and data-modeling best practices. Ensures analysts and data scientists can work efficiently with trustworthy, well-structured data.\n\n\nData Analyst\nExplores datasets, performs aggregations, builds dashboards, and answers business questions through descriptive and diagnostic analysis.\n\n\nBusiness Analyst\nThe business analyst plays a similar role to the data analyst while bringing domain-specific knowledge to their work. A financial analyst, for example, is a type of business analyst who specializes in working with data from the finance industry.\n\n\nData Scientist\nApplies statistical and machine-learning techniques to uncover patterns, generate predictions, and perform experimentation.\n\n\nBusiness Stakeholder / Manager\nInterprets and acts on insights, ensuring that data-driven decisions translate into measurable outcomes.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#flow",
    "href": "materials/statistics/slides/session1.html#flow",
    "title": "Descriptive Stats",
    "section": "Flow",
    "text": "Flow\n\n\n\n\n\n\nflowchart LR\n    A[Capture / Collection] --&gt; B[Storage / Management]\n    B --&gt; C[Processing / Transformation]\n    C --&gt; D[Analysis / Modeling]\n    D --&gt; E[Communication / Decision]\n\n    classDef stage fill:#f4f4f4,stroke:#555,stroke-width:1px,color:#000,font-weight:bold;\n    class A,B,C,D,E stage;\n\n    subgraph Roles [Key Roles]\n        DE[Data Engineer]\n        AE[Analytics Engineer]\n        DA[Data Analyst]\n        DS[Data Scientist]\n        BM[Business Manager]\n    end\n\n    DE -. supports .-&gt; A\n    DE -. builds pipelines .-&gt; B\n    AE -. transforms data .-&gt; C\n    DA -. explores data .-&gt; D\n    DS -. models data .-&gt; D\n    BM -. acts on insight .-&gt; E\n\n    style Roles fill:#f9f9f9,stroke:#ccc,stroke-width:1px,color:#333;",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#definition-of-data",
    "href": "materials/statistics/slides/session1.html#definition-of-data",
    "title": "Descriptive Stats",
    "section": "Definition of data",
    "text": "Definition of data\n\n\n\n\n\n\n\nDefinition\n\n\nData is used in its broadest sense: observations, measurements and facts (both quantitative and qualitative) that serve as information or evidence.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#zero-party-data",
    "href": "materials/statistics/slides/session1.html#zero-party-data",
    "title": "Descriptive Stats",
    "section": "Zero-party data",
    "text": "Zero-party data\nData intentionally or proactively shared by audiences, such as:\n\nResponses to polls, surveys or quizzes\nProfiling details added to online accounts or loyalty programmes",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#first-party-data",
    "href": "materials/statistics/slides/session1.html#first-party-data",
    "title": "Descriptive Stats",
    "section": "First-party data",
    "text": "First-party data\nProprietary data collected directly (with consent) via a company‚Äôs own channels:\n\nDigital interactions (website, apps)\nCustomer Relationship Management (CRM) systems\nContent engagement\nPoint-of-sale systems\nTransactions (accounting systems)\nInteractions with digital support and call centres",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#first-party-data-examples",
    "href": "materials/statistics/slides/session1.html#first-party-data-examples",
    "title": "Descriptive Stats",
    "section": "First-party data | Examples",
    "text": "First-party data | Examples\nExamples of first-party data products:\n\n\nSales performance metrics by region/country/category\nCRM extracts: contact info, purchase and interaction history for segmentation/personalization\nLoyalty/payment card purchase data showing behavioural patterns\nEmail marketing metrics: open rate, click-through rate, subscriber behaviour",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#second-party-data",
    "href": "materials/statistics/slides/session1.html#second-party-data",
    "title": "Descriptive Stats",
    "section": "Second-party data",
    "text": "Second-party data\nData not collected by the business itself but associated with its customers/audiences and obtained via a partnership or contractual agreement.\nExamples:\n\n\nRetail purchase data\nMarket research and survey data\nChannel partner/supplier data treated as first-party within that relationship\n\nTypical second-party sources in business:\n\nIn-store shopper research\nPanels/retail share data (e.g., Kantar/Nielsen/IRI/Mintel)\nBrand and communications tracking studies; ad testing; in-depth interviews\nWeb scraping (owned sites) and forums for sentiment, reviews, and competitive intelligence",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#third-party-data",
    "href": "materials/statistics/slides/session1.html#third-party-data",
    "title": "Descriptive Stats",
    "section": "Third-party data",
    "text": "Third-party data\nTrusted third-party examples:\n\nSocial platforms‚Äô aggregated audience and behaviour insights\nExternal website analytics (e.g., Google Analytics) offering traffic, conversion, and interaction insights\nVendors (e.g., Experian/Acxiom/Dun & Bradstreet) providing profiles, segmentation, and targeting datasets\nOpen/public data (e.g., census aggregates)\n\n\n\n\n\n\n\n\nAbout Regulations\n\n\nData collected by another entity that doesn‚Äôt have a direct link to your customers/audiences, often aggregated and licensed for use. The privacy regulations like GDPR have tightened access and usage.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#definition-of-insight",
    "href": "materials/statistics/slides/session1.html#definition-of-insight",
    "title": "Descriptive Stats",
    "section": "Definition of insight",
    "text": "Definition of insight\nAn actionable insight connects observations to what matters and what to do next:\n\nWhat? ‚Äî the observations from the data\n\nWhy? ‚Äî why it matters\n\nSo what? ‚Äî relevance and relative importance\n\nNow what? ‚Äî the recommended action",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#definition-of-insight-what-why-so-what-now-what",
    "href": "materials/statistics/slides/session1.html#definition-of-insight-what-why-so-what-now-what",
    "title": "Descriptive Stats",
    "section": "Definition of insight | What? Why? So what? Now what?",
    "text": "Definition of insight | What? Why? So what? Now what?\n\nKey reminders:\n\nA data point isn‚Äôt useful unless it links to why it matters.\nIt isn‚Äôt insightful unless it answers so what (why it‚Äôs relevant/important).\nIt isn‚Äôt actionable unless it leads to a feasible now what recommendation.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#reporting",
    "href": "materials/statistics/slides/session1.html#reporting",
    "title": "Descriptive Stats",
    "section": "Reporting",
    "text": "Reporting\nReporting is the systematic process of turning raw data into structured, interpretable information that supports monitoring, decision-making, and accountability.\nIt focuses on consistency, repeatability, and standardization.\nReporting in practice:\n\nCollecting data from one or more sources\n\nTransforming and aggregating data using agreed business rules\n\nPresenting outputs in standardized formats such as dashboards, tables, PDFs, or spreadsheets\n\n\n\n\n\n\n\n\nKeyword\n\n\nThe key word in reporting is standardized.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#storytelling",
    "href": "materials/statistics/slides/session1.html#storytelling",
    "title": "Descriptive Stats",
    "section": "Storytelling",
    "text": "Storytelling\nStorytelling is a communication practice that uses narrative structure to help audiences understand, interpret, and act.\nData storytelling applies storytelling techniques to communicate:\n\nInsights\n\nImplications\n\nRecommended actions\n\nIt goes beyond raw data by adding context, judgment, and meaning.\nVisualization supports storytelling, but visualization alone is not a story.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#reporting-vs-storytelling-1",
    "href": "materials/statistics/slides/session1.html#reporting-vs-storytelling-1",
    "title": "Descriptive Stats",
    "section": "Reporting vs Storytelling",
    "text": "Reporting vs Storytelling\n\n\n\n\n\n\n\n\nDimension\nData Reporting\nData Storytelling\n\n\n\n\nPrimary focus\nCommunicates the data\nCommunicates the insight and action\n\n\nApproach\nStandardized and repeatable\nCustomized to audience and situation\n\n\nAudience effort\nRequires data literacy\nReduces cognitive load for the audience\n\n\nCore skills\nData accuracy and visualization\nCritical thinking and communication\n\n\nTypical output\nDashboards, reports, KPIs\nNarratives, recommendations, decisions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#single-source-of-truth-ssot",
    "href": "materials/statistics/slides/session1.html#single-source-of-truth-ssot",
    "title": "Descriptive Stats",
    "section": "Single Source of Truth (SSOT)",
    "text": "Single Source of Truth (SSOT)\nA Single Source of Truth (SSOT) is a practice where one authoritative data source is defined for each metric or domain.\nAll reports, dashboards, and analyses rely on this source.\nIn simple terms:\nOne question ‚Üí one correct answer\nWhy SSOT matters:\n\nConsistency across reports\n\nTrust in numbers\n\nReduced duplicated logic\n\nFaster, more confident decision-making\n\nClear governance and ownership",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#video",
    "href": "materials/statistics/slides/session1.html#video",
    "title": "Descriptive Stats",
    "section": "Video",
    "text": "Video\nLet‚Äôs watch this",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#types-of-bias-in-data",
    "href": "materials/statistics/slides/session1.html#types-of-bias-in-data",
    "title": "Descriptive Stats",
    "section": "Types of Bias in Data",
    "text": "Types of Bias in Data\nUnderstanding bias is crucial in data analytics, as it affects the reliability and fairness of insights and models.\nAs junior data analysts, it‚Äôs easy to fall into the trap of believing that data is objective, that it represents raw truth, and that it cannot be misinterpreted.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#where-bias-comes-from",
    "href": "materials/statistics/slides/session1.html#where-bias-comes-from",
    "title": "Descriptive Stats",
    "section": "Where Bias Comes From",
    "text": "Where Bias Comes From\nData is produced either by humans or by machines and algorithms created by humans.\n\\[\\Downarrow\\]\nData inevitably reflects human assumptions, choices, and limitations.\nBias can therefore lead to poor decisions or false beliefs if left unchecked.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#data-ethics-and-bias",
    "href": "materials/statistics/slides/session1.html#data-ethics-and-bias",
    "title": "Descriptive Stats",
    "section": "Data Ethics and Bias",
    "text": "Data Ethics and Bias\nOne of the core principles of data ethics is transparency in how data is:\n\nCollected\n\nSampled\n\nProcessed\n\nInterpreted",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#remember",
    "href": "materials/statistics/slides/session1.html#remember",
    "title": "Descriptive Stats",
    "section": "Remember",
    "text": "Remember\n  \nBias awareness does not eliminate all issues, but it significantly reduces analytical risk.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#data-coverage-inclusion-biases",
    "href": "materials/statistics/slides/session1.html#data-coverage-inclusion-biases",
    "title": "Descriptive Stats",
    "section": "Data Coverage & Inclusion Biases",
    "text": "Data Coverage & Inclusion Biases\nBiases at this stage affect representation.\nIf present, all downstream analysis is structurally compromised.\nSelection Biases:\n\nSampling\nExclusion\nSurvivorship",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#selection-bias-11",
    "href": "materials/statistics/slides/session1.html#selection-bias-11",
    "title": "Descriptive Stats",
    "section": "Selection Bias | 1/1",
    "text": "Selection Bias | 1/1\nHappens when certain groups are systematically excluded or included due to how data is selected.\nExample:\nA marketing campaign‚Äôs success is evaluated only on customers who opened emails, ignoring those who didn‚Äôt ‚Üí engagement appears artificially high.\nHow to Avoid?\n\n\nRandomize inclusion criteria; avoid convenience filtering (e.g., ‚Äúopeners only‚Äù).\n\nCompare included vs.¬†excluded groups; use propensity scores or re-weighting.\n\nExpand recruitment channels and reduce barriers to inclusion.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#selection-bias-22",
    "href": "materials/statistics/slides/session1.html#selection-bias-22",
    "title": "Descriptive Stats",
    "section": "Selection Bias | 2/2",
    "text": "Selection Bias | 2/2\n\n.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#sampling-bias-12",
    "href": "materials/statistics/slides/session1.html#sampling-bias-12",
    "title": "Descriptive Stats",
    "section": "Sampling Bias | 1/2",
    "text": "Sampling Bias | 1/2\nOccurs when the data collected is not representative of the entire population.\nExample:\nA telecom company predicts churn using data only from urban customers, ignoring rural ones.\nThe model will likely perform poorly for rural areas.\nHow to Avoid?\n\n\nDefine the target population explicitly and sample across all key segments.\n\nUse probability sampling where possible; otherwise weight to population benchmarks.\n\nMonitor sample composition continuously and correct drift.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#sampling-bias-22",
    "href": "materials/statistics/slides/session1.html#sampling-bias-22",
    "title": "Descriptive Stats",
    "section": "Sampling Bias | 2/2",
    "text": "Sampling Bias | 2/2\n\n.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#survivorship-bias-12",
    "href": "materials/statistics/slides/session1.html#survivorship-bias-12",
    "title": "Descriptive Stats",
    "section": "Survivorship Bias | 1/2",
    "text": "Survivorship Bias | 1/2\nFocusing only on successful cases while ignoring failures.\nExample:\nAnalyzing only successful marketing campaigns inflates perceived effectiveness.\nHow to Avoid?\n\n\nTrack full cohorts, including churned, inactive, or failed cases.\n\nReport denominators and attrition explicitly.\n\nAvoid filtering by ‚Äúsurvived‚Äù outcomes in exploratory analysis.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#survivorship-bias-22",
    "href": "materials/statistics/slides/session1.html#survivorship-bias-22",
    "title": "Descriptive Stats",
    "section": "Survivorship Bias | 2/2",
    "text": "Survivorship Bias | 2/2\n\n.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#exclusion-bias",
    "href": "materials/statistics/slides/session1.html#exclusion-bias",
    "title": "Descriptive Stats",
    "section": "Exclusion Bias",
    "text": "Exclusion Bias\nImportant variables are mistakenly left out during data collection or preprocessing.\n\nExample:\nAn e-commerce model excludes device_type (mobile vs.¬†desktop), missing behavior differences that affect conversion.\nHow to Avoid?\n\n\n\nMap requirements with domain experts; maintain a \"must-have\" variable inventory.\n\nTrace feature lineage; run ablation tests to detect missing signal.\n\nIterate collection forms and ETL to capture omitted fields.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#exclusion-bias-1",
    "href": "materials/statistics/slides/session1.html#exclusion-bias-1",
    "title": "Descriptive Stats",
    "section": "Exclusion Bias",
    "text": "Exclusion Bias\n\n.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#data-collection-measurement-biases",
    "href": "materials/statistics/slides/session1.html#data-collection-measurement-biases",
    "title": "Descriptive Stats",
    "section": "Data Collection & Measurement Biases",
    "text": "Data Collection & Measurement Biases\nBiases at this stage affect how data is recorded and reported.\nEven with correct population coverage, poor measurement distorts reality.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#measurement-bias",
    "href": "materials/statistics/slides/session1.html#measurement-bias",
    "title": "Descriptive Stats",
    "section": "Measurement Bias",
    "text": "Measurement Bias\nArises from inaccurate tools or methods used to collect data.\nExample:\nA survey app records 0 when users skip a question instead of missing, misleading analysts.\n\nHow to Avoid?\n\nStandardize definitions and validation rules; treat missing explicitly.\n\nCalibrate and test instruments; run overlap periods when switching tools.\n\nInclude automated data-quality checks in ETL.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#recall-bias",
    "href": "materials/statistics/slides/session1.html#recall-bias",
    "title": "Descriptive Stats",
    "section": "Recall Bias",
    "text": "Recall Bias\nOccurs when participants don‚Äôt accurately remember past events.\nExample:\nRespondents under or over report store visits over the past month.\n\nHow to Avoid?\n\nShorten recall windows.\n\nUse diaries or passive behavioral data where possible.\n\nAsk concrete, bounded questions.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#response-bias",
    "href": "materials/statistics/slides/session1.html#response-bias",
    "title": "Descriptive Stats",
    "section": "Response Bias",
    "text": "Response Bias\nParticipants give socially desirable or expected answers rather than truthful ones.\nExample:\nCustomers rate satisfaction higher to appear polite.\nHow to Avoid?\n\n\nUse neutral wording and anonymity.\n\nPrefer behavioral measures over self-reports.\n\nInclude validity checks such as reverse-coded items.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#observer-bias",
    "href": "materials/statistics/slides/session1.html#observer-bias",
    "title": "Descriptive Stats",
    "section": "Observer Bias",
    "text": "Observer Bias\nA researcher‚Äôs expectations influence data collection or interpretation.\nExample:\nAn analyst expecting a new ad to perform well emphasizes positive feedback.\nHow to Avoid?\n\n\nBlind analysts to treatment where feasible.\n\nUse objective scoring rubrics and inter-rater reliability checks.\n\nAutomate extraction or labeling where appropriate.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#analysis-modeling-biases",
    "href": "materials/statistics/slides/session1.html#analysis-modeling-biases",
    "title": "Descriptive Stats",
    "section": "Analysis & Modeling Biases",
    "text": "Analysis & Modeling Biases\nBiases at this stage affect interpretation, reasoning, and model learning.\nThey often amplify earlier data issues.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#confirmation-bias-12",
    "href": "materials/statistics/slides/session1.html#confirmation-bias-12",
    "title": "Descriptive Stats",
    "section": "Confirmation Bias | 1/2",
    "text": "Confirmation Bias | 1/2\nTendency to favor data that confirms existing beliefs.\nExample:\nAnalyzing only high-discount months to prove discounts improve retention.\nHow to Avoid?\n\n\nDefine research questions and success criteria upfront.\n\nSeek disconfirming evidence.\n\nUse holdout periods and peer review.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#confirmation-bias-22",
    "href": "materials/statistics/slides/session1.html#confirmation-bias-22",
    "title": "Descriptive Stats",
    "section": "Confirmation Bias | 2/2",
    "text": "Confirmation Bias | 2/2\n\n.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#availability-bias",
    "href": "materials/statistics/slides/session1.html#availability-bias",
    "title": "Descriptive Stats",
    "section": "Availability Bias",
    "text": "Availability Bias\nRecent or vivid events are over-weighted in judgment.\nExample:\nOverestimating plane crash risk after extensive media coverage.\nHow to Avoid?\n\n\nUse base rates and long-term averages.\n\nPlace events in historical context.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#historical-bias",
    "href": "materials/statistics/slides/session1.html#historical-bias",
    "title": "Descriptive Stats",
    "section": "Historical Bias",
    "text": "Historical Bias\nOutdated or biased historical data perpetuates inequalities.\nExample:\nA credit model trained on biased historical lending decisions disadvantages certain groups.\nHow to Avoid?\n\nAudit legacy datasets for representation and proxies.\n\nRefresh training data and apply time-aware validation.\n\nMonitor subgroup performance.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#algorithmic-bias",
    "href": "materials/statistics/slides/session1.html#algorithmic-bias",
    "title": "Descriptive Stats",
    "section": "Algorithmic Bias",
    "text": "Algorithmic Bias\nAlgorithms learn or amplify biased patterns from data.\nExample:\nA hiring model trained on biased past decisions favors male applicants.\nHow to Avoid?\n\n\nRemove or regularize proxy features.\n\nEvaluate subgroup metrics and fairness constraints.\n\nRetrain using de-biased data or post-processing techniques.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#reporting-communication-biases",
    "href": "materials/statistics/slides/session1.html#reporting-communication-biases",
    "title": "Descriptive Stats",
    "section": "Reporting & Communication Biases",
    "text": "Reporting & Communication Biases\nBiases at this stage affect how insights are presented and interpreted.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#reporting-bias",
    "href": "materials/statistics/slides/session1.html#reporting-bias",
    "title": "Descriptive Stats",
    "section": "Reporting Bias",
    "text": "Reporting Bias\nSelective presentation of results that favor a narrative.\nExample:\nHighlighting CTR improvements while hiding declining customer satisfaction.\nHow to Avoid?\n\n\nPredefine reporting bundles with guardrail metrics.\n\nShow uncertainty and denominators.\n\nPublish full results or appendices.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session1.html#bias-summary",
    "href": "materials/statistics/slides/session1.html#bias-summary",
    "title": "Descriptive Stats",
    "section": "Bias Summary",
    "text": "Bias Summary\n\n\n\n\n\n\n\n\nBias Type\nKey Cause\nExample Context\n\n\n\n\nSelection Bias\nNon-random inclusion\nOnly counting email openers\n\n\nSampling Bias\nNon-representative sample\nUrban-only churn model\n\n\nSurvivorship Bias\nIgnoring failures\nStudying only successful campaigns\n\n\nExclusion Bias\nMissing variables\nOmitting device type\n\n\nMeasurement Bias\nFaulty data recording\n0 instead of missing\n\n\nRecall Bias\nMemory errors\nSelf-reported visits\n\n\nResponse Bias\nSocial desirability\nInflated satisfaction\n\n\nObserver Bias\nResearcher expectations\nSelective feedback\n\n\nConfirmation Bias\nFavoring expected results\nIgnoring non-discount data\n\n\nAvailability Bias\nRecency effects\nOverestimating rare risks\n\n\nHistorical Bias\nBiased legacy data\nCredit discrimination\n\n\nAlgorithmic Bias\nModel amplification\nGender bias in hiring\n\n\nReporting Bias\nSelective reporting\nHidden negative KPIs",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Descriptive Stats"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#why-data-types-matter",
    "href": "materials/statistics/slides/session2.html#why-data-types-matter",
    "title": "Data Types",
    "section": "Why Data Types Matter",
    "text": "Why Data Types Matter\nBefore calculating averages or plotting charts, it‚Äôs essential to recognize what kind of data you‚Äôre working with.\nThe classification of a variable determines:\n\nWhich summary statistics are meaningful?\nWhich visualizations can be used?\nHow relationships between variables should be interpreted?",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#qualitative-data",
    "href": "materials/statistics/slides/session2.html#qualitative-data",
    "title": "Data Types",
    "section": "Qualitative Data",
    "text": "Qualitative Data\nQualitative data describe qualities, categories, or labels rather than numbers.\n\n\n\n\n\n\n\n\nSubtype\nDefinition\nExamples\n\n\n\n\nNominal\nCategories with no natural order\nGender (Male/Female), City (Paris, Yerevan, Tokyo)\n\n\nOrdinal\nCategories with a meaningful order, but unequal spacing\nEducation Level (High &lt; Bachelor &lt; Master &lt; PhD), Satisfaction * (Low‚ÄìMedium‚ÄìHigh)*",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#quantitative-data",
    "href": "materials/statistics/slides/session2.html#quantitative-data",
    "title": "Data Types",
    "section": "Quantitative Data",
    "text": "Quantitative Data\nQuantitative data represent measurable quantities that can be used in arithmetic operations.\n\n\n\n\n\n\n\n\nSubtype\nDefinition\nExamples\n\n\n\n\nDiscrete\nCountable numbers, often integers\nNumber of customers, Complaints per day\n\n\nContinuous\nMeasured values within a range\nTemperature, Age, Revenue, Weight",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#additional-data-types",
    "href": "materials/statistics/slides/session2.html#additional-data-types",
    "title": "Data Types",
    "section": "Additional Data Types",
    "text": "Additional Data Types\n\n\n\n\n\n\n\n\nType\nDefinition\nExample Applications\n\n\n\n\nBinary\nTwo possible outcomes (Yes/No, 0/1)\nSubscription status, churn indicator\n\n\nTime-Series\nObservations recorded sequentially over time\nDaily sales, hourly temperature\n\n\nTextual / Unstructured\nWords, sentences, or documents\nCustomer reviews, tweets\n\n\nSpatial / Geographical\nLocation-based information\nStore coordinates, delivery zones",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#flowchart",
    "href": "materials/statistics/slides/session2.html#flowchart",
    "title": "Data Types",
    "section": "Flowchart",
    "text": "Flowchart\n\n\n\n\n\n%%{init: {\"theme\": \"default\", \"logLevel\": \"fatal\"}}%%\ngraph TD\n    A[\"Data Types\"] --&gt; B[\"Qualitative (Categorical)\"]\n    A --&gt; C[\"Quantitative (Numerical)\"]\n\n    B --&gt; D[\"Nominal\"]\n    B --&gt; E[\"Ordinal\"]\n\n    C --&gt; F[\"Discrete\"]\n    C --&gt; G[\"Continuous\"]\n\n    A --&gt; H[\"Other Types\"]\n    H --&gt; I[\"Binary\"]\n    H --&gt; J[\"Time-Series\"]\n    H --&gt; K[\"Textual / Unstructured\"]\n    H --&gt; L[\"Spatial / Geographical\"]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#mean",
    "href": "materials/statistics/slides/session2.html#mean",
    "title": "Data Types",
    "section": "Mean",
    "text": "Mean\nThe mean is the sum of all values divided by the number of observations.\n\n\n\nExample\nCalculation\n\n\n\n\nData: 5, 7, 8, 10\nMean = (5 + 7 + 8 + 10) / 4 = 7.5\n\n\n\nSample Mean\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\nPopulation Mean\n\n(same, only notation is different)\n\n\\[\n\\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#weighted-mean",
    "href": "materials/statistics/slides/session2.html#weighted-mean",
    "title": "Data Types",
    "section": "Weighted Mean",
    "text": "Weighted Mean\n\n\n\nType\nScore\nWeight (%)\n\n\n\n\nExam\n94\n50\n\n\nProject\n92\n35\n\n\nHomework\n100\n15\n\n\n\n(Weights do not need to add up to one!)\n\\[\n\\bar{x} =\n\\frac{\\displaystyle \\sum_{i=1}^{n} (w_i x_i)}\n     {\\displaystyle \\sum_{i=1}^{n} w_i}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#sample-mean-example",
    "href": "materials/statistics/slides/session2.html#sample-mean-example",
    "title": "Data Types",
    "section": "Sample Mean Example",
    "text": "Sample Mean Example\n\n\n\n\n\n\n\nWhen to Use?\n\n\n\nConveinent measurement clear to everybody\nWorks well with continuous or discrete numerical data.\n\nSensitive to outliers (extreme values can distort the result).\n\n\n\n\n\n\nGiven the 5 observations:\n\\[\n\\bar{x}\n= \\frac{\\sum_{i=1}^{5} x_i}{5}\n= \\frac{14.2 + 19.6 + 22.7 + 13.1 + 20.9}{5}\n= \\frac{90.5}{5}\n= 18.1 \\\n\\]\n\n\n\\[\n\\mu\n= \\frac{\\sum_{i=1}^{5} x_i}{5}\n= \\frac{300 + 320 + 270 + 210 + 8{,}000}{5}\n= \\frac{9{,}100}{5}\n= 1{,}820\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#median",
    "href": "materials/statistics/slides/session2.html#median",
    "title": "Data Types",
    "section": "Median",
    "text": "Median\nThe median is the value that separates the dataset into two equal halves.\nSteps to calculate:\n\nOrder the data from smallest to largest.\n\nIf the number of observations is odd ‚Üí midfdle value.\n\nIf even ‚Üí average of the two middle values.\n\n\n\n\nExample\nCalculation\n\n\n\n\nData: 2, 5, 7, 9, 12\nMedian = 7\n\n\nData: 3, 5, 8, 10\nMedian = (5 + 8)/2 = 6.5",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#mode",
    "href": "materials/statistics/slides/session2.html#mode",
    "title": "Data Types",
    "section": "Mode",
    "text": "Mode\nThe mode is the value that appears most often.\n\n\n\nExample\nCalculation\n\n\n\n\nData: 2, 3, 3, 4, 5, 5, 5, 7\nMode = 5\n\n\n\n\n\n\n\n\n\n\n\nWhen to Use?\n\n\n\nIdeal for categorical or discretfe data.\n\nA dataset can have:\n\nOne mode ‚Üí unimodal\n\nTwo modes ‚Üí bimodal\n\nMore than two ‚Üí multimodal",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#comparison-of-mean-median-and-mode",
    "href": "materials/statistics/slides/session2.html#comparison-of-mean-median-and-mode",
    "title": "Data Types",
    "section": "Comparison of Mean, Median, and Mode",
    "text": "Comparison of Mean, Median, and Mode\n\n\n\n\n\n\n\n\n\n\nMeasure\nBest For\nSensitive to Outliers?\nData Type\nExample Context\n\n\n\n\nMean\nSymmetrical distributions\nYes\nContinuous, Discrete\nAverage income\n\n\nMedian\nSkewed distributions\nNo\nContinuous\nTypical housing price\n\n\nMode\nCategorical / Repeated values\nNo\nNominal, Ordinal\nMost common product category",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#frequency-distributions",
    "href": "materials/statistics/slides/session2.html#frequency-distributions",
    "title": "Data Types",
    "section": "Frequency Distributions",
    "text": "Frequency Distributions\nVisual Insight:\n\nIn a perfectly symmetrical distribution: \\(Mean = Median = Mode\\)\n\nIn a right-skewed distribution: \\(Mean \\gt Median \\gt Mode\\)\n\nIn a left-skewed distribution: \\(Mean \\lt Median \\lt Mode\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#symmetric-distribution",
    "href": "materials/statistics/slides/session2.html#symmetric-distribution",
    "title": "Data Types",
    "section": "Symmetric Distribution",
    "text": "Symmetric Distribution\nMean ~ Median",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#left-skewed-distributions",
    "href": "materials/statistics/slides/session2.html#left-skewed-distributions",
    "title": "Data Types",
    "section": "Left Skewed Distributions",
    "text": "Left Skewed Distributions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#right-skewed-distributions",
    "href": "materials/statistics/slides/session2.html#right-skewed-distributions",
    "title": "Data Types",
    "section": "Right Skewed Distributions",
    "text": "Right Skewed Distributions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#symmetric-vs-left-skewed-vs-right-skewed",
    "href": "materials/statistics/slides/session2.html#symmetric-vs-left-skewed-vs-right-skewed",
    "title": "Data Types",
    "section": "Symmetric vs Left-Skewed vs Right-Skewed",
    "text": "Symmetric vs Left-Skewed vs Right-Skewed",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#better-visual",
    "href": "materials/statistics/slides/session2.html#better-visual",
    "title": "Data Types",
    "section": "Better Visual",
    "text": "Better Visual",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#measures-of-variability-1",
    "href": "materials/statistics/slides/session2.html#measures-of-variability-1",
    "title": "Data Types",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\nRange\nVariance\nStandard Deviation (SD)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#range",
    "href": "materials/statistics/slides/session2.html#range",
    "title": "Data Types",
    "section": "Range",
    "text": "Range\n\\[\n\\text{Range = Highest Value - Lowest Value}\n\\]\n\n\n\nThink about where is it applicable?\nAre there any limitations",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#variance",
    "href": "materials/statistics/slides/session2.html#variance",
    "title": "Data Types",
    "section": "Variance",
    "text": "Variance\nSample Variance:\n\\[\ns^{2} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^{2}}{n - 1}\n\\]\nPopulation Variance:\n\\[\n\\sigma^{2}\n=\n\\frac{\n\\displaystyle \\sum_{i=1}^{N} (x_i - \\mu)^{2}\n}{\nN\n}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#standard-deviation",
    "href": "materials/statistics/slides/session2.html#standard-deviation",
    "title": "Data Types",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nSample Standard Deviation (Standard Error)\n\\[\n\\displaystyle s=\\sqrt{s^2}\n\\]\nPopulation standard deviation\n\\[\n\\sigma\n=\n\\sqrt{\n\\frac{\n\\displaystyle \\sum_{i=1}^{N} (x_i - \\mu)^{2}\n}{\nN\n}\n}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#how-to-compare-variability",
    "href": "materials/statistics/slides/session2.html#how-to-compare-variability",
    "title": "Data Types",
    "section": "How to compare variability?",
    "text": "How to compare variability?\nIn which cases you can compare standard deviations?\n\n\n\nBox\nSample 1\nSample 2\n\n\n\n\nBox 1\n14.2\n18.2\n\n\nBox 2\n19.6\n17.9\n\n\nBox 3\n22.7\n18.1\n\n\nBox 4\n13.1\n18.1\n\n\nBox 5\n20.9\n18.2\n\n\nMean\n18.1\n18.1\n\n\nStandard deviation\n4.23\n0.12",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#visual-comparison-of-variability",
    "href": "materials/statistics/slides/session2.html#visual-comparison-of-variability",
    "title": "Data Types",
    "section": "Visual Comparison of Variability",
    "text": "Visual Comparison of Variability",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#when-variability-is-bad",
    "href": "materials/statistics/slides/session2.html#when-variability-is-bad",
    "title": "Data Types",
    "section": "When variability is bad?",
    "text": "When variability is bad?\n\nWhen consistency and quality control are important\n\nProduct weights, drug dosages, machine precision\n\nDelivery times, service response times\n\nFinancial risk (greater uncertainty)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#when-variability-is-good",
    "href": "materials/statistics/slides/session2.html#when-variability-is-good",
    "title": "Data Types",
    "section": "When variability is good?",
    "text": "When variability is good?\n\nBiological diversity and adaptability\n\nMarketing segmentation & A/B testing\n\nCreativity and innovation\n\nInvestments seeking higher upside\n\nIdentifying top performers (sports, hiring)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#when-variability-is-neutral",
    "href": "materials/statistics/slides/session2.html#when-variability-is-neutral",
    "title": "Data Types",
    "section": "When variability is neutral?",
    "text": "When variability is neutral?\n\nNatural randomness (weather, height, sampling variation)\n\n\n\n\n\n\n\n\nCaution\n\n\nVariability is not inherently good or bad as its value depends on what you are trying to achieve.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#why-n-1",
    "href": "materials/statistics/slides/session2.html#why-n-1",
    "title": "Data Types",
    "section": "Why \\(N-1\\)?",
    "text": "Why \\(N-1\\)?\nA degree of freedom (df) is an independent piece of information that can vary freely.\nWhenever we estimate a parameter from the sample, we introduce a constraint.\nEach constraint removes one degree of freedom.\nThe universal rule is:\n\\[\ndf = n - \\text{(number of estimated parameters)}.\n\\]\nDuring the mean estimation, since we have one constraint we remove only 1 hence\n\\[\ndf = n - 1\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#coefficient-of-variation-normalization",
    "href": "materials/statistics/slides/session2.html#coefficient-of-variation-normalization",
    "title": "Data Types",
    "section": "Coefficient of Variation (Normalization)",
    "text": "Coefficient of Variation (Normalization)\nThe coefficient of variation (CV) is a measure of relative variability.\nIt allows us to compare the variability of two datasets even when their units or scales differ.\n\n\n\nDate\nNike\nGoogle\n\n\n\n\nSeptember 14, 2012\n48.32\n709.68\n\n\nOctober 15, 2012\n47.81\n740.98\n\n\nNovember 15, 2012\n45.42\n647.26\n\n\nDecember 14, 2012\n48.46\n701.96\n\n\nJanuary 15, 2013\n53.64\n724.93\n\n\nFebruary 15, 2013\n54.95\n792.89\n\n\nMean\n49.77\n719.62\n\n\nStandard deviation\n3.70\n47.96\n\n\n\n\\[\n\\text{CV} = \\frac{s}{\\bar{x}} \\times 100\n\\]\nWhere:\n\n\\(s\\) = standard deviation\n\n\\(\\bar{x}\\) = mean\n\nCV is expressed as a percentage",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#z-score",
    "href": "materials/statistics/slides/session2.html#z-score",
    "title": "Data Types",
    "section": "Z-score",
    "text": "Z-score\nNumber of standard deviations that particular value is farther from the mean of its population or sample:\nPopulation:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nSample:\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\]\n=STANDARDIZE(x, mean, standard_deviation)\nPS. z-score also called a standard score.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#z-score-calculation",
    "href": "materials/statistics/slides/session2.html#z-score-calculation",
    "title": "Data Types",
    "section": "Z-score | Calculation",
    "text": "Z-score | Calculation\nSuppose we have:\n\nx = 540\nmean = 776.3\nstandard deviation = 385.1\n\n\\[z=\\frac{540-776.3}{385.1}=0.61\\]\n\n\nz-score &lt; 0: \\(\\rightarrow\\) bellow mean\nz-score = 0: \\(\\rightarrow\\) exactly on the mean\nz-score &gt; 0: \\(\\rightarrow\\) above the mean\n\nRule of Thumb for identifying outliers: Data values that have z-scores above +3 and below -3 can be categorized as outliers.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#empiracal-rule",
    "href": "materials/statistics/slides/session2.html#empiracal-rule",
    "title": "Data Types",
    "section": "Empiracal Rule",
    "text": "Empiracal Rule",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#empirical-rule-example-1",
    "href": "materials/statistics/slides/session2.html#empirical-rule-example-1",
    "title": "Data Types",
    "section": "Empirical Rule | Example 1",
    "text": "Empirical Rule | Example 1\nDaily step counts for a person over several months often form a normal-like distribution.\nAssuming:\n\nAverage steps per day: \\(\\mu = 8{,}000\\)\n\nStandard deviation: \\(\\sigma = 1{,}500\\)\n\nThen:\n\n68% of days fall within: \\([6500,\\ 9500]\\)\n95% of days fall within: \\([5000,\\ 11000]\\)\n99.7% of days fall within: \\([3500,\\ 12500]\\)\n\nFitness coaches use this rule to identify unusually inactive or exceptionally active days.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#empirical-rule-example-2",
    "href": "materials/statistics/slides/session2.html#empirical-rule-example-2",
    "title": "Data Types",
    "section": "Empirical Rule | Example 2",
    "text": "Empirical Rule | Example 2\nLarge standardized tests tend to be approximately normal.\nSuppose:\n\nMean SAT Math score: \\(\\mu = 520\\)\n\nStandard deviation: \\(\\sigma = 100\\)\n\nThen:\n\n68% score between: \\(520 \\pm 100 = [420,\\ 620]\\)\n95% score between: \\(520 \\pm 200 = [320,\\ 720]\\)\n99.7% score between: \\(520 \\pm 300 = [220,\\ 820]\\)\n\nAdmissions departments use this distribution to benchmark typical, strong, or exceptional performance.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#empirical-rule-summary",
    "href": "materials/statistics/slides/session2.html#empirical-rule-summary",
    "title": "Data Types",
    "section": "Empirical Rule | Summary",
    "text": "Empirical Rule | Summary\n\n\n\n\n\n\n\n\n\n\n\nScenario\nMean (\\(\\mu\\))\nStd (\\(\\sigma\\))\n68% Range\n95% Range\n99.7% Range\n\n\n\n\nBody Temperature (¬∞C)\n37¬∞C\n0.3¬∞C\n36.7‚Äì37.3¬∞C\n36.4‚Äì37.6¬∞C\n36.1‚Äì37.9¬∞C\n\n\nSAT Math Score\n520\n100\n420‚Äì620\n320‚Äì720\n220‚Äì820\n\n\nSteps per Day\n8000\n1500\n6500‚Äì9500\n5000‚Äì11000\n3500‚Äì12500\n\n\nRod Length (mm)\n100\n2\n98‚Äì102\n96‚Äì104\n94‚Äì106\n\n\nReaction Time (sec)\n0.25\n0.04\n0.21‚Äì0.29\n0.17‚Äì0.33\n0.13‚Äì0.37",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#chebyshevs-theorem",
    "href": "materials/statistics/slides/session2.html#chebyshevs-theorem",
    "title": "Data Types",
    "section": "Chebyshev‚Äôs Theorem",
    "text": "Chebyshev‚Äôs Theorem\nAny value of \\((z &gt; 1)\\), at least the following percentage of observations lie within ( z ) standard deviations of the mean:\n\\[\n\\left( 1 - \\frac{1}{z^2} \\right) \\times 100\n\\]\n\n\n\n\n\n\n\n\nZ-Score Band\nChebyshev Minimum\nInterpretation\n\n\n\n\n\\(|Z| &lt; 1.5\\)\n\\(1 - \\frac{1}{2.25} = 0.556 \\rightarrow 55.6\\%\\)\nAt least \\(55.6\\%\\) of data have z-scores between ‚Äì1.5 and +1.5\n\n\n\\(|Z| &lt; 2\\)\n\\(1 - \\frac{1}{4} = 0.75 \\rightarrow 75\\%\\)\nAt least \\(75\\%\\) fall within ‚Äì2 ‚â§ Z ‚â§ +2\n\n\n\\(|Z| &lt; 3\\)\n\\(1 - \\frac{1}{9} = 0.889 \\rightarrow 88.9\\%\\)\nAt least \\(88.9\\%\\) fall within ‚Äì3 ‚â§ Z ‚â§ +3\n\n\n\\(|Z| &lt; 4\\)\n\\(1 - \\frac{1}{16} = 0.9375\\rightarrow 93.75\\%\\)\nAt least \\(93.75\\%\\) fall within ‚Äì4 ‚â§ Z ‚â§ +4",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#chebyshev-vs-empirical-rule",
    "href": "materials/statistics/slides/session2.html#chebyshev-vs-empirical-rule",
    "title": "Data Types",
    "section": "Chebyshev vs Empirical Rule",
    "text": "Chebyshev vs Empirical Rule\n\n\n\nGoal\nNormal Data\nAny Data\n\n\n\n\nWant precise result\nUse z-scores + Empirical Rule\nNot valid\n\n\nWant a minimum guarantee?\nOptional\nUse z-scores + Chebyshev\n\n\nData is non symmetric?\nNot valid\nChebyshev works\n\n\nDistribution unknown?\nNot valid\nChebyshev works",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#relative-positions-percentiles",
    "href": "materials/statistics/slides/session2.html#relative-positions-percentiles",
    "title": "Data Types",
    "section": "Relative Positions | Percentiles",
    "text": "Relative Positions | Percentiles\nPercentiles describe where a value lies within a distribution.\n\nPercentile: percentage of values below a given value\n\nThe \\(p\\)-th percentile: at least \\(p\\) percent of observations lie below it\n\nMedian = 50th percentile\n\nDo not confuse percentiles with percentages",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#percentile-index-formula",
    "href": "materials/statistics/slides/session2.html#percentile-index-formula",
    "title": "Data Types",
    "section": "Percentile Index Formula",
    "text": "Percentile Index Formula\n\\[\ni = \\frac{p}{100}(n)\n\\]\nWhere:\n\n\\(p\\) = desired percentile\n\n\\(n\\) = number of observations\n\n\\(i\\) = index in the ordered dataset\n\n\n\nIf \\(i\\) is not a whole number ‚Üí round up\n\nIf \\(i\\) is a whole number ‚Üí take the midpoint of positions \\(i\\) and \\(i+1\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#percentile-rank",
    "href": "materials/statistics/slides/session2.html#percentile-rank",
    "title": "Data Types",
    "section": "Percentile Rank",
    "text": "Percentile Rank\n\\[\n\\text{Percentile Rank} =\n\\frac{\\text{Number of values below } x + 0.5}{\\text{Total number of values}} \\times 100\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#percentile-rank-example",
    "href": "materials/statistics/slides/session2.html#percentile-rank-example",
    "title": "Data Types",
    "section": "Percentile Rank Example",
    "text": "Percentile Rank Example\nThere are 14 scores below John.\n\\[\n\\frac{14 + 0.5}{16} \\times 100 = 90.625\n\\]\n\n\nJohn is in the 91st percentile\n\nHe scored better than 91% of individuals",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#ordered-dataset-example",
    "href": "materials/statistics/slides/session2.html#ordered-dataset-example",
    "title": "Data Types",
    "section": "Ordered Dataset Example",
    "text": "Ordered Dataset Example\n2, 4, 5, 7, 8, 9, 10, 12, 14, 15, 18, 21\n\\(n = 12\\)\n\nFind \\(P_{25}\\)\n\nFind \\(P_{50}\\)\n\nFind \\(P_{75}\\)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#finding-the-25th-percentile-p_25",
    "href": "materials/statistics/slides/session2.html#finding-the-25th-percentile-p_25",
    "title": "Data Types",
    "section": "Finding the 25th Percentile (\\(P_{25}\\))",
    "text": "Finding the 25th Percentile (\\(P_{25}\\))\n\\[\ni = \\frac{25}{100}(12) = 3\n\\]\n\n3rd value = 5\n\n4th value = 7\n\n\\[\nP_{25} = \\frac{5 + 7}{2} = 6\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#finding-the-median-p_50",
    "href": "materials/statistics/slides/session2.html#finding-the-median-p_50",
    "title": "Data Types",
    "section": "Finding the Median (\\(P_{50}\\))",
    "text": "Finding the Median (\\(P_{50}\\))\n\\[\ni = \\frac{50}{100}(12) = 6\n\\]\n\n6th value = 9\n\n7th value = 10\n\n\\[\nP_{50} = \\frac{9 + 10}{2} = 10\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#finding-the-75th-percentile-p_75",
    "href": "materials/statistics/slides/session2.html#finding-the-75th-percentile-p_75",
    "title": "Data Types",
    "section": "Finding the 75th Percentile (\\(P_{75}\\))",
    "text": "Finding the 75th Percentile (\\(P_{75}\\))\n\\[\ni = \\frac{75}{100}(12) = 9\n\\]\n\n9th value = 14\n\n10th value = 18\n\n\\[\nP_{75} = \\frac{14 + 18}{2} = 16\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#quartiles",
    "href": "materials/statistics/slides/session2.html#quartiles",
    "title": "Data Types",
    "section": "Quartiles",
    "text": "Quartiles\nQuartiles divide data into four equal parts.\n\n\\(Q_1\\) = 25th percentile\n\n\\(Q_2\\) = 50th percentile (median)\n\n\\(Q_3\\) = 75th percentile",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#interquartile-range-iqr",
    "href": "materials/statistics/slides/session2.html#interquartile-range-iqr",
    "title": "Data Types",
    "section": "Interquartile Range (IQR)",
    "text": "Interquartile Range (IQR)\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\]\n\nMeasures spread of the middle 50%\n\nRobust to outliers",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#boxplot-example",
    "href": "materials/statistics/slides/session2.html#boxplot-example",
    "title": "Data Types",
    "section": "Boxplot Example",
    "text": "Boxplot Example",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#measures-of-association-1",
    "href": "materials/statistics/slides/session2.html#measures-of-association-1",
    "title": "Data Types",
    "section": "Measures of Association",
    "text": "Measures of Association\nMeasures of association describe how two variables move together.\n\nDirection\n\nStrength\n\nLinearity",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#scatter-plot-example",
    "href": "materials/statistics/slides/session2.html#scatter-plot-example",
    "title": "Data Types",
    "section": "Scatter Plot Example",
    "text": "Scatter Plot Example",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#covariance",
    "href": "materials/statistics/slides/session2.html#covariance",
    "title": "Data Types",
    "section": "Covariance",
    "text": "Covariance\n\nIndicates direction of linear relationship\n\nPositive ‚Üí move together\n\nNegative ‚Üí move oppositely\n\nMagnitude is not interpretable\n\n\\[\ns_{xy} =\n\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#correlation",
    "href": "materials/statistics/slides/session2.html#correlation",
    "title": "Data Types",
    "section": "Correlation",
    "text": "Correlation\nCorrelation standardizes covariance.\n\\[\nr_{xy} = \\frac{s_{xy}}{s_x s_y}\n\\]\n\nRange: \\([-1, 1]\\)\n\nShows direction and strength\n\n\\(r = 0\\) ‚Üí no linear relationship",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#correlation-interpretation",
    "href": "materials/statistics/slides/session2.html#correlation-interpretation",
    "title": "Data Types",
    "section": "Correlation Interpretation",
    "text": "Correlation Interpretation\n\n\\(r &gt; 0\\) ‚Üí positive relationship\n\n\\(r &lt; 0\\) ‚Üí negative relationship\n\n\\(|r|\\) close to 1 ‚Üí strong relationship",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#correlation-vs-causation",
    "href": "materials/statistics/slides/session2.html#correlation-vs-causation",
    "title": "Data Types",
    "section": "Correlation vs Causation",
    "text": "Correlation vs Causation\n\nCorrelation does not imply causation\n\nPossible explanations:\n\nReverse causality\n\nCommon causes\n\nSelection bias",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#ice-cream-and-drowning-example",
    "href": "materials/statistics/slides/session2.html#ice-cream-and-drowning-example",
    "title": "Data Types",
    "section": "Ice Cream and Drowning Example",
    "text": "Ice Cream and Drowning Example\n\nIce cream sales rise in summer\n\nDrowning incidents rise in summer\n\nCommon cause:\n\n\\[\\text{hot weather}\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#firefighters-and-fire-damage",
    "href": "materials/statistics/slides/session2.html#firefighters-and-fire-damage",
    "title": "Data Types",
    "section": "Firefighters and Fire Damage",
    "text": "Firefighters and Fire Damage\n\nLarge fires ‚Üí more firefighters\n\nLarge fires ‚Üí more damage\n\nCommon cause:\n\n\\[\\text{fire severity}\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#salary-and-coffee-consumption",
    "href": "materials/statistics/slides/session2.html#salary-and-coffee-consumption",
    "title": "Data Types",
    "section": "Salary and Coffee Consumption",
    "text": "Salary and Coffee Consumption\n\nHigher salary ‚Üí more coffee consumption\n\nCoffee does not cause higher pay\n\nCommon cause:\n\n\\[\\text{job seniority}\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/slides/session2.html#shoe-size-and-reading-scores",
    "href": "materials/statistics/slides/session2.html#shoe-size-and-reading-scores",
    "title": "Data Types",
    "section": "Shoe Size and Reading Scores",
    "text": "Shoe Size and Reading Scores\n\nLarger shoe size ‚Üí higher reading score\n\nShoe size does not improve reading\n\nCommon cause:\n\n\\[\\text{age}\\]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Slides",
      "Data Types"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html",
    "href": "materials/statistics/session5.html",
    "title": "Statistics Session 05: Sampling",
    "section": "",
    "text": "Sampling\nConfidence Intervals\nHypothesis Testing\nStorytelling With Data",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html#agenda",
    "href": "materials/statistics/session5.html#agenda",
    "title": "Statistics Session 05: Sampling",
    "section": "",
    "text": "Sampling\nConfidence Intervals\nHypothesis Testing\nStorytelling With Data",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html#sampling",
    "href": "materials/statistics/session5.html#sampling",
    "title": "Statistics Session 05: Sampling",
    "section": "Sampling",
    "text": "Sampling\n\nPopulation vs.¬†Sample\nIn statistics:\n\na population is the full group we care about (all customers, all transactions, all students, all calls).\n\na sample is a smaller, representative portion of that population.\n\nFor example, when a market research company measures TV viewership, the population is every viewer, but the sample is only the households that have a Nielsen people-meter installed.\n\nThe goal is to learn something about everyone by studying just the selected few.\n\n\n\n\nWhy Not Measure Everyone?\nAt first glance, it may seem logical to ‚Äúmeasure everything‚Äù But in reality, studying an entire population is often:\n\nToo expensive\n\nToo time-consuming\n\nPhysically impossible\n\nDamaging to the items being measured (quality control settings)\n\n\nImagine you want to understand student attitudes toward academic integrity at ACA.\nYou could send a survey to all students, but:\n\nNot everyone will respond.\n\nCollecting, cleaning, and processing thousands of responses would take weeks.\n\nIn many cases, a properly selected sample (e.g.,15‚Äì25% of students) is enough to accurately represent the entire student body.\n\nA similar real-world example from the textbook shows how a college committee surveyed only a subset of students yet confidently inferred the attitudes of the whole institution.\n\n\n\nWhy Businesses Prefer Sampling?\nSampling allows organizations to:\n\nSave money and time\n\nCheck quality without damaging products\n\nLaunch decisions faster\n\nWork with continuous data streams when populations grow too fast\n\nAvoid unnecessary measurements when additional data adds little value\n\nIn telecom, banking, or marketing environments‚Äîlike you often cannot measure every interaction or manually label every event. Sampling helps you make reliable conclusions with controlled effort.\n\n\n\nThe Risks of Sampling\nSampling introduces uncertainty. Estimates may deviate from the true population values.\nHowever:\n\nStatistics gives us tools to quantify this risk\n\nMany sampling errors are small and predictable\n\nWell-designed sampling keeps accuracy high while effort stays low\n\n\nSampling is not a shortcut. It is a scientifically grounded method for making reliable decisions when full measurement is unnecessary, impractical, or impossible.\nIn both academic settings and real business operations, sampling enables accurate, fast, and cost-effective insights.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html#types-of-sampling",
    "href": "materials/statistics/session5.html#types-of-sampling",
    "title": "Statistics Session 05: Sampling",
    "section": "Types of Sampling",
    "text": "Types of Sampling\nThere are many options available for gathering samples from a population. The two basic types that we will discuss are:\n\nprobability sampling\nnonprobability sampling\n\n\nProbability Sampling\nA probability sample is a sample in which each member of the population has a known, nonzero, chance of being selected for the sample.\nThere are five main types of probability sampling techniques statisticians use:\n\nsimple random\nsystematic\nstratified\ncluster\nresampling\n\n\nSimple Random Sampling\nA simple random sample is a sample in which every member of the population has an equal chance of being chosen.\n\n\n\nSystematic Sampling\nIn systematic sampling, every k-th member of the population is chosen for the sample. The value of k is determined by dividing the size of the population (N) by the size of the sample (n).\nBut how do you know whcih value of k to choose? The value of k is the systematic sampling constant:\n\\[k = \\frac{N}{n}\\]\nwhere:\n\nN: Size of the population\nn: Size of the sample\n\nUsing my academic integrity survey, with a population of 1,800 students and a sample of 10:\n\\[k = \\frac{N}{n} = \\frac{1,800}{10}=180\\]\nFrom a listing of the entire population, I would choose every 180th student to be included in the sample.\n\nSystematic sampling is often easier to implement than a simple random sample.\n\nProblem\nA problem can arise if the population contains a repeating pattern that aligns with the interval (k). This periodicity can produce a biased sample.\nExample: selecting data every 4th week in an 8-week term may always fall on midterms or finals, leading to inflated study-hour data.\nSolution\nThe first selected unit does not have to be the first entry.We can randomly choose a starting position and then pick every (k)-th observation after that.\n\n\nStratified Sampling\nStratified sampling is used when specific subgroups within a population differ in ways that are important to the analysis. If these subgroups are not represented proportionally in the sample, the results may become biased‚Äîeven when random sampling procedures are used.\nConsider a cybersecurity compliance study within a large technology company. Previous internal audits show that senior engineers tend to follow security protocols far more consistently than junior engineers. If we rely on simple random or systematic sampling, it is possible‚Äîsimply by chance‚Äîthat 45% of the selected employees will be senior engineers, even though they represent only 15% of the workforce. Such a sample would lead us to overestimate overall compliance levels.\nStratified sampling prevents this type of distortion. The population is divided into meaningful groups (such as junior, mid-level, senior, and management), and each group is sampled in proportion to its actual size. This ensures that the final sample mirrors the true composition of the population and that no subgroup with unusually high or low compliance influences the results disproportionately.\nIn contexts such as academic integrity studies, organizational culture assessments, customer-segmentation research, or ACA-style analytics projects, stratified sampling provides a more balanced, representative, and trustworthy basis for drawing conclusions.\n\n\n\n\n\n\n\n\n\n\nStratum (Role Level)\nPopulation Size\nPopulation %\nSample Size (n = 200)\nCalculation\n\n\n\n\nJunior Engineers\n1,200\n40%\n80\n\\(0.40 \\times 200\\)\n\n\nMid-Level Engineers\n900\n30%\n60\n\\(0.30 \\times 200\\)\n\n\nSenior Engineers\n450\n15%\n30\n\\(0.15 \\times 200\\)\n\n\nManagement\n450\n15%\n30\n\\(0.15 \\times 200\\)\n\n\nTotal\n3,000\n100%\n200\n‚Äî\n\n\n\n\n\n\nCluster Sampling\nCluster sampling divides the population into mutually exclusive groups‚Äîcalled clusters‚Äî where each cluster is intended to be a small-scale representation of the entire population. Instead of sampling individuals directly, we randomly select entire clusters and then either survey every member of those clusters or take a simple random sample within them.\nClusters are often based on geography or natural groupings to simplify data collection. For example, in an academic integrity study, classrooms at specific times of day could serve as clusters. Once a set of classrooms is randomly selected, all students in those rooms (or a sample of them) can be surveyed.\nA common source of confusion is the difference between strata and clusters:\n\nStrata are groups whose members share a common characteristic (e.g., all freshmen). Strata are typically homogeneous within each group and are used to ensure proper representation in the sample.\nClusters, by contrast, are ‚Äúmini-populations‚Äù‚Äîoften heterogeneous mixtures of different types of individuals (e.g., a classroom containing freshmen, sophomores, juniors, and seniors). The goal is convenience and efficiency, not homogeneity.\n\nCluster sampling is widely used in business settings, especially when test-marketing new products. Companies select cities or regions as clusters and gather feedback from customers within those areas. When designed carefully, cluster sampling provides a cost-effective way to collect a probability sample while maintaining representativeness.\n\n\n\nResampling (Bootstrap)\nResampling is a statistical approach in which multiple samples are repeatedly drawn from an observed dataset to better understand the variability of a statistic. One of the most widely used resampling techniques is the bootstrap method, introduced by Bradley Efron at Stanford University.\nBootstrapping offers a practical, assumption-light method for estimating the uncertainty around statistical measures.\nBootstrapping works by drawing many samples with replacement from the original dataset. For each resample, we compute a statistic of interest‚Äîsuch as a mean, median, proportion, or variance. By repeating this process thousands of times, we obtain a distribution of the statistic, which helps us understand how much it varies and where the true population value is likely to fall.\nBootstrapping is flexible and powerful. Unlike many traditional statistical methods, it does not require strong assumptions about the underlying population distribution. It can be applied to estimate population parameters, standard errors, and measures of uncertainty for a wide variety of statistics.\n\nWebsite Conversion Rates (Digital Marketing)\nA marketing team measures the conversion rate of a landing page based on 600 user sessions. They use bootstrapping to repeatedly resample these outcomes and estimate the distribution of the conversion rate. This helps them compare campaign performance confidently without relying on theoretical formulas.\n\n\nModel Performance Metrics (Machine Learning)\nA data science team evaluates a predictive model using a dataset of 10,000 labeled observations. They apply bootstrapping to estimate the variability of accuracy, recall, or AUC scores. This produces more reliable estimates of model performance under different resampling conditions.\n\n\n\n\n\nSimple Random vs Systematic vs Stratified vs Cluster\n\n\n\n\nNonprobability Sampling\nA nonprobability sample is a sample in which the probability of a population member being selected for the sample is not known.\nA common type of nonprobability sample is a convenience sample. This type of sampling is useful when you are simply trying to gather some general information about the population.\n\nConvenience sample: A convenience sample is used when members of the population are chosen to become part of the sample simply because they are easily accessible.\nInternet Poll: Most individuals who participate in Internet polls have very little statistical training. They are impressed when they see thousands of responses recorded in real time and tend to believe the results.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html#sampling-and-nonsampling-errors",
    "href": "materials/statistics/session5.html#sampling-and-nonsampling-errors",
    "title": "Statistics Session 05: Sampling",
    "section": "Sampling and Nonsampling Errors",
    "text": "Sampling and Nonsampling Errors\nWhen we rely on a sample instead of measuring an entire population, some level of error is unavoidable. Population values‚Äîknown as parameters‚Äîare typically unknown, so we use statistics from our sample to estimate them. Because a sample represents only part of the population, the statistic will almost never match the parameter exactly. This difference is called sampling error, and it reflects the natural variability that occurs in any sampling process.\n\\[\\text{Sampling Error}=\\bar{x}-\\mu\\]\nwhere\n\n\\(\\bar{x}:\\) sample mean\n\\(\\mu:\\) population mean\n\nSampling error is expected and tends to decrease when the sample size increases. However, data collection can also be affected by nonsampling errors, which arise from factors unrelated to the sampling procedure. These include biased survey questions, measurement mistakes, data entry errors, or low response rates. Unlike sampling errors, nonsampling errors cannot be reduced simply by collecting more data.\n\nRecognizing the difference between sampling and nonsampling errors is essential for evaluating how accurate and reliable a study‚Äôs conclusions are.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html#sampling-distribution",
    "href": "materials/statistics/session5.html#sampling-distribution",
    "title": "Statistics Session 05: Sampling",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\nDistirbution of the Mean with Finite Population\nUp to this point, we have assumed that samples are drawn from a very large or effectively infinite population. In such cases, the sample represents only a tiny fraction of the population, so the usual standard error formula is appropriate.\nHowever, when the population is finite and the sample makes up a noticeable portion of that population, the standard error of the mean must be adjusted. This situation arises when the ratio of sample size to population size: \\(n/N \\gt 5\\).\nIn these cases, we use the finite population correction factor (FPC) to account for the fact that sampling without replacement reduces variability. The corrected standard error becomes smaller because the sample is capturing a larger portion of the population.\nFor example, if a population contains 100 individuals, and we sample more than 5 of them (i.e., \\(n &gt; 5\\)), the finite population correction factor should be applied to compute an accurate standard error.\nIgnoring the correction would overestimate the sampling error and lead to incorrect probability calculations.\n\\[\n\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\sqrt{\\frac{N - n}{N - 1}}\n\\]\nwhere:\n\n\\(\\sigma:\\) Population standard deviation\n\\(N:\\) Population size\n\\(n:\\) Sample size\n\\(\\frac{N-n}{N-1}:\\) finite population correction factor.\n\nThis adjustment ensures that probability calculations and confidence intervals accurately reflect the reduced sampling variability when the sample is a sizable proportion of the population.\nExample\nConsider a consulting firm serving 120 clients. Historically, the average satisfaction rating is 7.3. A recent survey of 50 clients shows an average of 7.8.\nBecause the sample is more than 5% of the population, the finite population correction must be applied:\n\nThe sample represents a large share of all clients.\nSampling without replacement reduces variability.\nUsing the FPC provides a more accurate standard error.\nA low resulting probability suggests the true population mean has improved.\n\nThis type of adjustment is essential in practical business settings:\n\nsmall customer lists\ninternal employee populations\nspecialized cohorts\n\nEffect of the Correction Factor\nThe following table illustrates how the standard error changes when the population size is 100 and the sample size increases. As \\(n\\) grows, the correction factor becomes smaller, reducing the standard error significantly.\n\n\n\n\n\n\n\n\n\nSample Size (n)\nStandard Error\nFinite Correction Factor\nStandard Error with FPC\n\n\n\n\n40\n0.111\n0.778\n0.086\n\n\n60\n0.090\n0.636\n0.057\n\n\n80\n0.078\n0.449\n0.035\n\n\n100\n0.070\n0\n0\n\n\n\nAs shown, when the sample covers the entire population (\\(n = 100\\)), the corrected standard error becomes zero, as no sampling error exists in a census.\n\n\nDistribution of the Proportion\nIn many real-world applications, we analyze proportions rather than means.\nFor example the % of:\n\ncustomers who churn\nvoters who support a candidate\nhouseholds that watch a specific media event.\n\nIn these situations, the statistic of interest is the sample proportion, and its behavior across repeated samples is described by the sampling distribution of the proportion.\nSuppose a national streaming analytics firm claims that 52% of U.S. households watched a large live sports event in 2024. A media agency wants to verify the claim and surveys 300 randomly selected households. In the sample, 147 households watched the event.\nTo evaluate whether this sample supports the firm‚Äôs claim, we check whether the sample size is large enough to use the normal approximation to the binomial distribution.\nThe following conditions must hold:\n\\[\nnp \\ge 5\n\\]\n\\[\nn(1 - p) \\ge 5\n\\]\nUsing the claimed proportion \\(p = 0.52\\) and sample size \\(n = 300\\):\n\\[\nnp = 300(0.52) = 156 \\ge 5\n\\]\n\\[\nn(1 - p) = 300(0.48) = 144 \\ge 5\n\\]\nThus, the normal approximation is appropriate.\n\n\nSample Proportion\nThe sample proportion is:\n\\[\n\\hat{p} = \\frac{x}{n}\n\\]\nFor this survey:\n\\[\n\\hat{p} = \\frac{147}{300} = 0.49\n\\]\n\nStandard Error of the Proportion\nThe standard error reflects how much sample proportions vary across repeated samples:\n\\[\n\\sigma_{\\hat{p}} = \\sqrt{ \\frac{p(1 - p)}{n} }\n\\]\n\n\nz-Score for the Sample Proportion\nTo evaluate how far the sample proportion is from the claimed population proportion:\n\\[\nz_{\\hat{p}} = \\frac{\\hat{p} - p}{\\sigma_{\\hat{p}}}\n\\]\nA large positive or negative value indicates that the observed sample proportion would be unlikely if the claimed population proportion were correct.\nThe sampling distribution of the proportion allows us to determine how probable it is to observe a sample proportion like the one in our study. If the probability is small, the sample provides evidence against the population claim. If the probability is reasonable, the sample is consistent with the claim.\nIn this example, the observed sample proportion (0.49) is close to the claimed population proportion (0.52), and with a moderate sample size, the difference is well within the range of expected sampling variability.\n\n\n\n\n\n\n\n\nflowchart TD\nA[\"Population with true proportion p\"] --&gt; B[\"Select a random sample\"]\nB --&gt; C[\"Count successes in the sample\"]\nC --&gt; D[\"Compute the sample proportion\"]\nD --&gt; E[\"Repeat the sampling process\"]\nE --&gt; F[\"Distribution of sample proportions\"]\nF --&gt; G[\"Shape: approximately normal\"]\nF --&gt; H[\"Center equals p\"]\nF --&gt; I[\"Spread depends on p and n\"]",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session5.html#confidence-intervals",
    "href": "materials/statistics/session5.html#confidence-intervals",
    "title": "Statistics Session 05: Sampling",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nOne of the most important roles statistics plays in today‚Äôs world is to:\n\ngather information from a sample,\nuse that information to make a statement about the population from which the sample was chosen.\n\n\nUnderstanding confidence intervals helps quantify how certain we are about the population parameter based on sample data.\n\n\nPoint Estimates\nA point estimate is a single value that best describes the population parameter of interest.\nThe most common point estimates are:\n\nthe sample mean (\\(\\bar{x}\\))\nthe sample proportion (\\(\\hat{p}\\))\n\n\nPoint estimates are easy to calculate, but they provide no information about the accuracy or uncertainty of the estimate. They simply give one number a ‚Äúbest guess‚Äù of the true population value.\n\n\n\n\n\n\n\nNoteConfidence Interval\n\n\n\nA confidence interval for the mean is an interval estimate around a sample mean that provides a range within which the true population mean is expected to lie.\n\n\n\n\n\n\n\n\nNoteConfidence Level\n\n\n\nA confidence level is the probability that the interval constructed from sample data will contain the population parameter of interest.\n\n\nThe purpose of generating a confidence interval is to provide an estimate for the true population mean by combining:\n\nthe sample mean \\(\\bar{x}\\)\n\nthe critical \\(z\\)-value\n\nthe standard error \\(\\sigma_{\\bar{x}}\\)\n\nA 90% CL means that 90% of such intervals‚Äîconstructed from repeated samples would contain the true population mean.\nTypically, confidence levels are set by the statistician at 90% or 95% and will occasionally go as high as 99%.\n\n\n\n\n\n\n\n\n\n\nConfidence Level\nŒ± (Significance Level)\nŒ±/2 (Each Tail)\nLower zŒ±/2\nUpper zŒ±/2\n\n\n\n\n90%\n0.10\n0.05\n-1.645\n1.645\n\n\n95%\n0.05\n0.025\n-1.960\n1.960\n\n\n99%\n0.01\n0.005\n-2.576\n2.576\n\n\n\n\n\nConfidence Intervals for the Mean | known SD\n\nLet‚Äôs say we want to construct a confidence interval for the average order size of the ABC customer based on my sample mean of $129.20 with a 90% confidence level. To determine this interval, we need two more pieces of information:\n\nthe sample size: \\(\\bar{x}\\)\n\nthe population standard deviation \\(\\sigma\\)\n\nSuppose the sample mean was based on the 32 orders (\\(n=32\\)). Also, we‚Äôll assume the population standard deviation is equal to \\(\\sigma=40.602\\)\n\n\n\n\n\n\nNoteAbout known Standard Deviation\n\n\n\nUsually domain experts are familiar with the value of SD\n\n\nStandard Error (SE):\n\\[\n\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} = \\frac{\\$ 40.602}{\\sqrt{32}} = \\$7.173\n\\]\nMargin of Error (MoE):\nA margin of error represents the width of the confidence interval between a sample mean and its upper limit or between a sample mean and its lower limit. Notice that the confidence interval is symmetrical around the sample mean.\n\\[MoE_{\\bar{x}} = z_{\\frac{\\alpha}{2}}\\sigma_{\\bar{x}}\\]\nConfidence Levels:\n\n\n\\[UCL = \\bar{x} + MoE\\]\n\n\\[LCL = \\bar{x} - MoE\\]\n\n\n\\[ \\Downarrow \\]\n\\[MoE_{90} = 1.645 \\cdot 7.176 = 11.80\\]\n\n\n\\[UCL = 129.2 + 11.8 = 141\\]\n\n\\[LCL = 129.2 - 11.8 = 117\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nCL\n\\(z_{Œ±/2}\\)\nSE\nMoE\nLCL\nUCL\n\n\n\n\n90%\n1.645\n7.173\n11.80\n117\n141\n\n\n95%\n1.960\n7.173\n14.06\n115\n143\n\n\n99%\n2.576\n7.174\n18.47\n110\n147\n\n\n\n\nA 99% confidence interval is even wider than the 95% and 90% intervals because a greater level of confidence requires covering a larger portion of the sampling distribution.\n\n\nSample Means and 90% Confidence Intervals\nA confidence interval does not guarantee that the true population mean will fall inside the interval calculated from a single sample. In the example, Sample 1 produced a 90% confidence interval that correctly captured the true population mean of 125. However, Sample 5 produced an interval that entirely missed the true mean.\nThis outcome is expected. A 90% confidence interval means that the method, when applied repeatedly across many random samples of the same size, will produce intervals that contain the population mean about 90% of the time. It does not mean that any individual interval has a 90% chance of containing the true value, nor does it guarantee that 9 out of 10 intervals from a small set of samples will succeed.\n\n\n\n\n\n\n\n\n\n\nSample\nSample Mean\nMargin of Error\nLower Limit\nUpper Limit\n\n\n\n\n1\n129.20\n11.80\n117.40\n141.00\n\n\n2\n132.00\n11.80\n120.20\n143.80\n\n\n3\n117.50\n11.80\n105.70\n129.30\n\n\n4\n128.20\n11.80\n116.40\n140.00\n\n\n5\n108.80\n11.80\n97.00\n120.60\n\n\n6\n130.10\n11.80\n118.30\n141.90\n\n\n7\n117.90\n11.80\n106.10\n129.70\n\n\n8\n120.10\n11.80\n108.30\n131.90\n\n\n9\n133.80\n11.80\n122.00\n145.60\n\n\n10\n119.00\n11.80\n107.20\n130.80\n\n\n\nThe key idea: confidence intervals describe the long-run performance of the estimation procedure, not certainty about the result of a single sample.\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals for the Mean | Unknown SD\nSo far our Confidence Interval examples have assumed that the population standard deviation (\\(\\sigma\\)), is known. In practice, this is rarely the case. Most of the time, we only have access to the sample itself, and we must estimate the population standard deviation using the sample standard deviation, \\(s\\).\nThe sample standard deviation can always be computed directly from the data:\n\\[\ns = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n - 1}}\n\\]\nSince \\(s\\) is based on the sample, it is only an estimate of the true population standard deviation. As a result, when \\(\\sigma\\) is unknown, we cannot rely on the normal distribution to calculate a confidence interval for the mean.\nInstead, we use the Student‚Äôs t-distribution, which adjusts for the uncertainty created by estimating \\(\\sigma\\) with \\(s\\).\nThe t-distribution behaves like the normal distribution when sample sizes are large, but it has heavier tails for smaller sample sizes. This accounts for the additional variability that comes from estimating the standard deviation. As the sample size increases, the t-distribution approaches the standard normal distribution.\nThe overall structure of the Confidence Interval formula remains the same:\n\n\\(\\sigma \\Rightarrow s\\)\n\nnormal distribution \\(\\Rightarrow\\) t-distribution\nz-table \\(\\Rightarrow\\) t-tabe\n\nThis adjustment allows us to construct valid confidence intervals in the realistic case where the population standard deviation is not available.\n\n\n\n\n\n\nNoteInteresting fact about T-distribution\n\n\n\nPlease watch this video.\n\n\n\nStudent‚Äôs t-distribution\n\nA bell-shaped and symmetric, similar to the normal distribution.\nIts exact shape depends on the degrees of freedom, defined as \\(n - 1\\) for a sample of size \\(n\\).\nThe total area under the curve is 1.0, just like any valid probability distribution.\nBecause it is wider and flatter than the normal distribution, the critical values from the t-distribution are larger than z-critical values for the same confidence level. This leads to wider confidence intervals, reflecting the extra uncertainty created by estimating the population standard deviation with the sample standard deviation.\nThe t-distribution is actually a family of distributions one for each degree of freedom. As the degrees of freedom increase, the t-distribution becomes more similar to the normal distribution. With more than 100 degrees of freedom, the two are practically identical.\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Confidence Intervals\nAs it is mentioned above, the calculation methodology is the same:\n\nCalculate Sample Mean\nCalcualate the Standard Error\nCalculate the Margin of Error\nCaluclate the Confidence Inervals\n\n\nNote the Confidence Level/Significance Level (\\(\\alpha\\)) must be given. Let‚Äôs assume that the \\(\\alpha = 0.05\\)\n\n\nWeekly Visitors\nThe shop owner expects to have more than 90 visiters per week for the sustainable develepment.\n\n\n\nWeek\nVisitors\n\n\n\n\n1\n116\n\n\n2\n83\n\n\n3\n89\n\n\n4\n87\n\n\n5\n81\n\n\n6\n109\n\n\n7\n114\n\n\n8\n123\n\n\n9\n102\n\n\n10\n131\n\n\n11\n96\n\n\n12\n74\n\n\n13\n109\n\n\n14\n106\n\n\n15\n118\n\n\n16\n78\n\n\n17\n91\n\n\n18\n98\n\n\n\nIn order to calculate the sample mean and the sample standard deviation we can use excel:\n\nDF: COUNT(B2:B19)-1 = 17\nMean: AVERAGE(B2:B19) = 100.3\nStandard Deviation | Sample: STDEV.S(B2:B19) = 16.6\nCritical t-score: T.INV.2T(alpha, df) = T.INV.2T(0.05,17) = 2.11\nMargin of Error: CONFIDENCE.T(alpha, sd,n) = CONFIDENCE.T(0.05, 16.6,18) = 8.25\n\nMargin of Error\n\\[\n\\hat{\\sigma}_{\\bar{x}} = \\frac{s}{\\sqrt{n}} = 3.92\n\\]\nConfidence Levels:\n\n\n\\[UCL = \\bar{x} + MoE\\]\n\n\\[LCL = \\bar{x} - MoE\\]\n\n\n\\[ \\Downarrow \\]\n\n\n\\[UCL = 100.3 + 8.25 = 108.55\\]\n\n\\[LCL = 100.3 - 8.25 = 92.05\\]\n\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\nBased on the result we are 95% confident that the true population mean for the number of visits per week is between 92 and 108. Because the entire interval exceeds 90 patients per week, it apears that the financial goals are being met.\n\n\n\n\n\n\n\n\nNoteRemember\n\n\n\nThe properties of the confidence intervals using the t-distribution are the same for those constructed using the normal distribtuion:\n\nIncreasing the confidence level will result in a wider (less precise) confidence interval.\nIncreasing the sample size will result in a narrower (more precise) confidence interval.\nThe margin of error is given by: \\(\\text{Margin of Error} = t_{\\alpha/2} \\, \\hat{\\sigma}_{\\bar{x}}\\)\n\n\n\n\n\n\n\nConfidence Intervals for Proportions\nThe confidence interval for the proportion is an interval estimate around a sample proportion that provides us with a range of where the true population proportion lies.\nRecall that the proportion data follow the binomial distribution, which can be approximated by the normal distribution under the following condition:\n\\[np \\ge 5 \\text{ and } n(1-n) \\ge 5\\]\nwhere:\n\np: probability of a success in the population\nn: sample size\n\n\n\n\n\n\n\nNoteMini Case Study\n\n\n\nSuppose the online channel would like to estimate the proportion of customers who are female in order to improve the channel‚Äôs advertising effectiveness.\nLet‚Äôs say from a random sample of 175 customers, 116 were female.\n\n\nSample Proportion\n\\[\\bar{p} = \\frac{x}{n} = \\frac{116}{175}=0.663\\]\nwhere:\n\nx: the number of observations of interest in the sample (successes)\nn: the sample size\n\nTo construct a confidence interval, we need the standard error of the proportion.\nThe true standard error (using the unknown population proportion \\(p\\)) is:\n\\[\n\\sigma_{\\bar{p}} = \\sqrt{\\frac{p(1 - p)}{n}}\n\\]\nWe do not know \\(p\\), yet the formula for standard error requires \\(p\\).\nWe can use the sample proportion \\(\\bar{p}\\) to approximate \\(p\\). This gives the approximate standard error:\n\\[\n\\hat{\\sigma}_{\\bar{p}} = \\sqrt{\\frac{\\bar{p}(1 - \\bar{p})}{n}}\n\\]\nAfter calculated the standard error, can calculate the 99% confidence interval for the proportion of female shopping customers using MoE and calculating UCL and LCL.\n\\[\n\\hat{\\sigma}_{\\bar{p}}\n= \\sqrt{\\frac{\\bar{p}(1 - \\bar{p})}{n}}\n= \\sqrt{\\frac{0.663(1 - 0.663)}{175}}\n= \\sqrt{\\frac{0.223}{175}}\n= \\sqrt{0.001274}\n= 0.0357\n\\]\n\\[ \\Downarrow \\]\n\\[\nUCL_{\\bar{p}} = \\bar{p} + z_{\\alpha/2}\\hat{\\sigma}_{\\bar{p}} = 0.663 + (2.575)(0.0357) =  0.755\n\\]\n\\[\nLCL_{\\bar{p}} = \\bar{p} - z_{\\alpha/2}\\hat{\\sigma}_{\\bar{p}} = 0.663 - (2.575)(0.0357) =0.571\n\\]\nBased on our sample proportion of 0.663, we are 99% confident that the proportion of female shoppers is between 0.572  and 0.755,\n\n\n\n\n\n\nCautionWhy is it wider?\n\n\n\nThe confidence interval seems wider as we want to be 99% confident of capturing the population proportion we need a wide interval\n\n\n\n\nDetermining the Required Sample Size\nSo far, we have focused on calculating a confidence interval and margin of error for a population when we know:\n\nThe confidence level\n\nThe sample size\n\nThe standard deviation\n\nNow we will try to reverse the process. Instead of computing the margin of error from the sample size, we determine the sample size needed to achieve a desired margin of error, given:\n\nThe confidence level\n\nThe population standard deviation\n\nThis procedure is extremely useful because one of the first practical questions in statistical work is: How large should the sample be?\n\ntoo large: wastes resources.\n\ntoo small: produces estimates that are not precise enough.\n\nWe will begin by determining the required sample size for estimating a population mean and proportion.\n\nSample Size | Estimating Population Mean\n\n\n\n\n\n\nTipAt&T Case Study\n\n\n\nA major telecom operator in 2024 needed an accurate estimate of average monthly mobile data usage. With heavy-streaming customers generating most of the traffic, the operator aimed to set plans that balance cost and capacity.\nTo achieve this at a 95% confidence level with a \\(\\pm2\\) GB margin of error, the operator calculated the sample size required to estimate the population mean efficiently and reliably.\nBased on internal analytics, the population standard deviation is believed to be \\(\\sigma = 8\\) GB.\n\n\nIn order to identify the Sample Size we need to do minor algebra for MoE Equation:\n\\[MoE_\\bar{x} = z_{\\frac{\\alpha}{2}} \\sigma_{\\bar{x}} = z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\Rightarrow  \\sqrt{n} = \\frac{z_{\\frac{\\alpha}{2}}\\sigma}{MoE}\\]\n\\[\nn = \\left( \\frac{z_{\\alpha/2}\\, 2\\sigma}{\\text{MoE}} \\right)^2\n\\]\nRemember for a 95% confidence level critical z-value would be \\(z_{\\alpha/2} = 1.96\\)\nCalculate and round up:\n\\[\nn = \\left( \\frac{1.96 \\cdot 8}{2} \\right)^2 = 61.46 = 62\n\\]\nThus, a sample of 62 customers is required to estimate the average monthly mobile data usage with a 95% confidence interval and a \\(\\pm2\\) GB margin of error.\n\n\nSample Size | Estimating Proportion\n\n\n\n\n\n\nTipFinTech Case Study: Biometric Login Adoption\n\n\n\nA leading FinTech app in 2025 wants to estimate the proportion of users who enable biometric login\n(FaceID/TouchID). This metric is crucial for security-feature adoption and for planning future upgrades.\nTo achieve a 95% confidence level with a \\(\\pm2\\)% margin of error, the company needs to determine the minimum sample size required to estimate this proportion accurately.\nA quick pilot test of 200 users revealed that 24% had biometric login enabled, giving\n\\[\\bar{p} = 0.24\\]\n\n\n\\[\nn = \\frac{z_{\\alpha/2}^2 \\, \\bar{p}(1 - \\bar{p})}{(MoE_{\\bar{p}})^2}\n\\]\nFor a 95% confidence level, we use:\n\n\\(z_{\\alpha/2} = 1.96\\)\n\\(\\bar{p} = 0.24\\)\n\\(MoE_{\\bar{p}} = 0.02\\)\n\nThus\n\\[\nn \\approx \\frac{(1.96)^2 (0.24)(0.76)}{(0.02)^2} \\approx 1402.6 \\Rightarrow 1403\n\\]\nA sample of 1,403 users is required to estimate the true proportion of biometric-enabled users within\na 95% confidence interval and \\(\\pm2\\) percentage points of precision.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 05: Sampling"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html",
    "href": "materials/statistics/session3.html",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "",
    "text": "Probabilistic Distributions\n\nContinuous Distributions\n\nNormal Distribution\n\nUniform Distribution\n\nExponential Distribution",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html#what-is-a-probabilistic-distributions",
    "href": "materials/statistics/session3.html#what-is-a-probabilistic-distributions",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "What Is a Probabilistic Distributions",
    "text": "What Is a Probabilistic Distributions\nIn Data Analytics, we rarely know outcomes with certainty.\nInstead of saying:\n‚ÄúTomorrow‚Äôs sales will be exactly 120 units‚Äù\nWe say:\n‚ÄúSales will most likely be around 120, but could reasonably vary‚Äù\nA probabilistic distribution is a formal way to describe this uncertainty by answering to the below three questions:\n\nWhat values can a variable take?\n\nHow likely is each value (or range of values)?\n\nHow is uncertainty spread across those values?\n\n\nFrom Raw Data to Distribution\nWhen we observe data repeatedly:\n\nCustomer purchases\n\nSession durations\n\nDelivery times\n\nPatterns emerge.\nA distribution is a model that summarizes those patterns instead of listing every observation.\n\n\nRandom Variables\nA random variable is a numerical description of an uncertain outcome.\nExamples:\n\nNumber of purchases today\n\nTime (in minutes) until a customer churns\n\nWhether a user clicks an ad (1 or 0)\n\nFrom the above example we could notice that the Random Variable could be\n\nDiscrete: take counts , yes/no outcomes\n\nNumber of compaints per day\nNumber of items sold\nEmail opened or not\netc..\n\nContinuous: measured not counted\n\nRevenue\nTime\nWeight\nDistance\netc..\n\n\n\n\nWell-Known Distributions",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html#continues-distribution",
    "href": "materials/statistics/session3.html#continues-distribution",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "Continues Distribution",
    "text": "Continues Distribution\nMany real-world business variables are measured: Revenue, Time, Cost, Duration, Distances etc..\nA distribution is continuous if:\n\nThe variable can take any real value in a range\n\nThere are infinitely many possible values\n\nExact values are not meaningful on their own\n\n\n\n\n\n\n\nImportantRemember\n\n\n\nFor a continuous random variable \\(X\\):\n\\[\nP(X = x) = 0\n\\]\nThis is not a mistake.\nProbability only makes sense over intervals:\n\\[\nP(a \\le X \\le b)\n\\]\n\n\nTo understand a Continuous probability distribution, we start with a simple experiment:\nWe go outside and measure people‚Äôs heights, one person at a time. Assume the true average height in the population is around 170 cm.\nWe begin with just a few measurements and gradually build the distribution.\nThe heights of the first 5 People:\n\n\n[174, 169, 175, 182, 168]\n\n\n\nHistogram with 5 People\n\n\n\n\n\n\n\n\n\n\n\nWider Bins\n\n\n\n\n\n\n\n\n\n\n\nNarrower Bins\n\n\n\n\n\n\n\n\n\n\n\nIncreasing Sample Size to 300\n\n\n\n\n\n\n\n\n\n\n\nProbability Density Function (PDF)\nContinuous distributions are described by a Probability Density Function (PDF).\nThe PDF:\n\nIs always non-negative\n\nIntegrates to 1 over its entire range\n\nDescribes how ‚Äúdense‚Äù probability is around a value\n\nMathematically:\nImagine as calculating the area under the curves!\n\\[\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\\]\n\n\n\n\n\n\nImportant\n\n\n\n\nA higher PDF value means: Values around that point are more likely\n\nIt does not mean: The probability at that exact point is higher\n\n\n\n\n\n\nExpected Value (Mean)\nThe expected value represents the long-run average outcome.\nFor a continuous distribution:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\\]\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\nExpected value answers:\n‚ÄúIf we repeated this process many times, what average outcome should we expect?‚Äù\n\n\n\n\nVariance\nVariance measures spread or uncertainty around the mean.\nFor a continuous distribution:\n\\[\nVar(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x)\\,dx\n\\]\nwhere \\(\\mu = E[X]\\).\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\n\nLow variance ‚Üí predictable outcomes\n\nHigh variance ‚Üí risky or unstable outcomes\n\n\n\n\n\n\nWhy Continuous Distributions Are Powerful in Analytics\nThey allow us to:\n\nModel natural variability\n\nEstimate probabilities over ranges\n\nBuild confidence intervals\n\nPerform forecasting and optimization\n\n\n\nContinuous Distributions in Practice\nIn this course, we focus on:\n\nNormal Distribution\n\nUniform Distribution\n\nExponential Distribution",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html#normal-distribution",
    "href": "materials/statistics/session3.html#normal-distribution",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nAs the sample size increased and bins became finer, the histogram began to resemble a smooth, symmetric, bell-shaped curve. This shape corresponds to the Normal Distribution.\nA Normal Distribution is a continuous distribution that is:\n\nSymmetric around its mean\n\nBell-shaped\n\nFully described by two parameters\n\nMean \\(\\mu\\)\n\nVariance \\(\\sigma^2\\)\n\n\nIt is denoted as:\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\n\n\n\n\n\n\n\n\n\n\nProbability Density Function\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left( -\\,\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n\\]\n\n\\(f(x)\\) gives the probability density at value \\(x\\). It describes how concentrated the distribution is around that point.\n\\(\\mu\\) is the mean.\n\n\\(\\sigma\\) is the standard deviation.\n\nsmall \\(\\sigma\\) makes the curve narrow and tall,\n\nlarge \\(\\sigma\\) makes it wide and flat.\n\n\nThe term \\(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\) ensures that the total area under the curve equals 1, as required for any probability distribution.\n\n\nExpecteed Value\nFor a normal random variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\):\n\\[\nE[X] = \\mu\n\\]\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\nThe expected value represents the typical or average outcome.\nExamples:\n\nAverage customer height\n\nAverage daily revenue\n\nAverage delivery time\n\n\n\n\n\nVariance of the Normal Distribution\nFor a normal random variable:\n\\[\nVar(X) = \\sigma^2\n\\]\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\nVariance controls spread:\n\nSmall \\(\\sigma^2\\) ‚Üí values tightly clustered around the mean\n\nLarge \\(\\sigma^2\\) ‚Üí values widely spread and more uncertain\n\n\n\n\n\nTwo Normal Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nSpreadsheet Demostration\n\nGeneration: =NORM.INV(RAND(),170,8)\nMean: =AVERAGE(range)\nStandard Deviation: =STDEV.P(range)\nVariance: =VAR.P(range)\nPDF: =NORM.DIST(x, mean, std, FALSE)\nCumulative Probability CDF: =NORM.DIST(x, mean, std, TRUE)\n\n\n\n\n\n\n\nTip\n\n\n\nCheckout the Normal Distribtion on practice here\n\n\n\n\nStandard Normal Distribution\nThe Standard Normal Distribution is a special case of the normal distribution where:\n\\[\n\\mu = 0 \\quad \\text{and} \\quad \\sigma = 1\n\\]\nIt is denoted as:\n\\[\nZ \\sim \\mathcal{N}(0,1)\n\\]\nAny normal random variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) can be transformed into a standard normal variable using standardization:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\n\n\n\n\n\n\nNoteRecall\n\n\n\nStandardization allows us to:\n\nCompare values measured on different scales\n\nCompute probabilities using a single reference distribution\n\nInterpret how many standard deviations a value is from the mean\n\n\n\n\n\n\nNormal vs Gaussian Distribution\nThe Normal Distribution and the Gaussian Distribution are the same thing.\nThey are two names for the same mathematical distribution.\n\nGaussian is the name used in mathematics and physics, after Carl Friedrich Gauss\n\nNormal is the name commonly used in statistics and data analytics\n\nBoth refer to the distribution defined by the PDF:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n\\]\nThey are completely interchangeable terms.",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html#uniform-distribution",
    "href": "materials/statistics/session3.html#uniform-distribution",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe Uniform Distribution models situations where all values within a range are equally likely.\nThere is:\n\nNo center\n\nNo peak\n\nNo value more likely than another\n\nImagine the following situations:\n\nA random number generator picks a number between 0 and 1\n\nA customer arrives at a store at a random time between 09:00 and 10:00\n\nA system assigns users randomly to time slots between 0 and 30 minutes\n\nIn all these cases:\n\nEvery value in the interval is equally likely\n\nAnother example:\nImagine a telecom system that assigns a customer to one of several identical support bots randomly.\nThe system waits somewhere between 0 and 10 seconds before routing the customer, and every value in that interval is equally likely.\nThis kind of process has no preference:\n\nnot more likely to assign earlier,\nnot more likely to assign later.\n\n\nDefinition\nA Uniform Distribution on the interval \\([a, b]\\) is denoted as:\n\\[\nX \\sim \\text{Uniform}(a, b)\n\\]\nwhere:\n\n\\(a\\) is the minimum possible value\n\n\\(b\\) is the maximum possible value\n\n\n\nProbability Density Function (PDF)\nIf \\(X \\sim U(a,b)\\), then:\n\\[\nf(x) = \\frac{1}{b - a}, \\quad a \\le x \\le b\n\\]\n\n\\(f(x)=0\\) outside the interval\n\\([a,b]\\) is equally likely.\n\nSuppose we observe a sample \\(x_1, x_2, \\dots, x_n\\) from a uniform distribution \\(U(a,b)\\).\nThe likelihood of the parameters \\((a,b)\\) given the data is:\n\\[\nL(a,b \\mid x_1,\\dots,x_n)=\\prod_{i=1}^n f(x_i)\n\\]\nBecause the PDF is constant inside the interval:\n\nIf all observations lie in \\([a,b]\\):\n\n\\[\nL(a,b \\mid x_1,\\dots,x_n)=\\left(\\frac{1}{b-a}\\right)^n\n\\]\n\nIf any observation lies outside \\([a,b]\\):\n\n\\[\nL(a,b \\mid x_1,\\dots,x_n)=0\n\\]\nSo uniform likelihood is simple: constant if all points are inside, zero otherwise.\n\n\nVisualizing Uniform Distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantImportant Interpretation\n\n\n\nBecause the PDF is flat:\n\nNo value inside \\([a,b]\\) is more likely than another\n\nProbability depends only on interval length, not position\n\n\n\n\n\nExpected Value (Mean)\nFor a uniform random variable \\(X \\sim \\text{Uniform}(a,b)\\):\n\\[\nE[X] = \\frac{a + b}{2}\n\\]\n\n\nExpected Value (Mean)\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\nThe expected value is simply the midpoint of the interval.\nExample:\nIf arrival time is uniformly distributed between 0 and 60 minutes,\nthe average arrival time is 30 minutes.\n\n\n\n\nVariance\nFor a uniform distribution:\n\\[\nVar(X) = \\frac{(b - a)^2}{12}\n\\]\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\n\nWider interval ‚Üí higher uncertainty\nNarrow interval ‚Üí more predictable outcomes\n\n\n\nIf \\(X \\sim \\text{Uniform}(0, 10)\\):\n\\[\nP(2 \\le X \\le 5) = \\frac{5 - 2}{10 - 0} = 0.3\n\\]\n\n\nBusiness Applications of the Uniform Distribution\nUniform distributions are used when:\n\nRandom assignment is required\nNo prior preference exists\n\nExamples:\n\nA/B testing randomization\nLoad balancing\nSimulation baselines\nRandom sampling assumptions\n\n\n\nProbability as Area\nBecause the density is constant:\n\\[\nP(c \\le X \\le d) = \\frac{d - c}{b - a}\n\\]\nThis is simply the fraction of the interval covered.\n\n\nSpreadsheet Demonstration (Uniform Distribution)\n\nGenerate Uniform Data: =RAND()*(b-a)+a\n\nExample for \\([0,10]\\): =RAND()*10\n\nExpected Value: =(a+b)/2\nVariance: =(b-a)^2/12\nProbability Between Two Values \\(c\\) and \\(d\\): =(d-c)/(b-a)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe uniform distribution assumes:\n\nNo structure\n\nNo memory\n\nNo preferred values",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html#exponential-distribution",
    "href": "materials/statistics/session3.html#exponential-distribution",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nWe now move to a continuous distribution that models waiting time until an event occurs.\nThis distribution is fundamentally different from Normal and Uniform distributions because:\n\nIt is not symmetric\n\nIt is right-skewed\n\nIt explicitly models time-to-event behavior\n\nConsider a supermarket or retail chain.\nCustomers arrive at the checkout lanes randomly, and the store wants to model:\nHow long until the next customer arrives at the counter?\nIf arrivals are independent and have no memory, then the waiting time** between customer arrivals follows an Exponential distribution.**\n\nif you‚Äôve been waiting 4 minutes already, the next customer is not due\nevery moment is a fresh start\n\nthe past does NOT influence the future (Markov Chain)\n\nThis is very common in retail analytics:\n\ntime until next customer walks into the store,\ntime until next person reaches a self-checkout station,\ntime until next event in an online store: purchase, add-to-cart, click, etc.\n\nAll of these waiting times are modeled by the Exponential distribution.\n\nDefinition\nA random variable \\(X\\) follows an Exponential Distribution if:\n\\[\nX \\sim \\text{Exp}(\\lambda)\n\\]\nwhere:\n\n\\(\\lambda &gt; 0\\) is the arrival rate (events per unit time)\n\\(\\frac{1}{\\lambda}\\) is the average waiting time\n\n\n\nProbability Density Function (PDF)\nIf \\(X \\sim \\text{Exp}(\\lambda)\\):\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\n\\]\nWhere:\n\n\\(\\lambda\\) = customer arrival rate (customers per minute)\n\n\\(1/\\lambda\\) = average waiting time\n\n\n\nExpected Value (Mean)\n\\[\nE[X] = \\frac{1}{\\lambda}\n\\]\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\nIf customers arrive at a rate of: \\(\\lambda = 2\\) customers per minute then the expected waiting time is: \\(1/2 = 0.5\\) minutes\n\n\n\n\n\nVariance\n\\[\nVar(X) = \\frac{1}{\\lambda^2}\n\\]\n\n\n\n\n\n\nTipBusiness Interpretation\n\n\n\n\nHigh arrival rate ‚Üí lower variability\n\nLow arrival rate ‚Üí higher uncertainty in waiting times\n\n\n\n\n\nLikelihood for Observed Retail Data\nSuppose we measure actual waiting times between customer arrivals:\n\\[\nx_1, x_2, \\dots, x_n\n\\]\nThe likelihood of \\(\\lambda\\) given the data is:\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}\n\\]\nThis simplifies to:\n\\[\nL(\\lambda)\n= \\lambda^n \\exp\\!\\left(-\\lambda \\sum_{i=1}^n x_i\\right)\n\\]\nLog-likelihood:\n\\[\n\\ell(\\lambda)\n= n \\ln(\\lambda) - \\lambda \\sum x_i\n\\]\nMaximum Likelihood Estimate (MLE):\n\\[\n\\hat{\\lambda} = \\frac{n}{\\sum x_i}\n\\]\nInterpretation:\n\nfast arrivals ‚Üí large \\(\\lambda\\)\n\nslow arrivals ‚Üí small \\(\\lambda\\)\n\nJust like checkout traffic in a retail store.\n\n\n\nVisualization\nWe use example waiting times in minutes: obs = [1.2, 0.5, 2.0, 0.8]\nThese could be times between customers reaching a checkout lane.\n\nLeft plot ‚Üí PDF comparison for \\(\\lambda = 1\\) and \\(\\lambda = 0.3\\)\n\nRight plot ‚Üí Likelihood curve for the observed retail dat\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nWhen customers arrive quickly and consistently, the waiting times shrink ‚Üí the likelihood favors a large \\(\\lambda\\).\nWhen customers arrive sporadically or slowly, the waiting times grow ‚Üí the likelihood favors a small \\(\\lambda\\).\n\nIn our observed data:\n\nAverage waiting time = \\(4.5 / 4 = 1.125\\) minutes\n\nMLE: \\(\\hat{\\lambda} = 4/4.5 = 0.889\\) customers/minute\n\nMeaning:\n\nthe best-fitting model suggests approximately 0.89 customers per minute,\n\nwhich corresponds to one customer roughly every 1.1 minutes.\n\nThis type of analysis is central in retail analytics for understanding staffing requirements, managing checkout lanes, predicting peak hours, and optimizing store operations.\n\n\nSpreadsheet Demonstration (Exponential Distribution)\n\nGenerate exponential data: =-LN(1-RAND())/lambda\nExpected value: =1/lambda\nVariance: =1/(lambda^2)\nCDF (event occurs within x): =1-EXP(-lambda*x)",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/statistics/session3.html#watching-materials",
    "href": "materials/statistics/session3.html#watching-materials",
    "title": "Statistics Session 03: Probabilistic Distributions",
    "section": "Watching Materials",
    "text": "Watching Materials\n\nNormal Distribution\nExponential Distribution",
    "crumbs": [
      "Syllabus",
      "Statistical Thinking",
      "Statistics",
      "Statistics Session 03: Probabilistic Distributions"
    ]
  },
  {
    "objectID": "materials/python/session1.html",
    "href": "materials/python/session1.html",
    "title": "Python Session 01: Coming Soon",
    "section": "",
    "text": "Coming soon.",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Python Session 01: Coming Soon"
    ]
  },
  {
    "objectID": "materials/python/slides/session6.html#comming.-soon",
    "href": "materials/python/slides/session6.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/python/slides/session7.html#comming.-soon",
    "href": "materials/python/slides/session7.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/python/slides/session2.html#comming.-soon",
    "href": "materials/python/slides/session2.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon",
    "crumbs": [
      "Syllabus",
      "Python",
      "Python",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/sql/session4.html",
    "href": "materials/sql/session4.html",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "",
    "text": "Now, we will be taking a closer look at how to filter data using the WHERE clause and the HAVING statement. With lots of examples and use cases along the way to prepare you for writing queries in the real world.\nAfter SELECT, these are the most widely used commands for a data analyst, so having a solid grasp of them is essential. The good news is that mastering them is easier than you think‚Äîyou‚Äôll see why in just a moment.\nOnce you‚Äôre comfortable using the WHERE and HAVING clauses to pinpoint the information you want, we‚Äôll introduce the CASE statement, which you can use to build new categories out of your data.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/session4.html#introduction",
    "href": "materials/sql/session4.html#introduction",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "",
    "text": "Now, we will be taking a closer look at how to filter data using the WHERE clause and the HAVING statement. With lots of examples and use cases along the way to prepare you for writing queries in the real world.\nAfter SELECT, these are the most widely used commands for a data analyst, so having a solid grasp of them is essential. The good news is that mastering them is easier than you think‚Äîyou‚Äôll see why in just a moment.\nOnce you‚Äôre comfortable using the WHERE and HAVING clauses to pinpoint the information you want, we‚Äôll introduce the CASE statement, which you can use to build new categories out of your data.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/session4.html#creating-a-denormalized-analysis-table",
    "href": "materials/sql/session4.html#creating-a-denormalized-analysis-table",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "Creating a Denormalized Analysis Table",
    "text": "Creating a Denormalized Analysis Table\nBefore diving deeper into filtering techniques, we will create a single, denormalized table that brings together all relevant columns needed for analysis.\nThe goal of this table is to simplify analytical queries by:\n\nJoining all related tables into one structure\nKeeping only business-relevant attributes used for filtering and analysis\nIgnoring surrogate and foreign keys, except for the transactional identifier\n\nThis approach is common in analytics and reporting, where ease of querying is more important than strict normalization.\n\nDenormalized Table Design\nThe denormalized table will be built around the sales table and will include:\n\nTransaction-level information\n\nOrder time attributes\n\nProduct attributes\n\nCustomer attributes\n\nEmployee attributes\n\nThe only identifier retained will be the transaction identifier.\n\n\nCreating sales_analysis Table\nCREATE TABLE IF NOT EXISTS sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name  AS employee_first_name,\n    e.last_name   AS employee_last_name,\n    e.salary      AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\n\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;\n\n\nAdding indexes\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);\nThe sales_analysis table now stores both timestamp-level and date-level order information and will serve as the base table for all filtering examples in this session.\n\n\n\n\n\n\nWarning\n\n\n\nWe are going to learn the JOINs so for now do not worry and simply capy and paste the above code",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/session4.html#where",
    "href": "materials/sql/session4.html#where",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "WHERE",
    "text": "WHERE\nWhen conducting an analysis, you will almost never work with every single record in a database. Instead, you will focus on a specific subset of data that is relevant to the business question you are trying to answer.\nIn the context of our case study, this might mean\n\nanalyzing sales made in a particular year,\ntransactions with high discounts,\nproducts belonging to a specific category, or\ncustomers from a certain city.\netc.\n\nTo extract only the records that matter for your analysis, you need to filter the data.\nIn SQL, filtering is primarily done using the WHERE clause. The WHERE clause allows you to include or exclude rows based on conditions applied to columns in your tables.\n\n\n\n\n\n\nTip\n\n\n\nFor example, you may want to look only at sales where:\n\nthe total sales amount exceeds a certain threshold,\norders placed in a specific quarter\nproducts priced above the average.\n\n\n\nTo define these filtering conditions, the WHERE clause is combined with operators.\nSome of these operators are intuitive and closely resemble everyday language, such as BETWEEN, OR, IN, AND or LIKE.\nBy understanding how these operators work, you can precisely control which rows from tables like sales, orders, products, or customers are included in your analysis.\n\nSimple WHERE Conditions\n\n\n\n\n\n\nJUST a REMINDER\n\n\n\nYou can filter rows based on a single condition.\nExample: select transactions where total sales exceed 100,000.\nSELECT\n    transaction_id,\n    order_date_date,\n    product_name,\n    total_sales\nFROM sales_analysis\nWHERE total_sales &gt; 1000;\nOnly rows with total_sales greater than 1000 are included in the result.\nExample: select transactions for products in the Electronics category.\nSELECT\n    transaction_id,\n    product_name,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE category = 'Electronics';\n\n\nCombining Conditions with AND\nThe AND operator requires all conditions to be true for a row to be included.\nExample: select transactions from 2024 with high total sales.\nSELECT\n    transaction_id,\n    order_date_date,\n    year,\n    product_name,\n    total_sales\nFROM sales_analysis\nWHERE year = 2023\n  AND total_sales &gt; 10000;\nRows must satisfy both conditions at the same time.\nExample: select Electronics sales from the city of East Amanda.\nSELECT\n    transaction_id,\n    city,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE city = 'East Amanda'\n  AND category = 'Electronics';\n\n\nCombining Conditions with OR\nThe OR operator requires at least one condition to be true.\nExample: select transactions that occurred either in East Amanda or Smithside.\nSELECT\n    transaction_id,\n    order_date_date,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE city = 'East Amanda'\n   OR city = 'Smithside';\nExample: select transactions for products in either the Toys or Books categories.\nSELECT\n    transaction_id,\n    product_name,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE category = 'Toys'\n   OR category = 'Books';\n\n\nUsing BETWEEN for Ranges\nThe BETWEEN operator is used to filter values within a specified range.\nThe range is inclusive of both boundary values.\nExample: select transactions with total sales between 50,000 and 150,000.\nSELECT\n    transaction_id,\n    order_date_date,\n    total_sales\nFROM sales_analysis\nWHERE total_sales BETWEEN 50000 AND 150000;\nExample: select transactions from years between 2022 and 2024.\nSELECT\n    transaction_id,\n    year,\n    total_sales\nFROM sales_analysis\nWHERE year BETWEEN 2022 AND 2024;\n\n\n\n\n\n\nNote\n\n\n\nwe can achieve the same result by doing:\nWHERE year&gt;=2022 and year&lt;2024\n\n\nWhen filtering against a fixed list of known values, the IN and NOT IN operators provide a cleaner and more readable alternative to multiple OR conditions.\n\n\n\nUsing IN\nThe IN operator checks whether a value matches any value in a specified list.\nExample: select transactions from selected cities.\nSELECT\n    transaction_id,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE city IN ('East Amanda', 'Smithside', 'Lake Thomas');\nThis query is functionally equivalent to combining multiple OR conditions, but is easier to read and maintain.\nExample: select transactions for specific product categories.\nSELECT\n    transaction_id,\n    product_name,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE category IN ('Electronics', 'Books');\n\n\n\nUsing NOT IN\nThe NOT IN operator excludes rows that match any value in the specified list.\nExample: exclude transactions from specific cities.\nSELECT\n    transaction_id,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE city NOT IN ('East Lori', 'Anthonymouth');\nExample: exclude low-priority product categories.\nSELECT\n    transaction_id,\n    product_name,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE category NOT IN ('Toys', 'Books');\n\n\nFiltering Text with LIKE\nThe LIKE operator is used for pattern matching on text columns.\nThe percent sign % represents zero or more characters.\nExample: select products whose name starts with the letter ‚ÄúE‚Äù.\nSELECT\n    transaction_id,\n    product_name,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE product_name LIKE 'E%';\nExample: select cities whose name contains the word ‚ÄúNorth‚Äù.\nSELECT\n    transaction_id,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE city LIKE '%North%';\n\n\n\n\n\n\n\nLIKE Operator Example\nDescription\n\n\n\n\nWHERE product_name LIKE 'Elec%'\nFinds values that start with ‚ÄúElec‚Äù\n\n\nWHERE product_name LIKE '%Phone'\nFinds values that end with ‚ÄúPhone‚Äù\n\n\nWHERE product_name LIKE '%Pro%'\nFinds values that contain ‚ÄúPro‚Äù anywhere\n\n\nWHERE city LIKE '_ast%'\nFinds values with ‚Äúast‚Äù starting from the second character\n\n\nWHERE city LIKE 'N%_%'\nFinds values that start with ‚ÄúN‚Äù and are at least 3 characters\n\n\nWHERE category LIKE 'B%ks'\nFinds values that start with ‚ÄúB‚Äù and end with ‚Äúks‚Äù\n\n\n\n\n\n\n\n\n\nWarningcapitalization\n\n\n\nWhen using the LIKE operator, it is important to pay attention to capitalization. In PostgreSQL, LIKE is case-sensitive by default. This means that searching for a value with different letter casing may return no results.\nFor example, product categories such as Electronics or city names like East Amanda must be matched using the correct capitalization. Searching for 'electronics' or 'east amanda' with LIKE would not return any rows.\nTo handle this, you can normalize the text by converting both the column and the search pattern to the same case using string functions. Common approaches include using LOWER() or UPPER() to make the comparison case-insensitive.\nAlternatively, PostgreSQL provides the ILIKE operator, which performs case-insensitive pattern matching and is often more convenient when working with user-entered text or inconsistent capitalization.\n\n\n\n\nHandling NULL Values Correctly\n\n\n\n\n\n\nCautionIS NULL or NOT NULL\n\n\n\nIn SQL, NULL represents missing or unknown data.\nIt is not equal to anything, including another NULL.\nThis means the following will not work as expected:\nWHERE discount = NULL\nTo correctly filter missing values, always use:\nWHERE discount IS NULL\nTo filter rows where a value exists, use:\nWHERE discount IS NOT NULL\nBe especially careful when using NOT IN.\nIf the list contains a NULL, the condition may return no rows at all.\nWhen working with columns that may contain missing values, consider explicitly handling NULL values before applying IN or NOT IN.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/session4.html#having",
    "href": "materials/sql/session4.html#having",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "HAVING",
    "text": "HAVING\n\nDetecting Duplicates with HAVING\nIn analytical work, identifying duplicate records is an important data quality task.\nDuplicates can distort metrics, inflate counts, and lead to incorrect conclusions.\nThe HAVING clause combined with COUNT() is commonly used to detect duplicates after grouping.\n\n\n\nIdentifying Duplicate Transactions\nAlthough transaction_id is expected to be unique, this example demonstrates the general technique for detecting duplicates.\nSELECT\n    transaction_id,\n    COUNT(*) AS occurrence_count\nFROM sales_analysis\nGROUP BY transaction_id\nHAVING COUNT(*) &gt; 1;\nIf this query returns rows, it indicates duplicated transaction records in the analysis table.\n\n\n\nDetecting Duplicate Product Sales on the Same Date\nThis example checks whether the same product appears multiple times on the same date, which may or may not be expected depending on the business logic.\nSELECT\n    product_name,\n    order_date_date,\n    COUNT(*) AS occurrence_count\nFROM sales_analysis\nGROUP BY product_name, order_date_date\nHAVING COUNT(*) &gt; 1\nORDER BY occurrence_count DESC;\nThis helps identify repeated entries for the same product on a given day.\n\n\n\nFinding Customers with Multiple Transactions on the Same Day\nThis analysis identifies customers who made multiple purchases on the same date.\nSELECT\n    customer_name,\n    order_date_date,\n    COUNT(*) AS transaction_count\nFROM sales_analysis\nGROUP BY customer_name, order_date_date\nHAVING COUNT(*) &gt; 1\nORDER BY transaction_count DESC;\nThis can reveal bulk purchases, repeat behavior, or potential data duplication.\n\n\n\nIdentifying Potential Duplicate Sales Amounts\nSometimes duplicates appear as identical sales values repeated multiple times.\nSELECT\n    total_sales,\n    order_date_date,\n    COUNT(*) AS occurrence_count\nFROM sales_analysis\nGROUP BY total_sales, order_date_date\nHAVING COUNT(*) &gt; 1\nORDER BY occurrence_count DESC;\nThis pattern can signal repeated transactions or data loading issues.\n\n\nRevenue-Focused Analysis\nThis example focuses on identifying product categories that generated significant revenue during a specific period.\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY category\nHAVING SUM(total_sales) &gt; 500000\nORDER BY total_sales_amount DESC;\nThis analysis highlights the strongest revenue-driving categories for the year and filters out categories with marginal impact.\n\n\n\nTransaction Volume Analysis by City\nHere, the focus is on understanding customer activity levels across different cities.\nSELECT\n    city,\n    COUNT(transaction_id) AS transaction_count\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY city\nHAVING COUNT(transaction_id) &gt; 1000\nORDER BY transaction_count DESC;\nThis helps identify cities with consistently high transaction volumes.\n\n\n\nRevenue Performance by City\nTransaction volume alone does not fully describe performance. This example focuses on revenue contribution by city.\nSELECT\n    city,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY city\nHAVING SUM(total_sales) &gt; 400000\nORDER BY total_sales_amount DESC;\nThis distinguishes cities that generate substantial revenue from those driven primarily by transaction count.\n\n\n\nHigh-Frequency, Low-Revenue Categories\nThis analysis helps detect categories that sell often but contribute relatively little to total revenue.\nSELECT\n    category,\n    COUNT(transaction_id) AS transaction_count,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY category\nHAVING COUNT(transaction_id) &gt; 500\n   AND SUM(total_sales) &lt; 300000\nORDER BY transaction_count DESC;\nSuch patterns may indicate lower-priced items or aggressive discounting strategies.\n\n\n\nAverage Transaction Value by City\nInstead of total revenue, this example focuses on transaction quality.\nSELECT\n    city,\n    AVG(total_sales) AS avg_transaction_value\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY city\nHAVING AVG(total_sales) &gt; 800\nORDER BY avg_transaction_value DESC;\nThis highlights markets where customers tend to make higher-value purchases.\n\n\n\nRevenue with Controlled Discounting\nThis analysis combines revenue performance with pricing discipline.\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount,\n    AVG(discount) AS avg_discount\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY category\nHAVING SUM(total_sales) &gt; 400000\n   AND AVG(discount) &lt; 0.15\nORDER BY total_sales_amount DESC;\nThis helps identify categories that perform well financially without relying heavily on discounts.\n\n\nUsing HAVING Instead of WHERE ‚Äî Common Interview Trap\n\n\n\n\n\n\nWarning\n\n\n\nIt is possible to filter by non-aggregated columns (such as category or city) using the HAVING clause if those columns appear in the GROUP BY clause.\nFor example, this query is syntactically valid:\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nGROUP BY category\nHAVING category = 'Electronics';\nHowever, this is not equivalent in intent or best practice to using WHERE.\nThe preferred and correct approach is:\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE category = 'Electronics'\nGROUP BY category;\n\nWill the results be the same?\nYes ‚Äî in this specific case, the final result will be the same.\nBut the execution logic is different:\n\nWHERE filters rows before aggregation\nHAVING filters groups after aggregation\n\nUsing HAVING to filter non-aggregated columns means:\n\nMore rows are processed than necessary\nAggregation work is done first, then filtered\nPerformance may be worse on large datasets\n\n\n\nInterview takeaway\n\nFiltering raw columns ‚Üí use WHERE\nFiltering aggregated results ‚Üí use HAVING\nUsing HAVING instead of WHERE for column filters is usually a code smell, even if it returns the same result\n\nInterviewers often ask this to test whether you understand query execution order, not just syntax.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/session4.html#case-statement",
    "href": "materials/sql/session4.html#case-statement",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "CASE Statement",
    "text": "CASE Statement\nNow that you are comfortable filtering data, you can move on to creating derived logic directly inside your queries using the CASE statement.\nThe CASE statement allows you to define conditional rules and return different values depending on whether those conditions are met. Conceptually, it works very similarly to everyday logic:\n\nIf a condition is true, return one value\nIf another condition is true, return a different value\nOtherwise, return a default value\n\nIn SQL, this logic is expressed using WHEN, THEN, ELSE, and END.\n\n\nBasic Structure of CASE\nThe general structure of a CASE statement looks like this:\nCASE\n    WHEN condition_1 THEN result_1\n    WHEN condition_2 THEN result_2\n    ELSE default_result\nEND\nThe CASE statement is typically written inside the SELECT clause and creates a new derived column in the query result.\n\n\n\nCategorizing Transactions by Sales Size\nExample: classify each transaction based on its total sales amount.\nSELECT\n    transaction_id,\n    total_sales,\n    CASE\n        WHEN total_sales &gt;= 100000 THEN 'High Value'\n        WHEN total_sales &gt;= 50000 THEN 'Medium Value'\n        ELSE 'Low Value'\n    END AS sales_segment\nFROM sales_analysis;\nThis query creates a new column called sales_segment that categorizes each transaction based on business-defined thresholds.\n\n\n\nCreating Customer-Facing Labels\nExample: label transactions based on discount behavior.\nSELECT\n    transaction_id,\n    discount,\n    CASE\n        WHEN discount IS NULL THEN 'No Discount Information'\n        WHEN discount = 0 THEN 'No Discount'\n        WHEN discount &lt;= 0.10 THEN 'Low Discount'\n        ELSE 'High Discount'\n    END AS discount_category\nFROM sales_analysis;\nThis is useful when translating raw numeric values into interpretable categories.\n\n\nCategorizing Products by Price Range\nExample: group products into pricing tiers.\nSELECT\n    product_name,\n    price,\n    CASE\n        WHEN price &gt;= 1000 THEN 'Premium'\n        WHEN price &gt;= 500 THEN 'Mid-Range'\n        ELSE 'Budget'\n    END AS price_category\nFROM sales_analysis;\nThis type of logic is frequently used in reporting and segmentation.\n\n\nCASE with Dates and Time-Based Logic\nExample: label transactions as early or late in the year.\nSELECT\n    transaction_id,\n    order_date_date,\n    CASE\n        WHEN quarter IN (1, 2) THEN 'First Half'\n        ELSE 'Second Half'\n    END AS year_period\nFROM sales_analysis;\nThis helps simplify time-based analysis without modifying the underlying table.\n\n\n\nCASE with Aggregation\nCASE can also be combined with aggregation to create conditional metrics.\nExample: calculate total sales for discounted vs non-discounted transactions.\nSELECT\n    CASE\n        WHEN discount &gt; 0 THEN 'Discounted'\n        ELSE 'Full Price'\n    END AS pricing_type,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nGROUP BY pricing_type;\nHere, the CASE expression defines the grouping logic.\n\n\n\n\n\n\n\nTipImportant Note About CASE\n\n\n\nA CASE statement creates a derived column only in the query result.\nIt does not create or modify columns in the underlying database table.\nIf you need to store the result permanently, you must use CREATE TABLE AS or ALTER TABLE with an update statement.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/session4.html#homework",
    "href": "materials/sql/session4.html#homework",
    "title": "Session 04: Data Analysis with SQL | Part II",
    "section": "Homework",
    "text": "Homework\nIn this task, you will apply advanced filtering and conditional logic using SQL.\nThe main objective is to demonstrate your ability to combine multiple filtering techniques with complex CASE statements in an analytical context.\nAll logic must be implemented using:\n\nWHERE\nAND, OR, IN, BETWEEN, LIKE\nGROUP BY\nHAVING\nCASE WHEN\n\nData source: sales_analysis table (as prepared in this session)\n\n\nTask 1 | Complex Transaction Segmentation (CASE + WHERE)\nCreate a query that classifies each transaction into a business segment using a CASE statement.\nYour segmentation must consider at least all of the following dimensions:\n\nTransaction value (total_sales)\nDiscount level (discount)\nProduct category\nCity\n\nExample ideas (you must define your own rules and labels):\n\nHigh-value Electronics transactions with low discount\nMedium-value transactions with moderate discount\nLow-value or heavily discounted transactions\nAny additional meaningful segment you define\n\nRequirements:\n\nUse CASE WHEN with multiple conditions combined using AND / OR\nUse WHERE to limit the analysis to year 2023\nReturn at least:\n\ntransaction_id\ncity\ncategory\ntotal_sales\nThe derived segmentation column\n\n\n\n\nTask 2 | Category-Level Performance Analysis (CASE + GROUP BY + HAVING)\nCreate an aggregated analysis at the product category level.\nSteps:\n\nGroup data by category\nCalculate at least:\n\nTotal sales\nNumber of transactions\nAverage discount\n\nUse a CASE statement to assign a performance label to each category, such as:\n\nStrong Performer\nAverage Performer\nUnderperformer\n\n\nRequirements:\n\nUse WHERE to restrict data to year 2023\nUse CASE based on aggregated values\nUse HAVING to exclude categories with very low activity\nSort results by a relevant business metric\n\n\n\nTask 3 | City-Level Activity Analysis (COUNT + HAVING + CASE)\nAnalyze customer activity patterns by city.\nSteps:\n\nGroup data by city\nUse COUNT(*) to measure transaction volume\nUse CASE to classify cities into activity tiers\n\nExample activity tiers (you must define thresholds):\n\nHigh Activity\nMedium Activity\nLow Activity\n\nRequirements:\n\nUse COUNT(*)\nUse HAVING to filter cities based on transaction volume\nUse CASE to convert numeric metrics into categorical labels\nLimit analysis to year 2023\n\n\n\nTask 4 | Discount Behavior Analysis (CASE + HAVING)\nAnalyze how discounts are applied across categories.\nSteps:\n\nGroup data by category\nCalculate:\n\nAverage discount\nTotal sales\n\nUse CASE to label categories based on discount behavior, such as:\n\nDiscount-Heavy\nModerate Discount\nLow or No Discount\n\n\nRequirements:\n\nUse CASE with aggregated metrics\nUse HAVING to exclude categories with insufficient data\nOrder results to clearly show patterns\n\n\n\nSubmission Guidelines\n\nCreate filters.sql in your project\nEach task should be answered with at least one query\nQueries must be readable, well-structured, and logically correct\nMeaningful aliases are required for all derived columns\nDo not modify the underlying table structure\nPush the results into GitHub\n\n\n\n\n\n\n\nTipReminder\n\n\n\ngit add filters.sql\ngit commit -m \"adding filter related homewokr\"\ngit push",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 04: Data Analysis with SQL | Part II"
    ]
  },
  {
    "objectID": "materials/sql/index.html",
    "href": "materials/sql/index.html",
    "title": "SQL",
    "section": "",
    "text": "Session 01: Intro to Relational Databases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 02: Intro to PostgreSQL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 03: Data Analysis with SQL | Part I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 04: Data Analysis with SQL | Part II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 05: Data Analysis with SQL | Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 06: Data Analysis with SQL | Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 07: Data Analysis with SQL | Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession 08: Data Analysis with SQL | JOINs\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "SQL"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#agenda",
    "href": "materials/sql/slides/session4.html#agenda",
    "title": "Filtering",
    "section": "Agenda",
    "text": "Agenda\n\n\nBuild an analysis-ready denormalized table (sales_analysis)\nFilter rows with WHERE using key operators\nFilter aggregated results with HAVING\nUse CASE WHEN to create analytical categories",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#run-docker",
    "href": "materials/sql/slides/session4.html#run-docker",
    "title": "Filtering",
    "section": "Run Docker",
    "text": "Run Docker\nOpen Docker Desktop, Run and open Pgadmin:\ndocker compose up -d",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#create-sales_analysis",
    "href": "materials/sql/slides/session4.html#create-sales_analysis",
    "title": "Filtering",
    "section": "Create sales_analysis",
    "text": "Create sales_analysis\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#indexes-for-filtering-performance",
    "href": "materials/sql/slides/session4.html#indexes-for-filtering-performance",
    "title": "Filtering",
    "section": "Indexes for Filtering Performance",
    "text": "Indexes for Filtering Performance\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#simple-where-examples",
    "href": "materials/sql/slides/session4.html#simple-where-examples",
    "title": "Filtering",
    "section": "Simple WHERE Examples",
    "text": "Simple WHERE Examples\nSELECT\n    transaction_id,\n    order_date_date,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE total_sales &gt; 100000;\nSELECT\n    transaction_id,\n    city,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE city = 'East Amanda';",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#and-combine-conditions",
    "href": "materials/sql/slides/session4.html#and-combine-conditions",
    "title": "Filtering",
    "section": "AND: Combine Conditions",
    "text": "AND: Combine Conditions\nSELECT\n    transaction_id,\n    year,\n    city,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE year = 2023\n  AND category = 'Electronics'\n  AND total_sales &gt; 100000;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#or-multiple-acceptable-conditions",
    "href": "materials/sql/slides/session4.html#or-multiple-acceptable-conditions",
    "title": "Filtering",
    "section": "OR: Multiple Acceptable Conditions",
    "text": "OR: Multiple Acceptable Conditions\nSELECT\n    transaction_id,\n    year,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE year = 2023\n  AND (city = 'Smithside' OR city = 'Lake Thomas');",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#in-and-not-in-cleaner-than-many-ors",
    "href": "materials/sql/slides/session4.html#in-and-not-in-cleaner-than-many-ors",
    "title": "Filtering",
    "section": "IN and NOT IN: Cleaner Than Many ORs",
    "text": "IN and NOT IN: Cleaner Than Many ORs\nSELECT\n    transaction_id,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE city IN ('East Amanda', 'Smithside', 'Lake Thomas');\nSELECT\n    transaction_id,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE category NOT IN ('Toys', 'Books');",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#like-pattern-filtering-wildcards",
    "href": "materials/sql/slides/session4.html#like-pattern-filtering-wildcards",
    "title": "Filtering",
    "section": "LIKE: Pattern Filtering (Wildcards)",
    "text": "LIKE: Pattern Filtering (Wildcards)\nSELECT\n    transaction_id,\n    city,\n    total_sales\nFROM sales_analysis\nWHERE city LIKE 'East%';\nSELECT\n    transaction_id,\n    category,\n    total_sales\nFROM sales_analysis\nWHERE category LIKE '%Garden%';",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#like-wildcards-cheatsheet",
    "href": "materials/sql/slides/session4.html#like-wildcards-cheatsheet",
    "title": "Filtering",
    "section": "LIKE Wildcards Cheatsheet",
    "text": "LIKE Wildcards Cheatsheet\n\n\n\n\n\n\n\nPattern Example\nMeaning\n\n\n\n\nWHERE product_name LIKE 'Elec%'\nStarts with ‚ÄúElec‚Äù\n\n\nWHERE product_name LIKE '%Phone'\nEnds with ‚ÄúPhone‚Äù\n\n\nWHERE product_name LIKE '%Pro%'\nContains ‚ÄúPro‚Äù anywhere\n\n\nWHERE city LIKE '_ast%'\n‚Äúast‚Äù begins at the second character\n\n\nWHERE city LIKE 'N%_%'\nStarts with ‚ÄúN‚Äù and has at least 3 characters\n\n\nWHERE category LIKE 'B%ks'\nStarts with ‚ÄúB‚Äù and ends with ‚Äúks‚Äù",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#null-filtering",
    "href": "materials/sql/slides/session4.html#null-filtering",
    "title": "Filtering",
    "section": "NULL Filtering",
    "text": "NULL Filtering\n\n\n\n\n\n\nTip\n\n\n\nUse IS NULL and IS NOT NULL\nDo not use = NULL or &lt;&gt; NULL\n\n\n\n\n\nSELECT\n    transaction_id,\n    discount\nFROM sales_analysis\nWHERE discount IS NULL;\nSELECT\n    transaction_id,\n    discount\nFROM sales_analysis\nWHERE discount IS NOT NULL;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#wrong-vs-correct-count",
    "href": "materials/sql/slides/session4.html#wrong-vs-correct-count",
    "title": "Filtering",
    "section": "Wrong vs Correct (COUNT)",
    "text": "Wrong vs Correct (COUNT)\nSELECT\n    city,\n    COUNT(*) AS transaction_count\nFROM sales_analysis\nWHERE COUNT(*) &gt; 1000\nGROUP BY city;\nSELECT\n    city,\n    COUNT(*) AS transaction_count\nFROM sales_analysis\nGROUP BY city\nHAVING COUNT(*) &gt; 1000;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#analytical-having-example-2023",
    "href": "materials/sql/slides/session4.html#analytical-having-example-2023",
    "title": "Filtering",
    "section": "Analytical HAVING Example (2023)",
    "text": "Analytical HAVING Example (2023)\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY category\nHAVING SUM(total_sales) &gt; 500000\nORDER BY total_sales_amount DESC;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#detect-duplicates-with-having",
    "href": "materials/sql/slides/session4.html#detect-duplicates-with-having",
    "title": "Filtering",
    "section": "Detect Duplicates with HAVING",
    "text": "Detect Duplicates with HAVING\nSELECT\n    transaction_id,\n    COUNT(*) AS occurrence_count\nFROM sales_analysis\nGROUP BY transaction_id\nHAVING COUNT(*) &gt; 1;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#having-vs-where-interview-trap",
    "href": "materials/sql/slides/session4.html#having-vs-where-interview-trap",
    "title": "Filtering",
    "section": "HAVING vs WHERE (Interview Trap)",
    "text": "HAVING vs WHERE (Interview Trap)\n\n\n\n\n\n\nWarning\n\n\nFiltering raw columns belongs in WHERE.\nHAVING is for aggregated conditions.\n\n\n\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nGROUP BY category\nHAVING category = 'Electronics';\nSELECT\n    category,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE category = 'Electronics'\nGROUP BY category;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#case-when-derived-categories",
    "href": "materials/sql/slides/session4.html#case-when-derived-categories",
    "title": "Filtering",
    "section": "CASE WHEN: Derived Categories",
    "text": "CASE WHEN: Derived Categories\n\n\nCreate analytical labels\nTranslate numeric logic into business meaning",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#case-transaction-segmentation",
    "href": "materials/sql/slides/session4.html#case-transaction-segmentation",
    "title": "Filtering",
    "section": "CASE: Transaction Segmentation",
    "text": "CASE: Transaction Segmentation\nSELECT\n    transaction_id,\n    total_sales,\n    CASE\n        WHEN total_sales &gt;= 100000 THEN 'High Value'\n        WHEN total_sales &gt;= 50000 THEN 'Medium Value'\n        ELSE 'Low Value'\n    END AS sales_segment\nFROM sales_analysis;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session4.html#case-with-aggregation",
    "href": "materials/sql/slides/session4.html#case-with-aggregation",
    "title": "Filtering",
    "section": "CASE with Aggregation",
    "text": "CASE with Aggregation\nSELECT\n    CASE\n        WHEN discount &gt; 0 THEN 'Discounted'\n        ELSE 'Full Price'\n    END AS pricing_type,\n    SUM(total_sales) AS total_sales_amount\nFROM sales_analysis\nWHERE year = 2023\nGROUP BY pricing_type;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Filtering"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#what-is-a-database",
    "href": "materials/sql/slides/session1.html#what-is-a-database",
    "title": "Intro to Relational Databases",
    "section": "What is a Database",
    "text": "What is a Database\nA database is a collection of stored data, organized in tables of rows and columns, and managed by a database management system (DBMS).\nAnalysts use ‚Äúdatabase‚Äù to refer collectively to:\n\nThe stored data\n\nThe DBMS\n\nThe related tools used to access and manipulate that data",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#tables-rows-columns",
    "href": "materials/sql/slides/session1.html#tables-rows-columns",
    "title": "Intro to Relational Databases",
    "section": "Tables, Rows, Columns",
    "text": "Tables, Rows, Columns\nA table consists of:\n\nRows: represent entities\n\nColumns: represent attributes\n\nCells: intersection of row √ó column containing a single data element\n\nA data element may be:\n\nNumeric\n\nText\n\nOr any other data type supported by the DBMS",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#example-table",
    "href": "materials/sql/slides/session1.html#example-table",
    "title": "Intro to Relational Databases",
    "section": "Example Table",
    "text": "Example Table\n\n\n\n\n\nCol A\nCol B\nCol C\n\n\n\n\nRow 1\nA1\nB1\nC1\n\n\nRow 2\nA2\nB2 (Cell) \nC2\n\n\nRow 3\nA3\nB3\nC3",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#why-this-structure-matters",
    "href": "materials/sql/slides/session1.html#why-this-structure-matters",
    "title": "Intro to Relational Databases",
    "section": "Why This Structure Matters",
    "text": "Why This Structure Matters\n\nPredictable row‚Äìcolumn structure\n\nEach column has a predefined data type\n\nEnsures data quality and consistency\n\nEnables efficient search, filtering, and querying\n\nForms the foundation of SQL",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#spreadsheets-vs-databases",
    "href": "materials/sql/slides/session1.html#spreadsheets-vs-databases",
    "title": "Intro to Relational Databases",
    "section": "Spreadsheets vs Databases",
    "text": "Spreadsheets vs Databases\n\n\n\n\n\n\n\n\nFeature\nSpreadsheet\nDatabase\n\n\n\n\nUser Access\nSingle-user\nMulti-user, concurrent access\n\n\nData Capacity\nLimited (thousands to ~1M rows)\nMassive (millions to billions of records)\n\n\nPerformance\nLoads entire file into memory\nRetrieves only required data\n\n\nData Operations\nBasic to moderate\nBasic to highly complex (joins, aggregations)\n\n\nQuerying\nManual filtering/sorting\nSQL queries, fast and optimized\n\n\nScalability\nLow\nHigh (vertical & horizontal scaling)\n\n\nReliability\nProne to errors and file corruption\nACID-compliant, transaction-safe\n\n\nCollaboration\nDifficult; conflicts possible\nSeamless concurrent usage\n\n\nAutomation\nLimited\nHigh; integrates with scripts and applications\n\n\nData Integrity\nMinimal enforcement\nStrong data types, constraints, relationships",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#example-actor-table",
    "href": "materials/sql/slides/session1.html#example-actor-table",
    "title": "Intro to Relational Databases",
    "section": "Example Actor Table",
    "text": "Example Actor Table\n\n\n\nactor_id\nfirst_name\nlast_name\nlast_update\n\n\n\n\n1\nPenelope\nGuiness\n2013-05-26 14:47:57.62\n\n\n2\nNick\nWahlberg\n2013-05-26 14:47:57.62\n\n\n3\nEd\nChase\n2013-05-26 14:47:57.62\n\n\n4\nJennifer\nDavis\n2013-05-26 14:47:57.62\n\n\n5\nJohnny\nLollobrigida\n2013-05-26 14:47:57.62\n\n\n\n\nFilter first_name = 'Ed' manually\n\nCount filtered rows manually\n\nSlow and memory-heavy with large data\n\nOnly one query at a time\n\n\\[\\downarrow\\]\nSELECT COUNT(*)\nFROM actor\nWHERE first_name = 'Ed';",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#structured-vs-unstructured-vs-semi-structured-data",
    "href": "materials/sql/slides/session1.html#structured-vs-unstructured-vs-semi-structured-data",
    "title": "Intro to Relational Databases",
    "section": "Structured vs Unstructured vs Semi-Structured Data",
    "text": "Structured vs Unstructured vs Semi-Structured Data\nTo understand database types, we must first distinguish how data can be organized. There are three major categories:\n\nStructured data\n\nUnstructured data\n\nSemi-structured data",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#structured-data",
    "href": "materials/sql/slides/session1.html#structured-data",
    "title": "Intro to Relational Databases",
    "section": "Structured Data",
    "text": "Structured Data\nHighly organized and stored in a tabular format.\nCharacteristics:\n\nRows represent entities\n\nColumns represent attributes\n\nEvery cell contains a single value\n\nEach column has a predefined data type (TEXT, INTEGER, NUMERIC, etc.)\n\nExample:\n\n\n\n\nactor_id\nfirst_name\nlast_name\nlast_update\n\n\n\n\n1\nPenelope\nGuiness\n2013-05-26 14:47:57.62\n\n\n2\nNick\nWahlberg\n2013-05-26 14:47:57.62\n\n\n3\nEd\nChase\n2013-05-26 14:47:57.62\n\n\n4\nJennifer\nDavis\n2013-05-26 14:47:57.62\n\n\n5\nJohnny\nLollobrigida\n2013-05-26 14:47:57.62",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#unstructured-data",
    "href": "materials/sql/slides/session1.html#unstructured-data",
    "title": "Intro to Relational Databases",
    "section": "Unstructured Data",
    "text": "Unstructured Data\nData with no fixed model, no rows, no columns.\nExamples:\n\nEmails\n\nPDFs\n\nText documents\n\nImages, graphics\n\nVideos, audio\n\nSocial media posts\n\nProperties:\n\nHarder to search and process\n\nRequires specialized tools\n\nOften free-form and unpredictable",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#semi-structured-data",
    "href": "materials/sql/slides/session1.html#semi-structured-data",
    "title": "Intro to Relational Databases",
    "section": "Semi-Structured Data",
    "text": "Semi-Structured Data\nData that is not tabular, yet still contains organizational markers.\nExamples:\n\nHTML\n\nJSON\n\nXML\n\nLog files\n\nKey‚Äìvalue records\n\nProperties:\n\nMore flexible than structured\n\nMore organized than unstructured\n\nStructure is embedded through tags, nesting, or key‚Äìvalue rules\n\nExample (JSON):\n{\n  \"user_id\": 1001,\n  \"name\": \"Maria\",\n  \"roles\": [\"admin\", \"manager\"]\n}",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#centralized-vs-distributed",
    "href": "materials/sql/slides/session1.html#centralized-vs-distributed",
    "title": "Intro to Relational Databases",
    "section": "Centralized VS Distributed",
    "text": "Centralized VS Distributed\n\n\n\n\nCentralized vs distributed database diagram",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session1.html#data-management-architectures",
    "href": "materials/sql/slides/session1.html#data-management-architectures",
    "title": "Intro to Relational Databases",
    "section": "Data Management Architectures",
    "text": "Data Management Architectures",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Intro to Relational Databases"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#functions",
    "href": "materials/sql/slides/session5.html#functions",
    "title": "Numeric Functions",
    "section": "Functions",
    "text": "Functions\nWe are going to cover the Built-in SQL functions which will deal with the \n\nNumbers\nTexts\nDates\nData Types",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#run-docker",
    "href": "materials/sql/slides/session5.html#run-docker",
    "title": "Numeric Functions",
    "section": "Run Docker",
    "text": "Run Docker\nOpen Docker Desktop, Run and open Pgadmin:\ndocker compose up -d",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#create-sales_analysis",
    "href": "materials/sql/slides/session5.html#create-sales_analysis",
    "title": "Numeric Functions",
    "section": "Create sales_analysis",
    "text": "Create sales_analysis\nDROP TABLE IF EXISTS sales_analysis;\n\nCREATE TABLE sales_analysis AS\nSELECT\n    s.transaction_id,\n\n    o.order_date,\n    DATE(o.order_date) AS order_date_date,\n    o.year,\n    o.quarter,\n    o.month,\n\n    c.customer_name,\n    c.city,\n    c.zip_code,\n\n    p.product_name,\n    p.category,\n    p.price,\n\n    e.first_name AS employee_first_name,\n    e.last_name  AS employee_last_name,\n    e.salary     AS employee_salary,\n\n    s.quantity,\n    s.discount,\n    s.total_sales\nFROM sales AS s\nJOIN orders AS o\n    ON s.order_id = o.order_id\nJOIN customers AS c\n    ON s.customer_id = c.customer_id\nJOIN products AS p\n    ON s.product_id = p.product_id\nLEFT JOIN employees AS e\n    ON s.employee_id = e.employee_id;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#indexes-for-filtering-performance",
    "href": "materials/sql/slides/session5.html#indexes-for-filtering-performance",
    "title": "Numeric Functions",
    "section": "Indexes for Filtering Performance",
    "text": "Indexes for Filtering Performance\nCREATE INDEX idx_sales_analysis_order_date\n    ON sales_analysis(order_date_date);\n\nCREATE INDEX idx_sales_analysis_year\n    ON sales_analysis(year);\n\nCREATE INDEX idx_sales_analysis_city\n    ON sales_analysis(city);\n\nCREATE INDEX idx_sales_analysis_category\n    ON sales_analysis(category);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#what-is-a-function",
    "href": "materials/sql/slides/session5.html#what-is-a-function",
    "title": "Numeric Functions",
    "section": "What is a Function ?",
    "text": "What is a Function ?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#sum-basic",
    "href": "materials/sql/slides/session5.html#sum-basic",
    "title": "Numeric Functions",
    "section": "SUM() | Basic",
    "text": "SUM() | Basic\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\nNULL\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntotal_revenue\n\n\n\n\n220.00",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#sum-with-group-by",
    "href": "materials/sql/slides/session5.html#sum-with-group-by",
    "title": "Numeric Functions",
    "section": "SUM() | with GROUP BY",
    "text": "SUM() | with GROUP BY\n\n\n\ncategory\ntotal_sales\n\n\n\n\nAccessories\n50.00\n\n\nAccessories\n70.00\n\n\nElectronics\n120.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  category,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY category;\n\\[\\downarrow\\]\n\n\n\ncategory\ntotal_revenue\n\n\n\n\nAccessories\n120.00\n\n\nElectronics\n120.00",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#sum-use-cases",
    "href": "materials/sql/slides/session5.html#sum-use-cases",
    "title": "Numeric Functions",
    "section": "SUM() | Use Cases",
    "text": "SUM() | Use Cases\n\nSimple\n\ntotal revenue calculation\n\ntotal cost or volume\n\nhigh-level KPI reporting\n\n\nAggregated\n\nrevenue by product category\n\ncost by department\n\nKPI breakdowns for dashboards",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#sum-attention",
    "href": "materials/sql/slides/session5.html#sum-attention",
    "title": "Numeric Functions",
    "section": "SUM() | ATTENTION",
    "text": "SUM() | ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | Always Check the Grain\n\n\nBefore using SUM(), always verify the grain of the table.\n\nsales_analysis is at the transaction level\n\nsumming values duplicated per transaction (e.g.¬†salaries) produces incorrect results\n\naggregation must match the entity you want to measure\n\nCorrect use of SUM() depends as much on data structure as on SQL syntax.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-what-it-does",
    "href": "materials/sql/slides/session5.html#avg-what-it-does",
    "title": "Numeric Functions",
    "section": "AVG() | What It Does?",
    "text": "AVG() | What It Does?\n\nAVG() calculates the arithmetic mean of numeric values.\nIt answers the question: \\(\\rightarrow\\) ‚ÄúWhat is the typical value?‚Äù",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-basic",
    "href": "materials/sql/slides/session5.html#avg-basic",
    "title": "Numeric Functions",
    "section": "AVG() | Basic",
    "text": "AVG() | Basic\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\n0.00\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  AVG(discount) AS avg_discount\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\navg_discount\n\n\n\n\n0.10",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-null-behavior",
    "href": "materials/sql/slides/session5.html#avg-null-behavior",
    "title": "Numeric Functions",
    "section": "AVG() | NULL Behavior",
    "text": "AVG() | NULL Behavior\nAVG() ignores NULL values.\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  AVG(discount) AS avg_discount\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\n\navg_discount\n\n\n\n\n0.15",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-why-this-happens",
    "href": "materials/sql/slides/session5.html#avg-why-this-happens",
    "title": "Numeric Functions",
    "section": "AVG() | Why This Happens?",
    "text": "AVG() | Why This Happens?\nWhat AVG() actually computes:\n\\[\n\\frac{\\text{SUM of non-NULL values}}{\\text{COUNT of non-NULL values}}\n\\]\nIn this case:\n\\[\n\\frac{0.10 + 0.20}{2} = 0.15\n\\]",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-with-group-by",
    "href": "materials/sql/slides/session5.html#avg-with-group-by",
    "title": "Numeric Functions",
    "section": "AVG() | with GROUP BY",
    "text": "AVG() | with GROUP BY\n\n\n\ncategory\nprice\n\n\n\n\nAccessories\n25.00\n\n\nAccessories\n35.00\n\n\nElectronics\n120.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  category,\n  AVG(price) AS avg_price\nFROM sales_analysis\nGROUP BY category;\n\\[\\downarrow\\]\n\n\n\ncategory\navg_price\n\n\n\n\nAccessories\n30.00\n\n\nElectronics\n120.00",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-per-entity-analysis",
    "href": "materials/sql/slides/session5.html#avg-per-entity-analysis",
    "title": "Numeric Functions",
    "section": "AVG() | Per-Entity Analysis",
    "text": "AVG() | Per-Entity Analysis\nThis shifts analysis from global to behavioral.\nSELECT\n  customer_name,\n  AVG(total_sales) AS avg_transaction_value\nFROM sales_analysis\nGROUP BY customer_name;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-use-cases",
    "href": "materials/sql/slides/session5.html#avg-use-cases",
    "title": "Numeric Functions",
    "section": "AVG() | Use Cases",
    "text": "AVG() | Use Cases\n\nSimple\n\naverage order value\n\naverage discount rate\n\naverage price\n\nAggregated\n\naverage sales per customer\n\naverage price by category\n\nbenchmarking typical performance",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#avg-attention",
    "href": "materials/sql/slides/session5.html#avg-attention",
    "title": "Numeric Functions",
    "section": "AVG() | ATTENTION",
    "text": "AVG() | ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | Distribution Matters\n\n\nThe Average is sensitive to outliers:**\n\none extreme value can distort the result\n\nskewed data leads to misleading averages\n\nBefore using AVG(), always consider:\n\noutliers\n\nskewness\n\nwhether a ‚Äútypical‚Äù value makes sense",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#section",
    "href": "materials/sql/slides/session5.html#section",
    "title": "Numeric Functions",
    "section": "‚Ä¶",
    "text": "‚Ä¶\ninterpretation matters more than syntax",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-what-they-do",
    "href": "materials/sql/slides/session5.html#min-and-max-what-they-do",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | What They Do?",
    "text": "MIN() and MAX() | What They Do?\nMIN() and MAX() are aggregate functions used to identify extreme values in a dataset.\nThey answer:\n\nWhat is the smallest value?\nWhat is the largest value?\n\nCommonly used in:\n\nexploratory analysis\ndata validation\nboundary checks",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-basic",
    "href": "materials/sql/slides/session5.html#min-and-max-basic",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | Basic",
    "text": "MIN() and MAX() | Basic\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\n80.00\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  MIN(total_sales) AS min_sale,\n  MAX(total_sales) AS max_sale\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nmin_sale\nmax_sale\n\n\n\n\n80.00\n120.50",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-null-behavior",
    "href": "materials/sql/slides/session5.html#min-and-max-null-behavior",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | NULL Behavior",
    "text": "MIN() and MAX() | NULL Behavior\nBoth MIN() and MAX() ignore NULL values.\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.50\n\n\n1002\nNULL\n\n\n1003\n99.50\n\n\n\n\\[\\downarrow\\]\nSELECT\n  MIN(total_sales) AS min_sale,\n  MAX(total_sales) AS max_sale\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nmin_sale\nmax_sale\n\n\n\n\n99.50\n120.50",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-with-group-by",
    "href": "materials/sql/slides/session5.html#min-and-max-with-group-by",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | with GROUP BY",
    "text": "MIN() and MAX() | with GROUP BY\n\n\n\ncategory\nprice\n\n\n\n\nAccessories\n25.00\n\n\nAccessories\n35.00\n\n\nElectronics\n120.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  category,\n  MIN(price) AS min_price,\n  MAX(price) AS max_price\nFROM sales_analysis\nGROUP BY category;\n\\[\\downarrow\\]\n\n\n\ncategory\nmin_price\nmax_price\n\n\n\n\nAccessories\n25.00\n35.00\n\n\nElectronics\n120.00\n120.00",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-text-columns",
    "href": "materials/sql/slides/session5.html#min-and-max-text-columns",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | Text Columns",
    "text": "MIN() and MAX() | Text Columns\n\n\n\nproduct_name\n\n\n\n\nCable\n\n\nMouse\n\n\nKeyboard\n\n\n\n\\[\\downarrow\\]\nSELECT\n  MIN(product_name) AS first_product,\n  MAX(product_name) AS last_product\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nfirst_product\nlast_product\n\n\n\n\nCable\nMouse",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-use-cases",
    "href": "materials/sql/slides/session5.html#min-and-max-use-cases",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | Use Cases",
    "text": "MIN() and MAX() | Use Cases\n\nNumeric\n\ncheapest and most expensive products\nminimum and maximum sales values\nboundary validation\n\nText / Dates\n\nfirst and last records\nalphabetical boundary checks\nearliest and latest dates",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#min-and-max-attention",
    "href": "materials/sql/slides/session5.html#min-and-max-attention",
    "title": "Numeric Functions",
    "section": "MIN() and MAX() | ATTENTION",
    "text": "MIN() and MAX() | ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | Extremes Are Not Typical\n\n\nMIN() and MAX() show boundaries, not typical behavior:\n\nthey are sensitive to outliers\na single abnormal value can dominate the result\n\nAlways combine extremes with:\n\nAVG()\nCOUNT()\ndistribution analysis\n\nto get a reliable analytical picture.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#section-1",
    "href": "materials/sql/slides/session5.html#section-1",
    "title": "Numeric Functions",
    "section": "‚Ä¶",
    "text": "‚Ä¶\nMIN() and MAX() define limits, not reality",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#count-what-it-does",
    "href": "materials/sql/slides/session5.html#count-what-it-does",
    "title": "Numeric Functions",
    "section": "COUNT() | What It Does",
    "text": "COUNT() | What It Does\nCOUNT() measures how many rows or values exist.\nIt is used for:\n\nvolume\ncardinality\ndata completeness\n\nUnlike SUM() or AVG(), it does not depend on numeric magnitude.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#count-basic-row-counting",
    "href": "materials/sql/slides/session5.html#count-basic-row-counting",
    "title": "Numeric Functions",
    "section": "COUNT() | Basic Row Counting",
    "text": "COUNT() | Basic Row Counting\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  COUNT(*) AS total_rows\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntotal_rows\n\n\n\n\n3",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#countcolumn-non-null-values",
    "href": "materials/sql/slides/session5.html#countcolumn-non-null-values",
    "title": "Numeric Functions",
    "section": "COUNT(column) | Non-NULL Values",
    "text": "COUNT(column) | Non-NULL Values\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  COUNT(discount) AS non_null_discounts\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\nnon_null_discounts\n\n\n\n\n2",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#count-with-group-by",
    "href": "materials/sql/slides/session5.html#count-with-group-by",
    "title": "Numeric Functions",
    "section": "COUNT() | with GROUP BY",
    "text": "COUNT() | with GROUP BY\n\n\n\nproduct_name\ntransaction_id\n\n\n\n\nMouse\n1001\n\n\nMouse\n1002\n\n\nKeyboard\n1003\n\n\n\n\\[\\downarrow\\]\nSELECT\n  product_name,\n  COUNT(*) AS transaction_count\nFROM sales_analysis\nGROUP BY product_name;\n\\[\\downarrow\\]\n\n\n\nproduct_name\ntransaction_count\n\n\n\n\nMouse\n2\n\n\nKeyboard\n1",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#countdistinct-unique-values",
    "href": "materials/sql/slides/session5.html#countdistinct-unique-values",
    "title": "Numeric Functions",
    "section": "COUNT(DISTINCT) | Unique Values",
    "text": "COUNT(DISTINCT) | Unique Values\nTo answer:\n\nhow many unique customers exist?\nhow many unique entities are present?\n\n\nSELECT\n  COUNT(DISTINCT customer_name) AS unique_customers\nFROM sales_analysis;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#countdistinct-with-group-by",
    "href": "materials/sql/slides/session5.html#countdistinct-with-group-by",
    "title": "Numeric Functions",
    "section": "COUNT(DISTINCT) | with GROUP BY",
    "text": "COUNT(DISTINCT) | with GROUP BY\nNumber of Customers per City?\n\nSELECT\n  city,\n  COUNT(DISTINCT customer_name) AS unique_customers\nFROM sales_analysis\nGROUP BY city;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#count-duplicate-detection",
    "href": "materials/sql/slides/session5.html#count-duplicate-detection",
    "title": "Numeric Functions",
    "section": "COUNT() | Duplicate Detection",
    "text": "COUNT() | Duplicate Detection\nSELECT\n  transaction_id,\n  COUNT(*) AS duplicate_count\nFROM sales_analysis\nGROUP BY transaction_id\nHAVING COUNT(*) &gt; 1;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#count-null-rules",
    "href": "materials/sql/slides/session5.html#count-null-rules",
    "title": "Numeric Functions",
    "section": "COUNT() | NULL Rules",
    "text": "COUNT() | NULL Rules\n\nCOUNT(*) counts rows (NULLs included)\nCOUNT(column) ignores NULLs\nCOUNT(DISTINCT column) ignores NULLs and duplicates\nall NULL values ‚Üí result is 0\n\nUnderstanding this prevents silent metric errors.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#count-attention",
    "href": "materials/sql/slides/session5.html#count-attention",
    "title": "Numeric Functions",
    "section": "COUNT() | ATTENTION",
    "text": "COUNT() | ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | Define What You Are Counting\n\n\nBefore using COUNT(), always ask:\n\nam I counting rows or entities?\ndo I need DISTINCT?\nis the table grain aligned with my question?",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#section-2",
    "href": "materials/sql/slides/session5.html#section-2",
    "title": "Numeric Functions",
    "section": "‚Ä¶",
    "text": "‚Ä¶\nCorrect syntax does not guarantee correct analytics",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#what-is-row-level-arithmetic",
    "href": "materials/sql/slides/session5.html#what-is-row-level-arithmetic",
    "title": "Numeric Functions",
    "section": "What Is Row-Level Arithmetic?",
    "text": "What Is Row-Level Arithmetic?\nRow-level arithmetic refers to calculations performed on each individual row of a table.\nUnlike aggregate functions (SUM, AVG, COUNT):\n\nrows are not reduced\nnew derived columns are created",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#why-row-level-arithmetic-matters",
    "href": "materials/sql/slides/session5.html#why-row-level-arithmetic-matters",
    "title": "Numeric Functions",
    "section": "Why Row-Level Arithmetic Matters",
    "text": "Why Row-Level Arithmetic Matters\nRow-level arithmetic is foundational for:\n\nfeature engineering\nrevenue and cost calculations\nmetric normalization\npreparing data for aggregation\n\nIn practice:\nrow-level calculations ‚Üí aggregation",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#basic-calculation",
    "href": "materials/sql/slides/session5.html#basic-calculation",
    "title": "Numeric Functions",
    "section": "Basic Calculation",
    "text": "Basic Calculation\n\n\n\ntransaction_id\nquantity\nprice\n\n\n\n\n1001\n2\n25.00\n\n\n1002\n1\n80.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  transaction_id,\n  quantity * price AS calculated_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntransaction_id\ncalculated_revenue\n\n\n\n\n1001\n50.00\n\n\n1002\n80.00",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#combining-multiple-columns",
    "href": "materials/sql/slides/session5.html#combining-multiple-columns",
    "title": "Numeric Functions",
    "section": "Combining Multiple Columns",
    "text": "Combining Multiple Columns\n\n\n\ntransaction_id\nquantity\nprice\ndiscount\n\n\n\n\n1001\n2\n25.00\n0.10\n\n\n1002\n1\n80.00\n0.00\n\n\n\n\\[\\downarrow\\]\nSELECT\n  transaction_id,\n  quantity * price * (1 - discount) AS net_revenue\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntransaction_id\nnet_revenue\n\n\n\n\n1001\n45.00\n\n\n1002\n80.00",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#order-of-operations",
    "href": "materials/sql/slides/session5.html#order-of-operations",
    "title": "Numeric Functions",
    "section": "Order of Operations",
    "text": "Order of Operations\nSQL follows standard arithmetic precedence:\n\nparentheses ()\nmultiplication * and division /\naddition + and subtraction -\n\nUse parentheses to make logic explicit.\nSELECT\n  transaction_id,\n  (quantity * price) - (quantity * price * discount) AS net_revenue\nFROM sales_analysis;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#numeric-transformations",
    "href": "materials/sql/slides/session5.html#numeric-transformations",
    "title": "Numeric Functions",
    "section": "Numeric Transformations",
    "text": "Numeric Transformations\nRow-level arithmetic is often combined with numeric functions.\nSELECT\n  transaction_id,\n  ROUND(quantity * price, 2) AS rounded_revenue\nFROM sales_analysis;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#row-level-vs-aggregation",
    "href": "materials/sql/slides/session5.html#row-level-vs-aggregation",
    "title": "Numeric Functions",
    "section": "Row-Level vs Aggregation",
    "text": "Row-Level vs Aggregation\nTwo distinct concepts:\n\nrow-level calculations ‚Üí preserve rows\naggregate calculations ‚Üí reduce rows\n\nSELECT\n  SUM(quantity * price) AS total_revenue\nFROM sales_analysis;\nHere:\n\ncalculation happens per row\naggregation happens after",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#attention",
    "href": "materials/sql/slides/session5.html#attention",
    "title": "Numeric Functions",
    "section": "ATTENTION",
    "text": "ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | NULL Propagation\n\n\nIn arithmetic expressions:\n\nif any operand is NULL, the result is NULL\nNULLs can silently remove rows from aggregations",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#section-3",
    "href": "materials/sql/slides/session5.html#section-3",
    "title": "Numeric Functions",
    "section": "‚Ä¶",
    "text": "‚Ä¶\nRow-level arithmetic is the bridge between raw data and analytics",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#what-are-ceiling-and-floor",
    "href": "materials/sql/slides/session5.html#what-are-ceiling-and-floor",
    "title": "Numeric Functions",
    "section": "What Are CEILING() and FLOOR()?",
    "text": "What Are CEILING() and FLOOR()?\nCEILING() and FLOOR() are numeric functions used to round values to integer boundaries.\n\nCEILING() rounds a value up to the nearest integer\n\nFLOOR() rounds a value down to the nearest integer\n\nThey are commonly used for:\n\nthreshold logic\nrange construction\ndistribution analysis",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#thresholds-and-ranges-input",
    "href": "materials/sql/slides/session5.html#thresholds-and-ranges-input",
    "title": "Numeric Functions",
    "section": "Thresholds and Ranges | Input",
    "text": "Thresholds and Ranges | Input\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n120.10\n\n\n1002\n80.90\n\n\n\n\\[\\downarrow\\]\nSELECT\n  transaction_id,\n  CEILING(total_sales) AS rounded_up,\n  FLOOR(total_sales)   AS rounded_down\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\ntransaction_id\nrounded_up\nrounded_down\n\n\n\n\n1001\n121\n120\n\n\n1002\n81\n80",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#thresholds-and-ranges-interpretation",
    "href": "materials/sql/slides/session5.html#thresholds-and-ranges-interpretation",
    "title": "Numeric Functions",
    "section": "Thresholds and Ranges | Interpretation",
    "text": "Thresholds and Ranges | Interpretation\n\nCEILING() is useful when values must meet a minimum threshold\nFLOOR() is useful when values must not exceed a maximum threshold",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-and-floor-use-cases",
    "href": "materials/sql/slides/session5.html#ceiling-and-floor-use-cases",
    "title": "Numeric Functions",
    "section": "CEILING() and FLOOR() | Use Cases",
    "text": "CEILING() and FLOOR() | Use Cases\n\nbilling thresholds (minimum chargeable units)\nrounding prices or quantities for operational rules\nconstructing numeric ranges\ndiscretizing continuous values",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY",
    "text": "CEILING() with GROUP BY\nCEILING() can be used to group continuous numeric values into fixed-width ranges.\nThis is a common technique in:\n\nexploratory analysis\ndescriptive analytics\ndashboard preparation",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by-input",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by-input",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY | Input",
    "text": "CEILING() with GROUP BY | Input\n\n\n\ntransaction_id\ntotal_sales\n\n\n\n\n1001\n12.40\n\n\n1002\n47.80\n\n\n1003\n52.10\n\n\n1004\n79.90\n\n\n1005\n101.25\n\n\n1006\n138.60",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by-output",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by-output",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY | Output",
    "text": "CEILING() with GROUP BY | Output\n\n\n\ntransaction_id\ntotal_sales\nrevenue_range (50)\n\n\n\n\n1001\n12.40\n50\n\n\n1002\n47.80\n50\n\n\n1003\n52.10\n100\n\n\n1004\n79.90\n100\n\n\n1005\n101.25\n150\n\n\n1006\n138.60\n150",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by-range-logic",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by-range-logic",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY | Range Logic",
    "text": "CEILING() with GROUP BY | Range Logic\nRange assignment formula:\n\\[\n\\text{revenue_range} = \\lceil \\frac{\\text{total_sales}}{50} \\rceil \\times 50\n\\]\nThis means:\n\n\\(0 &lt; x \\le 50 \\rightarrow 50\\)\n\\(50 &lt; x \\le 100 \\rightarrow 100\\)\n\\(100 &lt; x \\le 150 \\rightarrow 150\\)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by-query",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by-query",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY | Query",
    "text": "CEILING() with GROUP BY | Query\nSELECT\n  CEILING(total_sales / 50.0) * 50 AS revenue_range,\n  COUNT(*) AS transactions,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY CEILING(total_sales / 50.0) * 50\nORDER BY revenue_range;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#revenue-ranges-intermediate-mapping",
    "href": "materials/sql/slides/session5.html#revenue-ranges-intermediate-mapping",
    "title": "Numeric Functions",
    "section": "Revenue Ranges | Intermediate Mapping",
    "text": "Revenue Ranges | Intermediate Mapping\n\n\n\ntransaction_id\ntotal_sales\nrevenue_range\n\n\n\n\n1001\n12.40\n50\n\n\n1002\n47.80\n50\n\n\n1003\n52.10\n100\n\n\n1004\n79.90\n100\n\n\n1005\n101.25\n150\n\n\n1006\n138.60\n150",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by-query-output",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by-query-output",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY | Query Output",
    "text": "CEILING() with GROUP BY | Query Output\n\n\n\nrevenue_range\ntransactions\ntotal_revenue\n\n\n\n\n50\n2\n60.20\n\n\n100\n2\n132.00\n\n\n150\n2\n239.85",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#ceiling-with-group-by-interpretation",
    "href": "materials/sql/slides/session5.html#ceiling-with-group-by-interpretation",
    "title": "Numeric Functions",
    "section": "CEILING() with GROUP BY | Interpretation",
    "text": "CEILING() with GROUP BY | Interpretation\nEach row represents a revenue range, not individual transactions.\n\nrevenue_range = 100 represents\n\\[50 &lt; \\text{total\\_sales} \\le 100\\]\ntransactions shows how many transactions fall into that range\ntotal_revenue shows the total sales within the range",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#revenue-ranges-attention",
    "href": "materials/sql/slides/session5.html#revenue-ranges-attention",
    "title": "Numeric Functions",
    "section": "Revenue Ranges | ATTENTION",
    "text": "Revenue Ranges | ATTENTION\n\n\n\n\n\n\n\nAnalytical Warning | Ranges Are Design Choices\n\n\nRanges created with CEILING() or FLOOR() are analytical assumptions.\n\ndifferent range widths lead to different interpretations\noverly wide ranges hide structure\noverly narrow ranges introduce noise\n\nAlways justify:\n\nrange size\nboundary logic\nbusiness meaning",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#section-4",
    "href": "materials/sql/slides/session5.html#section-4",
    "title": "Numeric Functions",
    "section": "‚Ä¶",
    "text": "‚Ä¶\nCEILING() and FLOOR() help transform continuous values into\ninterpretable analytical structures given the fact that ranges are chosen deliberately",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#coalesce-what-it-does",
    "href": "materials/sql/slides/session5.html#coalesce-what-it-does",
    "title": "Numeric Functions",
    "section": "COALESCE() | What It Does",
    "text": "COALESCE() | What It Does\nCOALESCE() is a SQL function used for explicit NULL handling.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#coalesce-why-it-matters",
    "href": "materials/sql/slides/session5.html#coalesce-why-it-matters",
    "title": "Numeric Functions",
    "section": "COALESCE() | Why It Matters",
    "text": "COALESCE() | Why It Matters\nBy default, most aggregate functions ignore NULL values.\n\n\n\n\n\n\nNote\n\n\nCOALESCE() lets you override this behavior intentionally.\n\n\n\nCommon strategies include:\n\nreplacing NULL with 0\nreplacing NULL with the average\nreplacing NULL with the median",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#treat-missing-as-zero-input",
    "href": "materials/sql/slides/session5.html#treat-missing-as-zero-input",
    "title": "Numeric Functions",
    "section": "Treat Missing as Zero | Input",
    "text": "Treat Missing as Zero | Input\n\n\n\ntransaction_id\ndiscount\n\n\n\n\n1001\n0.10\n\n\n1002\nNULL\n\n\n1003\n0.20\n\n\n\n\\[\\downarrow\\]\nSELECT\n  AVG(COALESCE(discount, 0)) AS avg_discount_with_zeros\nFROM sales_analysis;\n\\[\\downarrow\\]\n\n\n\navg_discount_with_zeros\n\n\n\n\n0.10",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#treat-missing-as-zero-interpretation",
    "href": "materials/sql/slides/session5.html#treat-missing-as-zero-interpretation",
    "title": "Numeric Functions",
    "section": "Treat Missing as Zero | Interpretation",
    "text": "Treat Missing as Zero | Interpretation\n\\[\n(0.10 + 0 + 0.20) / 3 = 0.10\n\\]\nWhen to Use\n\nNULL truly means zero impact\nKPIs explicitly require zero inclusion\noperational metrics (counts, volumes)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#mean-imputation-concept",
    "href": "materials/sql/slides/session5.html#mean-imputation-concept",
    "title": "Numeric Functions",
    "section": "Mean Imputation | Concept",
    "text": "Mean Imputation | Concept\nMissing values are replaced with the overall average\nSELECT AVG(discount) AS avg_discount\n       FROM sales_analysis\n\\[\\downarrow\\]\nSELECT\n  AVG(COALESCE(discount, avg_discount)) AS avg_discount_mean_imputed\nFROM sales_analysis;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#mean-imputation-when-to-use",
    "href": "materials/sql/slides/session5.html#mean-imputation-when-to-use",
    "title": "Numeric Functions",
    "section": "Mean Imputation | When to Use",
    "text": "Mean Imputation | When to Use\nAppropriate when:\n\n\ndistribution is approximately symmetric\nno strong outliers exist\nmissing values are random and rare\nreporting-level analysis\n\nAvoid when:\n\n\n\ndata is skewed\noutliers are present\nperformance or incentive metrics are involved",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#robust-imputation-concept",
    "href": "materials/sql/slides/session5.html#robust-imputation-concept",
    "title": "Numeric Functions",
    "section": "Robust Imputation | Concept",
    "text": "Robust Imputation | Concept\nMedian imputation is more robust than average imputation because it is not affected by outliers.\nPostgreSQL supports median via PERCENTILE_CONT(0.5).\n\\[\\downarrow\\]\nSELECT\n         PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY discount) AS median_discount\nFROM sales_analysis\n\\[\\downarrow\\]\nSELECT\n  AVG(COALESCE(discount, median_discount)) AS avg_discount_median_imputed\nFROM sales_analysis,",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#median-imputation-notes",
    "href": "materials/sql/slides/session5.html#median-imputation-notes",
    "title": "Numeric Functions",
    "section": "Median Imputation | Notes",
    "text": "Median Imputation | Notes\nWhen to Use\n\n\nskewed distributions\npresence of outliers\nfinancial or behavioral metrics\nfairness-sensitive analysis\n\nWhen NOT to Use\n\n\n\nvery small datasets\nstrict business rules\noperational counting logic",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#practical-rule-of-thumb",
    "href": "materials/sql/slides/session5.html#practical-rule-of-thumb",
    "title": "Numeric Functions",
    "section": "Practical Rule of Thumb",
    "text": "Practical Rule of Thumb\n\naverage ‚âà median ‚Üí symmetric\n\naverage &gt; median ‚Üí right-skewed\n\naverage &lt; median ‚Üí left-skewed\n\n\n\n\n\n\n\nTip\n\n\nReview skewness concepts from\nIntro to Statistics ‚Äì Distributions",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session5.html#average-vs-median",
    "href": "materials/sql/slides/session5.html#average-vs-median",
    "title": "Numeric Functions",
    "section": "Average vs Median",
    "text": "Average vs Median\n\n\n\nScenario\nPrefer\n\n\n\n\nSymmetric distribution\nAverage\n\n\nSkewed distribution\nMedian\n\n\nPresence of outliers\nMedian\n\n\nKPI reporting\nAverage\n\n\nRobust / fairness analysis\nMedian\n\n\nModeling or downstream ML\nMedian",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Numeric Functions"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#learning-goals-1",
    "href": "materials/sql/slides/session3.html#learning-goals-1",
    "title": "Basic Queries: DDL DLM",
    "section": "Learning Goals",
    "text": "Learning Goals\nBy the end of this section, you will be able to:\n\nexplain what happens behind the scenes when a SQL query is executed\nunderstand SQL as a declarative language\ninterpret query plans at a high level\nwrite more efficient analytical queries\nuse core SQL clauses to refine query performance",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#why-behind-the-scenes-matters",
    "href": "materials/sql/slides/session3.html#why-behind-the-scenes-matters",
    "title": "Basic Queries: DDL DLM",
    "section": "Why ‚ÄúBehind the Scenes‚Äù Matters",
    "text": "Why ‚ÄúBehind the Scenes‚Äù Matters\nBefore diving into more SQL syntax, let‚Äôs pause to understand what happens behind the scenes when a SQL query is executed.\nThink about an e-commerce company, similar to our products‚Äìsales case study.\n\ninventory management\norder fulfillment\npayment processing\ndelivery optimization\n\nThe same idea applies to SQL queries.\n\n\n\n\n\n\nImportant\n\n\nWhat you write is only part of the story; how the database executes it matters just as much.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#sql-as-a-declarative-language",
    "href": "materials/sql/slides/session3.html#sql-as-a-declarative-language",
    "title": "Basic Queries: DDL DLM",
    "section": "SQL as a Declarative Language",
    "text": "SQL as a Declarative Language\nSQL is a declarative programming language.\nThis means:\n\nyou tell the database what result you want\nyou do not tell it how to get that result\n\n\n\n\n\n\n\nImportant\n\n\nThe database engine (RDBMS) decides the most efficient execution strategy.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#declarative-query-example",
    "href": "materials/sql/slides/session3.html#declarative-query-example",
    "title": "Basic Queries: DDL DLM",
    "section": "Declarative Query Example",
    "text": "Declarative Query Example\nSELECT \n  product_name\nFROM products;\nThis statement tells the database:\n\nreturn values from product_name\nread data from the products table\n\nIt does not specify:\n\nwhich index to use\nhow rows are scanned\nwhether execution is sequential or indexed\n\nAll of this is handled internally by the database engine.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#query-optimization-happens-automatically",
    "href": "materials/sql/slides/session3.html#query-optimization-happens-automatically",
    "title": "Basic Queries: DDL DLM",
    "section": "Query Optimization Happens Automatically",
    "text": "Query Optimization Happens Automatically\nWhen a query is submitted:\n\nthe database parses the SQL\nsyntax and permissions are checked\nthe query planner evaluates execution strategies\nthe optimizer selects the best plan\nresults are returned\n\nWriting efficient SQL means trusting the database, while still writing clear and well-structured queries.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#why-this-matters-for-data-analysts",
    "href": "materials/sql/slides/session3.html#why-this-matters-for-data-analysts",
    "title": "Basic Queries: DDL DLM",
    "section": "Why This Matters for Data Analysts",
    "text": "Why This Matters for Data Analysts\nAs a data analyst:\n\nyou focus on business logic and correctness\nthe database focuses on performance and execution\nclean SQL enables better optimization\nproper indexes and constraints improve speed\n\nUnderstanding this process helps you:\n\ndiagnose slow queries\nwrite scalable SQL\ncollaborate with data engineers and DBAs",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#why-query-plans-matter",
    "href": "materials/sql/slides/session3.html#why-query-plans-matter",
    "title": "Basic Queries: DDL DLM",
    "section": "Why Query Plans Matter",
    "text": "Why Query Plans Matter\nAs queries grow more complex and datasets become larger, performance matters.\nPoorly optimized queries can:\n\nslow down dashboards\nincrease infrastructure costs\ndelay decision-making\n\nFaster queries mean:\n\nlower computational cost\nbetter system performance\nquicker insights",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#what-is-a-query-plan",
    "href": "materials/sql/slides/session3.html#what-is-a-query-plan",
    "title": "Basic Queries: DDL DLM",
    "section": "What Is a Query Plan?",
    "text": "What Is a Query Plan?\nA query plan shows how the database intends to execute a query.\nMost RDBMSs (including PostgreSQL) estimate:\n\nexecution cost\nnumber of rows processed\noperations such as scans, joins, filters\n\nYou generate a query plan using EXPLAIN.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#query-plan-example",
    "href": "materials/sql/slides/session3.html#query-plan-example",
    "title": "Basic Queries: DDL DLM",
    "section": "Query Plan Example",
    "text": "Query Plan Example\nEXPLAIN\nSELECT \n  *\nFROM sales;\nPostgreSQL returns a plan, not the data.\nExample output:\nSeq Scan on sales  (cost=0.00..92.00 rows=5000 width=34)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#reading-a-simple-query-plan",
    "href": "materials/sql/slides/session3.html#reading-a-simple-query-plan",
    "title": "Basic Queries: DDL DLM",
    "section": "Reading a Simple Query Plan",
    "text": "Reading a Simple Query Plan\n\nSeq Scan ‚Üí full table scan\nsales ‚Üí table being scanned\ncost ‚Üí estimated execution cost\nrows ‚Üí estimated number of rows\nwidth ‚Üí average row size in bytes\n\nSequential scans are common when:\n\ntables are small\nno suitable index exists\nmost rows are needed",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#why-refinement-matters",
    "href": "materials/sql/slides/session3.html#why-refinement-matters",
    "title": "Basic Queries: DDL DLM",
    "section": "Why Refinement Matters",
    "text": "Why Refinement Matters\nReducing query cost often starts with requesting less data.\nThe foundation of almost every analytical query is SELECT.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#select-refresher",
    "href": "materials/sql/slides/session3.html#select-refresher",
    "title": "Basic Queries: DDL DLM",
    "section": "SELECT Refresher",
    "text": "SELECT Refresher\nBasic structure:\nSELECT \n    column_name_1,\n    column_name_2\nFROM table_name;\nExample using products:\nSELECT\n  product_name,\n  price,\n  category\nFROM products;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#why-column-selection-matters",
    "href": "materials/sql/slides/session3.html#why-column-selection-matters",
    "title": "Basic Queries: DDL DLM",
    "section": "Why Column Selection Matters",
    "text": "Why Column Selection Matters\nSelecting only needed columns:\n\nreduces data transfer\nimproves performance\nmakes results easier to interpret\n\n\n\n\n\n\n\n\nTry It Yourself\n\n\nCompare:\nEXPLAIN\nSELECT \n  *\nFROM products;\nEXPLAIN\nSELECT \n  product_name, \n  price\nFROM products;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#sorting-results",
    "href": "materials/sql/slides/session3.html#sorting-results",
    "title": "Basic Queries: DDL DLM",
    "section": "Sorting Results",
    "text": "Sorting Results\nORDER BY sorts results in ascending or descending order.\nCommon use cases:\n\nfirst or last records\nhighest or lowest values\nalphabetical ordering",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#order-by-examples",
    "href": "materials/sql/slides/session3.html#order-by-examples",
    "title": "Basic Queries: DDL DLM",
    "section": "ORDER BY Examples",
    "text": "ORDER BY Examples\nDefault (ascending):\nSELECT \n  product_name\nFROM products\nORDER BY product_name;\nDescending (Z \\(\\rightarrow\\) A):\nSELECT\n  product_name,\n  category\nFROM products\nORDER BY product_name DESC;\nNumeric sorting:\nSELECT\n  product_name,\n  price\nFROM products\nORDER BY price DESC;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#order-by-key-rules",
    "href": "materials/sql/slides/session3.html#order-by-key-rules",
    "title": "Basic Queries: DDL DLM",
    "section": "ORDER BY Key Rules",
    "text": "ORDER BY Key Rules\n\napplied after SELECT and FROM\ndefault order is ascending\nDESC reverses order\nsupports multiple columns\n\nORDER BY category ASC, price DESC;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#limiting-output-size",
    "href": "materials/sql/slides/session3.html#limiting-output-size",
    "title": "Basic Queries: DDL DLM",
    "section": "Limiting Output Size",
    "text": "Limiting Output Size\nLIMIT restricts the number of rows returned.\nUseful when:\n\npreviewing data\nreducing cost\nretrieving top-N results",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#limit-example",
    "href": "materials/sql/slides/session3.html#limit-example",
    "title": "Basic Queries: DDL DLM",
    "section": "LIMIT Example",
    "text": "LIMIT Example\nSELECT\n  product_name,\n  price\nFROM products\nORDER BY price DESC\nLIMIT 10;\n\\[\\downarrow\\]\n\nsorts by price\nreturns only 10 rows\n\n\n\n\n\n\n\nImportant\n\n\n\nLIMIT must be at the end\nvalue must be numeric",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#group-by-basics",
    "href": "materials/sql/slides/session3.html#group-by-basics",
    "title": "Basic Queries: DDL DLM",
    "section": "GROUP BY Basics",
    "text": "GROUP BY Basics\nGROUP BY groups rows sharing the same values.\nIt is commonly used with aggregate functions:\n\nCOUNT()\nSUM()\nAVG()\nMIN()\nMAX()",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#group-by-example",
    "href": "materials/sql/slides/session3.html#group-by-example",
    "title": "Basic Queries: DDL DLM",
    "section": "GROUP BY Example",
    "text": "GROUP BY Example\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id;\n\\[\\downarrow\\]\n\ngroups by product\naggregates revenue\nreturns one row per product",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#group-by-with-order-by-and-limit",
    "href": "materials/sql/slides/session3.html#group-by-with-order-by-and-limit",
    "title": "Basic Queries: DDL DLM",
    "section": "GROUP BY with ORDER BY and LIMIT",
    "text": "GROUP BY with ORDER BY and LIMIT\nReturns top 5 products by revenue.\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nORDER BY total_revenue DESC\nLIMIT 5;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#distinct-basics",
    "href": "materials/sql/slides/session3.html#distinct-basics",
    "title": "Basic Queries: DDL DLM",
    "section": "DISTINCT Basics",
    "text": "DISTINCT Basics\nDISTINCT returns unique values.\nSyntax:\nSELECT DISTINCT \n  column_name\nFROM table_name;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#distinct-examples",
    "href": "materials/sql/slides/session3.html#distinct-examples",
    "title": "Basic Queries: DDL DLM",
    "section": "DISTINCT Examples",
    "text": "DISTINCT Examples\nSELECT DISTINCT \n  category\nFROM products;\nMultiple columns:\nSELECT DISTINCT\n  category,\n  price\nFROM products;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#distinct-vs-group-by",
    "href": "materials/sql/slides/session3.html#distinct-vs-group-by",
    "title": "Basic Queries: DDL DLM",
    "section": "DISTINCT vs GROUP BY",
    "text": "DISTINCT vs GROUP BY\nBoth can return unique combinations.\nDISTINCT:\nSELECT DISTINCT\n  category,\n  price\nFROM products;\nGROUP BY:\nSELECT\n  category,\n  price\nFROM products\nGROUP BY category, price;\nUse:\n\nDISTINCT for simplicity\nGROUP BY when aggregation is needed",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#why-having-exists",
    "href": "materials/sql/slides/session3.html#why-having-exists",
    "title": "Basic Queries: DDL DLM",
    "section": "Why HAVING Exists",
    "text": "Why HAVING Exists\n\nHAVING is used to filter aggregated results \\[\\rightarrow\\] filters groups after aggregation.\nWHERE filters rows before aggregation\n\nThis distinction is critical for analytical queries.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#where-vs-having-conceptual-difference",
    "href": "materials/sql/slides/session3.html#where-vs-having-conceptual-difference",
    "title": "Basic Queries: DDL DLM",
    "section": "WHERE vs HAVING | Conceptual Difference",
    "text": "WHERE vs HAVING | Conceptual Difference\n\nWHERE \\(\\rightarrow\\) filters raw rows\nGROUP BY \\(\\rightarrow\\) creates groups\nHAVING \\(\\rightarrow\\) filters aggregated groups\n\nYou cannot use aggregate functions in WHERE.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#logical-query-execution-order",
    "href": "materials/sql/slides/session3.html#logical-query-execution-order",
    "title": "Basic Queries: DDL DLM",
    "section": "Logical Query Execution Order",
    "text": "Logical Query Execution Order\nSQL is written top-down, but executed differently.\nLogical order (simplified):\n\nFROM\nWHERE\nGROUP BY\nHAVING\nSELECT\nORDER BY\nLIMIT\n\nThis explains why HAVING exists.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#basic-having-syntax",
    "href": "materials/sql/slides/session3.html#basic-having-syntax",
    "title": "Basic Queries: DDL DLM",
    "section": "Basic HAVING Syntax",
    "text": "Basic HAVING Syntax\nSELECT\n  group_column,\n  AGGREGATE_FUNCTION(column)\nFROM table_name\nGROUP BY group_column\nHAVING AGGREGATE_FUNCTION(column) condition;\nHAVING always works together with GROUP BY.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#having-example-filter-by-total-revenue",
    "href": "materials/sql/slides/session3.html#having-example-filter-by-total-revenue",
    "title": "Basic Queries: DDL DLM",
    "section": "HAVING Example | Filter by Total Revenue",
    "text": "HAVING Example | Filter by Total Revenue\nSuppose you want products that generated more than 10,000 in total revenue.\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nHAVING SUM(total_sales) &gt; 10000;\n\\[\\downarrow\\]\n\ngroups sales by product\ncalculates total revenue per product\nkeeps only products above the threshold",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#having-vs-where-practical-comparison",
    "href": "materials/sql/slides/session3.html#having-vs-where-practical-comparison",
    "title": "Basic Queries: DDL DLM",
    "section": "HAVING vs WHERE | Practical Comparison",
    "text": "HAVING vs WHERE | Practical Comparison\nUsing WHERE (incorrect):\nSELECT\n  product_id,\n  SUM(total_sales)\nFROM sales\nWHERE SUM(total_sales) &gt; 10000\nGROUP BY product_id;\nThis query fails because SUM() is not available at the WHERE stage.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#combining-where-and-having",
    "href": "materials/sql/slides/session3.html#combining-where-and-having",
    "title": "Basic Queries: DDL DLM",
    "section": "Combining WHERE and HAVING",
    "text": "Combining WHERE and HAVING\nYou can (and often should) use both.\n\nWHERE filters rows early\nHAVING filters aggregated results\n\nSELECT\n  product_id,\n  SUM(total_sales) AS total_revenue\nFROM sales\nWHERE total_sales &gt; 0\nGROUP BY product_id\nHAVING SUM(total_sales) &gt; 10000;\nThis is more efficient than using HAVING alone.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#having-with-count",
    "href": "materials/sql/slides/session3.html#having-with-count",
    "title": "Basic Queries: DDL DLM",
    "section": "HAVING with COUNT",
    "text": "HAVING with COUNT\nFind products with at least 50 transactions.\nSELECT\n  product_id,\n  COUNT(transaction_id) AS transaction_count\nFROM sales\nGROUP BY product_id\nHAVING COUNT(transaction_id) &gt;= 50;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#having-with-multiple-conditions",
    "href": "materials/sql/slides/session3.html#having-with-multiple-conditions",
    "title": "Basic Queries: DDL DLM",
    "section": "HAVING with Multiple Conditions",
    "text": "HAVING with Multiple Conditions\nSELECT\n  product_id,\n  COUNT(transaction_id) AS transaction_count,\n  SUM(total_sales) AS total_revenue\nFROM sales\nGROUP BY product_id\nHAVING\n  COUNT(transaction_id) &gt;= 50\n  AND SUM(total_sales) &gt; 10000;\nThis filters on multiple aggregated metrics.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#when-to-use-having",
    "href": "materials/sql/slides/session3.html#when-to-use-having",
    "title": "Basic Queries: DDL DLM",
    "section": "When to Use HAVING",
    "text": "When to Use HAVING\nUse HAVING when:\n\nfiltering aggregated results\nworking with SUM, COUNT, AVG, etc.\napplying business thresholds to groups\n\nDo not use HAVING when row-level filtering is sufficient.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/slides/session3.html#execution-order",
    "href": "materials/sql/slides/session3.html#execution-order",
    "title": "Basic Queries: DDL DLM",
    "section": "Execution order",
    "text": "Execution order\nSELECT                                   -- 6\n  p.product_id,                          -- 6\n  SUM(s.total_sales) AS total_revenue    -- 6\nFROM sales AS s                          -- 1\nJOIN products AS p                       -- 2\n  ON s.product_id = p.product_id         -- 2\nWHERE s.total_sales &gt; 0                  -- 3\nGROUP BY p.product_id                    -- 4\nHAVING SUM(s.total_sales) &gt; 10000        -- 5\nORDER BY total_revenue DESC              -- 7\nLIMIT 5;                                 -- 8",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Slides",
      "Basic Queries: DDL DLM"
    ]
  },
  {
    "objectID": "materials/sql/session7.html",
    "href": "materials/sql/session7.html",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "",
    "text": "Up to this point, we have worked with numbers and text.\nNow we move to the most important analytical dimension of all: the Time.\nAlmost every real business question is, at its core, a story over time.\n\nWhat happened before?\nWhat changed after?\nHow fast did something grow?\nWhen did behavior shift?\nHow often did something was happenng?\n\nDates are not just metadata ‚Äî they define cause, sequence, and interpretation.\nThis session is about learning how to think with dates, not just manipulate them.\nImagine this question:\n\n‚ÄúDid our sales improve after the campaign launch?‚Äù\n\nYou open the data and see:\n\ntransactions before March\n\ntransactions after March\n\nBut then you realize:\n\nsome rows have timestamps\n\nsome have only dates\n\nsome are stored in UTC\n\nothers are local time\n\nsome records are missing days\n\nSuddenly, the question becomes unanswerable.\nThe problem is not SQL syntax. The problem is misunderstanding time.\nDate functions exist to restore order, structure, and comparability.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#dates-as-stories-not-columns",
    "href": "materials/sql/session7.html#dates-as-stories-not-columns",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "",
    "text": "Up to this point, we have worked with numbers and text.\nNow we move to the most important analytical dimension of all: the Time.\nAlmost every real business question is, at its core, a story over time.\n\nWhat happened before?\nWhat changed after?\nHow fast did something grow?\nWhen did behavior shift?\nHow often did something was happenng?\n\nDates are not just metadata ‚Äî they define cause, sequence, and interpretation.\nThis session is about learning how to think with dates, not just manipulate them.\nImagine this question:\n\n‚ÄúDid our sales improve after the campaign launch?‚Äù\n\nYou open the data and see:\n\ntransactions before March\n\ntransactions after March\n\nBut then you realize:\n\nsome rows have timestamps\n\nsome have only dates\n\nsome are stored in UTC\n\nothers are local time\n\nsome records are missing days\n\nSuddenly, the question becomes unanswerable.\nThe problem is not SQL syntax. The problem is misunderstanding time.\nDate functions exist to restore order, structure, and comparability.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#dates-create-analytical-structure",
    "href": "materials/sql/session7.html#dates-create-analytical-structure",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "Dates Create Analytical Structure",
    "text": "Dates Create Analytical Structure\nA single timestamp can represent many analytical dimensions:\n\nday\n\nweek\n\nmonth\n\nquarter\n\nyear\n\nweekday vs weekend\n\nWithout extracting and structuring these dimensions:\n\ntrends disappear\n\nseasonality is hidden\n\ncomparisons become invalid\n\nDates turn events into patterns.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#the-story-we-will-follow",
    "href": "materials/sql/session7.html#the-story-we-will-follow",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "The Story We Will Follow",
    "text": "The Story We Will Follow\nThroughout this session, we will use the sales_analysis table created earlier.\nEach row represents a transaction, and each transaction happened at a specific point in time.\nOur goal is to answer progressively deeper questions:\n\nWhen did something happen?\nHow long did it take?\nHow many events happened in a period?\nHow does time aggregation change interpretation?\n\n\n\n\n\n\n\nTipSales Analysis\n\n\n\nRemember to create/re-create the table using the following URL",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#date-vs-timestamp-the-first-fork-in-the-road",
    "href": "materials/sql/session7.html#date-vs-timestamp-the-first-fork-in-the-road",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "Date vs Timestamp | The First Fork in the Road",
    "text": "Date vs Timestamp | The First Fork in the Road\nBefore using any date function, let‚Äôs try to understand what kind of time we have.\nDATE\n\ncalendar day only\n\nno time of day\n\nexample: 2024-05-12\n\nTIMESTAMP\n\ndate + time\n\nsequence-sensitive\n\nexample: 2024-05-12 14:37:22\n\nThis distinction matters because:\n\ngrouping behaves differently\n\ncomparisons behave differently\n\ntruncation behaves differently\n\nMost analytical mistakes start here.\n\n\n\n\n\n\nImportantIn our case\n\n\n\nIn sales_analysis\n\norder_date_date: date column\norder_date: timestamp",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#extract",
    "href": "materials/sql/session7.html#extract",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "EXTRACT()",
    "text": "EXTRACT()\nDates contain multiple dimensions inside a single value \\(\\rightarrow\\) Pulling Meaning from Dates\n\nyear\nquarter\nmonths\ndate\nweekday (0 \\(\\rightarrow\\) Sunday)\n\nEXTRACT() allows us to isolate these components.\nEXTRACT(field FROM date)\n\nExample 1 | Extract Year, Month, Day\nQuery:\nSELECT\n  order_date_date,\n  EXTRACT(YEAR FROM order_date_date)  AS year,\n  EXTRACT(MONTH FROM order_date_date) AS month,\n  EXTRACT(DAY FROM order_date_date)   AS day, \n  EXTRACT(DOW FROM some_date) as weekday\nFROM sales_analysis\nLIMIT 5\n\\[\\downarrow\\]\n\n\n\norder_date_date\nyear\nmonth\nday\nweekday\n\n\n\n\n2021-02-11\n2021\n2\n11\n4\n\n\n2022-12-10\n2022\n12\n10\n6\n\n\n2021-02-22\n2021\n2\n22\n1\n\n\n2022-07-12\n2022\n7\n12\n2\n\n\n2021-04-19\n2021\n4\n19\n1\n\n\n\n\n\n\n\n\n\nImportantExtract Time\n\n\n\nWe can also extract time using EPOCHS\nEXTRACT(EPOCHS FROM order_date_date)   AS secconds,\nEXTRACT(EPOCHS FROM order_date_date)/60   AS minutes,\nEXTRACT(EPOCHS FROM order_date_date)/3600   AS minutes\n\n\n\n\nDATE_PART()\nDATE_PART() is functionally equivalent to EXTRACT().\nDATE_PART('month', order_date_date)\nBoth return numeric components of a date.\n\n\nWhen to Prefer Which\n\nEXTRACT() ‚Üí ANSI-standard, portable SQL\n\nDATE_PART() ‚Üí PostgreSQL-native, readable\n\nIn this course, we prefer EXTRACT() for consistency.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#date_trunc-controlling-the-grain-of-time",
    "href": "materials/sql/session7.html#date_trunc-controlling-the-grain-of-time",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "DATE_TRUNC() | Controlling the Grain of Time",
    "text": "DATE_TRUNC() | Controlling the Grain of Time\nIn analytics, grain matters.\nThe same data can tell very different stories depending on how time is grouped.\nDATE_TRUNC() allows us to round timestamps down to a chosen temporal level\nand therefore control the analytical grain.\nDATE_TRUNC('unit', timestamp)\n\n\nExample 1 | Monthly Sales Trend\nSELECT\n  DATE_TRUNC('month', order_date_date) AS month,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('month', order_date_date)\nORDER BY month;\n\\[\\downarrow\\]\n\n\n\nmonth\ntotal_revenue\n\n\n\n\n2020-01-01 00:00:00+00\n38621.59\n\n\n2020-02-01 00:00:00+00\n30460.16\n\n\n2020-04-01 00:00:00+00\n18096.39\n\n\n2020-05-01 00:00:00+00\n23722.28\n\n\n2020-06-01 00:00:00+00\n42599.07\n\n\n\n\n\nExample 2 | The Most Profitable Months\nSELECT\n  DATE_TRUNC('month', order_date_date) AS month,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('month', order_date_date)\nORDER BY SUM(total_sales) DESC\nLIMIT 5;\n\\[\\downarrow\\]\n\n\n\nmonth\ntotal_revenue\n\n\n\n\n2022-12-01 00:00:00+00\n52510.67\n\n\n2022-07-01 00:00:00+00\n49011.10\n\n\n2023-12-01 00:00:00+00\n46700.47\n\n\n2021-09-01 00:00:00+00\n46433.23\n\n\n2020-06-01 00:00:00+00\n42599.07\n\n\n\n\n\nExample 3 | Quarterly Sales Trend\nQuarterly aggregation is useful for:\n\nstrategic planning\n\nexecutive reporting\n\nseasonality analysis\n\nSELECT\n  DATE_TRUNC('quarter', order_date_date) AS quarter,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('quarter', order_date_date)\nORDER BY quarter\nLIMIT 8\n;\n\\[\\downarrow\\]\n\n\n\nquarter\ntotal_revenue\n\n\n\n\n2020-01-01 00:00:00+00\n69081.75\n\n\n2020-04-01 00:00:00+00\n84417.74\n\n\n2020-07-01 00:00:00+00\n62003.53\n\n\n2020-10-01 00:00:00+00\n69127.16\n\n\n2021-01-01 00:00:00+00\n74184.68\n\n\n2021-04-01 00:00:00+00\n68431.42\n\n\n2021-07-01 00:00:00+00\n84194.98\n\n\n2021-10-01 00:00:00+00\n63543.31\n\n\n\nAt the quarterly grain:\n\nshort-term noise is further reduced\n\nperformance becomes comparable across years\n\nseasonality patterns are easier to detect\n\nThis grain is ideal for high-level trend analysis.\n\n\n\nExample 4 | Yearly Sales Trend\nYearly aggregation answers long-term performance questions.\nSELECT\n  DATE_TRUNC('year', order_date_date) AS year,\n  SUM(total_sales) AS total_revenue\nFROM sales_analysis\nGROUP BY DATE_TRUNC('year', order_date_date)\nORDER BY year;\n\\[\\downarrow\\]\n\n\n\nyear\ntotal_revenue\n\n\n\n\n2020-01-01 00:00:00+00\n284630.18\n\n\n2021-01-01 00:00:00+00\n290354.39\n\n\n2022-01-01 00:00:00+00\n345714.03\n\n\n2023-01-01 00:00:00+00\n350319.25\n\n\n\nAt the yearly grain:\n\ngrowth trajectories become clear\n\noutliers disappear\n\nlong-term strategy can be evaluated\n\nYear-level aggregation is not for operational decisions as it is for directional understanding.\n\n\nAnalytical Best Practice\n\nalways justify your time grain\n\nnever mix grains in the same comparison\n\nre-aggregate before drawing conclusions\n\nvalidate results by switching grains\n\nDATE_TRUNC() does not change the data, it changes how you see the data.\nThat is why it is one of the most important date functions in analytics.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#current_date-now-working-with-the-present",
    "href": "materials/sql/session7.html#current_date-now-working-with-the-present",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "CURRENT_DATE, NOW() | Working With the Present",
    "text": "CURRENT_DATE, NOW() | Working With the Present\nIn analytics, we often need a reference point called now.\nThis allows us to answer questions such as:\n\nhow recent is this transaction?\n\nhow many days ago did something happen?\n\nis this record outdated?\n\nPostgreSQL provides two core functions for this purpose.\n\n\nCURRENT_DATE\nReturns today‚Äôs date without time.\nSELECT CURRENT_DATE;\n\ntype: DATE\n\nchanges once per day\n\nideal for day-level comparisons\n\n\n\n\nNOW() / CURRENT_DATE\nReturns the current timestamp (date + time).\nSELECT NOW();\n\ntype: TIMESTAMP WITH TIME ZONE\n\nincludes hours, minutes, seconds\n\nideal for sequence-sensitive logic\n\nSELECT CURRENT_DATE\n\n\n\n\n\n\nCautionAttention\n\n\n\n\nYou must use () with NOW &gt; NOW(), SELECT NOW\nYou must NOT use () CURRENT_DATE, SELECT CURRENT_DATE\n\n\n\nUsing CURRENT_DATE vs NOW() changes interpretation.\n\nCURRENT_DATE ‚Üí calendar-based logic\n\nNOW() ‚Üí event-sequence logic\n\nAlways align the function with the granularity of your question.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#measuring-time-gaps-date-arithmetic",
    "href": "materials/sql/session7.html#measuring-time-gaps-date-arithmetic",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "Measuring Time Gaps | Date Arithmetic",
    "text": "Measuring Time Gaps | Date Arithmetic\nDates become analytical only when we compare them.\nPostgreSQL allows direct subtraction between dates.\n\nExample 1 | Days Since Order\nSELECT\n  order_date_date,\n  CURRENT_DATE - order_date_date AS days_since_order\nFROM sales_analysis\nLIMIT 5;\n\\[\\downarrow\\]\n\n\n\norder_date_date\ndays_since_order\n\n\n\n\n2021-02-11\n1797\n\n\n2022-12-10\n1130\n\n\n2021-02-22\n1786\n\n\n2022-07-12\n1281\n\n\n2021-04-19\n1730\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nsubtraction of two DATEs returns an integer\nthe unit is days\nthis is ideal for:\n\nrecency analysis\n\nchurn logic\n\naging reports\n\n\n\n\n\n\nExample 2 | Get Number of Months\n\nOption 1\nSELECT\n  order_date_date,\n  (CURRENT_DATE - order_date_date )/30  AS days_since_order\nFROM sales_analysis\nLIMIT 5;\n\n\n\n\n\n\nWarning\n\n\n\nThis is wrong and produces systematic bias.\nBefor applying this we must be sure that it is accepted within the organization\n\n\n\n\nOption 2\nSELECT\n  (CURRENT_DATE - order_date_date) / 30.4375 AS approx_months\nFROM orders;\nWhy 30.4375 ?\nthe average days per months \\(\\rightarrow\\) 365.25 / 12\n\nNot exact\nNot calendar-safe\nFine for ML features/high level analysis\n\n\n\nOption 3\nThis produces the number of fully completed calendar months between two dates.\nSELECT\n  order_date_date,\n    (DATE_PART('year', CURRENT_DATE) - DATE_PART('year', order_date_date)) * 12\n  + (DATE_PART('month', CURRENT_DATE) - DATE_PART('month', order_date_date))\n  - CASE\n      WHEN DATE_PART('day', CURRENT_DATE)\n        &lt; DATE_PART('day', order_date_date)\n      THEN 1 ELSE 0\n  END AS full_months\nFROM sales_analysis\nORDER BY order_date_date DESC\nLIMIT 10;\nStep-by-step explanation of full months calculation:\n\nDATE_PART('year', CURRENT_DATE) - DATE_PART('year', order_date_date) computes the difference in years between today and the order date.\nmultiplying by 12 converts the year difference into months.\nDATE_PART('month', CURRENT_DATE) - DATE_PART('month', order_date_date) adds the month difference within the current year.\nthe CASE statement adjusts for incomplete months:\n\nif today‚Äôs day-of-month is earlier than the order day,\nthe current month is not yet fully completed,\nso we subtract 1 month.\n\n\n\n\n\n\n\n\nNoteDATEDIFF() In SQL Server\n\n\n\nIn case of SQL server we can use DATEDIFF() function\nDATEDIFF(MM, Date1, Date2)",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#interval-expressing-duration-explicitly",
    "href": "materials/sql/session7.html#interval-expressing-duration-explicitly",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "INTERVAL | Expressing Duration Explicitly",
    "text": "INTERVAL | Expressing Duration Explicitly\nAn INTERVAL represents a span of time, not a point.\nThis allows us to add or subtract time meaningfully.\n\n\nExample | Orders in the Last 30 Days\nSELECT\n  COUNT(*) AS recent_orders\nFROM sales_analysis\nWHERE order_date_date &gt;= CURRENT_DATE - INTERVAL '30 days';\n\n\n\nCommon Interval Units\n\nINTERVAL '7 days'\n\nINTERVAL '1 month'\n\nINTERVAL '1 year'\n\nIntervals are human-readable and analytically expressive.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#date-interval-shifting-time",
    "href": "materials/sql/session7.html#date-interval-shifting-time",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "Date + Interval | Shifting Time",
    "text": "Date + Interval | Shifting Time\nYou can shift dates forward or backward using intervals.\nsome_date +/-  INTERVAL \"INT unit\"\n\\[\\downarrow\\]\nsome_date +  INTERVAL \"2 days\"\n\nsome_date -   INTERVAL \"3 months\"\n\nsome_date +  INTERVAL \"5 years\"\n\nExample | Simulated Future Date\nSELECT\n  order_date_date,\n  order_date_date + INTERVAL '14 days' AS follow_up_date\nFROM sales_analysis\nLIMIT 5;\n\n\nAnalytical Use Cases\n\nfollow-up scheduling\n\ngrace periods\n\nforecasting reference points\n\nIntervals preserve calendar logic, unlike numeric arithmetic.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#age",
    "href": "materials/sql/session7.html#age",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "AGE()",
    "text": "AGE()\nAGE() calculates the exact calendar distance between two dates or timestamps.\nUnlike simple subtraction, AGE() respects:\n\nvarying month lengths\n\nleap years\n\nreal calendar boundaries\n\nIt returns an INTERVAL, not a number.\n\n\nExample | Age Between Order Date and Today\nSELECT\n  order_date_date,\n  AGE(CURRENT_DATE, order_date_date) AS order_age\nFROM sales_analysis\nLIMIT 5;\n\\[\\downarrow\\]\n\n\n\norder_date_date\norder_age\n\n\n\n\n2021-02-11\n4 years 11 mons 2 days\n\n\n2022-12-10\n3 years 1 mon 3 days\n\n\n2021-02-22\n4 years 10 mons 19 days\n\n\n2022-07-12\n3 years 6 mons 1 day\n\n\n2021-04-19\n4 years 8 mons 24 days\n\n\n\n\n\n\nExtracting Components from AGE()\nYou can extract individual components from the interval.\nSELECT\n  order_date_date,\n  EXTRACT(YEAR FROM AGE(CURRENT_DATE, order_date_date))  AS years,\n  EXTRACT(MONTH FROM AGE(CURRENT_DATE, order_date_date)) AS months,\n  EXTRACT(DAY FROM AGE(CURRENT_DATE, order_date_date))   AS days\nFROM sales_analysis\nLIMIT 5;\n\n\n\n\n\n\n\nWarningAnalytical Warning | AGE() Extraction\n\n\n\n\nextracted years, months, days are not independent\nmonths reset after 12\ndays reset after month boundaries\n\nNever treat extracted components as additive totals.\nUse AGE() for human interpretation,\nuse numeric arithmetic for modeling features.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#analytical-best-practices-dates",
    "href": "materials/sql/session7.html#analytical-best-practices-dates",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "Analytical Best Practices | Dates",
    "text": "Analytical Best Practices | Dates\nDates are not just values ‚Äî they encode ordering and causality.\n\nalways align time grain with the question\n\nnever mix DATE and TIMESTAMP blindly\n\nuse INTERVAL instead of hard-coded numbers\n\nvalidate insights by changing reference dates\n\nTime-aware analytics is what separates reporting from analysis.\nDates do not answer questions on their own as they allow stories to be told correctly.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#in-class-task",
    "href": "materials/sql/session7.html#in-class-task",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "In class Task",
    "text": "In class Task\nGoal: practice time-grain control and recency logic.\nTask instructions:\n\naggregate total sales by:\n\nmonth\nquarter\n\nidentify:\n\nthe top 3 months by revenue\nthe top quarter by revenue\n\ncompute:\n\ndays since each transaction\n\nfilter:\n\ntransactions from the last 60 days (you should get the empty table)\n\n\nDeliverable:\n\nrun queries in pgAdmin\n\nvisually inspect result tables\n\ndiscuss how changing the time grain changes interpretation",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session7.html#homework",
    "href": "materials/sql/session7.html#homework",
    "title": "Session 07: Data Analysis with SQL | Functions",
    "section": "Homework",
    "text": "Homework\nStory-driven task:\nYou are asked to evaluate sales performance over time for management.\nRequired steps:\n\nbuild monthly, quarterly, and yearly aggregations\ncompare trends across different grains\nidentify:\n\nstrongest growth period\nweakest period\n\ncompute:\n\ndays since last transaction per customer\n\nuse AGE() to describe customer recency in calendar terms\n\nVisualization:\n\nuse pgAdmin charts to visualize:\n\nmonthly revenue trend\nquarterly comparison\n\nannotate findings with written interpretation\n\nFinal deliverable:\n\nSQL queries\nscreenshots of pgAdmin charts\na short narrative explaining:\n\nwhat changed over time\nwhy grain choice matters\nwhich conclusions depend on time aggregation\n\nPush everything to github\n\n\n\n\n\n\n\nTip\n\n\n\nYou can experiment a lot, as well",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 07: Data Analysis with SQL | Functions"
    ]
  },
  {
    "objectID": "materials/sql/session2.html",
    "href": "materials/sql/session2.html",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "",
    "text": "Imagine you‚Äôve just joined a company as a data analyst.\nThe company sells products through an online channel and stores its operational data in a PostgreSQL relational database.\nThe database captures information about:\n\ncustomers who place orders\nproducts that are sold\nemployees involved in the sales process\norders placed over time\nindividual sales transactions\n\nYour role as an analyst is to query this data to answer business questions related to:\n\nrevenue and sales performance\nproduct popularity\ncustomer behavior\nemployee contribution\ntime-based trends",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#sales-analytics-database",
    "href": "materials/sql/session2.html#sales-analytics-database",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "",
    "text": "Imagine you‚Äôve just joined a company as a data analyst.\nThe company sells products through an online channel and stores its operational data in a PostgreSQL relational database.\nThe database captures information about:\n\ncustomers who place orders\nproducts that are sold\nemployees involved in the sales process\norders placed over time\nindividual sales transactions\n\nYour role as an analyst is to query this data to answer business questions related to:\n\nrevenue and sales performance\nproduct popularity\ncustomer behavior\nemployee contribution\ntime-based trends",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#learning-goals",
    "href": "materials/sql/session2.html#learning-goals",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Learning Goals",
    "text": "Learning Goals\nIn the previous session, you learned about different types of databases and the database management systems used to define, query, and manage them.\nWithout databases, you wouldn‚Äôt be able to retrieve, update, or delete data.\nBefore analysis can begin, however, data must first be stored, structured, and related correctly.\nIn this session, we will learn about the core building blocks of a relational database:\n\ntables, rows, and columns\nkeys and relationships\nindexes and performance trade-offs\ndata types in PostgreSQL\ndatabase schemas and analytical modeling concepts\n\nBy the end of this session, you should be able to read and understand a relational schema and write more efficient SQL queries.\n\n\n\n\n\n\nTip\n\n\n\nCheck out how to use pgAdmin 4 for better navigation.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#data-storage-structures",
    "href": "materials/sql/session2.html#data-storage-structures",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Data Storage Structures",
    "text": "Data Storage Structures\nA relational database consists of one or more tables made up of rows and columns, and these tables are linked by relationships.\nIn this course, we will work with a sales analytics database that includes tables such as:\n\nsales\norders\nproducts\ncustomers\nemployees\n\nEach table stores information about a specific business entity, and relationships between tables allow us to combine this information during analysis.\nThe list on the left represents all the columns in a table. Each column represents a specific attribute of a transaction, such as total_sales, quantity, or product_id. Each row (record) represents a single transaction.\n\n\n\n\n\n\n\nTip\n\n\n\nAnother name for a record is a tuple.\nWe will revisit tuples when working with Python and relational data.\n\n\nTables are such a common structure that you‚Äôve likely interacted with them without realizing it.\nFor example, when you place an order on an e-commerce website like Amazon:\n\nEach row represents a different order or transaction\nColumns represent attributes such as:\n\ncustomer name\naddress\nproduct ordered\nprice\norder date\n\n\nTo illustrate, here‚Äôs an example order report.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#keys-and-indexes",
    "href": "materials/sql/session2.html#keys-and-indexes",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Keys and Indexes",
    "text": "Keys and Indexes\nTo store data efficiently and enable relationships between tables, relational databases rely on keys and indexes.\n\nKeys\nA key is a column (or set of columns) that uniquely identifies a row in a table.\nKeys are essential for locating records and defining relationships between tables.\nIf you refer back to the sales table, you‚Äôll notice a key icon next to the transaction_id column.\nThis indicates that transaction_id is a key.\nIn our database, each row in the sales table represents a single transaction line, uniquely identified by transaction_id.\n\n\nPrimary Key\nA primary key uniquely identifies each record in a table.\nEach table can have only one primary key, and it must:\n\nBe unique\nNever be NULL\nRemain stable when referenced by other tables\n\nIn the sales table, transaction_id is the primary key.\n\n\n\ntransaction_id\norder_id\nproduct_id\ncustomer_id\ntotal_sales\n\n\n\n\n1001\n501\n12\n3001\n49.99\n\n\n1002\n501\n18\n3001\n19.99\n\n\n1003\n502\n12\n3005\n49.99\n\n\n1004\n503\n25\n3008\n89.99\n\n\n\n\n\n\nCandidate Key\nA candidate key is any column (or set of columns) that can uniquely identify a row.\nFor example, in the customers table, customer_id and email columns are candidate keys.\nThis means both columns are candidate keys.\nFrom the set of candidate keys, one is chosen as the primary key.\nIn this schema, customer_id is selected as the primary key, while email remains an alternative identifier.\n\n\n\n\n\n\n\n\n\ncustomer_id\ncustomer_name\nemail\ncity\n\n\n\n\n3001\nAlice Johnson\nalice.johnson@email.com\nBerlin\n\n\n3002\nMark Thompson\nmark.thompson@email.com\nParis\n\n\n3003\nElena Petrova\nelena.p@email.com\nMadrid\n\n\n3004\nDavid Chen\ndavid.chen@email.com\nLondon\n\n\n\n\n\n\nComposite Key\nA composite key is formed by combining two or more columns when no single column is sufficient to uniquely identify a row.\nFor example, (order_id, product_id) could uniquely identify rows in some transactional systems.\n\n\n\norder_id\nproduct_id\nproduct_name\nquantity\n\n\n\n\n501\n12\nWireless Mouse\n1\n\n\n501\n18\nUSB-C Cable\n2\n\n\n502\n12\nWireless Mouse\n1\n\n\n503\n25\nMechanical Keyboard\n1\n\n\n\nIn this table:\n\norder_id alone is not unique\nproduct_id alone is not unique\nThe combination (order_id, product_id) uniquely identifies each row\n\nThis combination forms a composite key.\n\n\nSurrogate Key\nA surrogate key is a system-generated identifier with no business meaning.\nThe transaction_id column is a surrogate key; its only purpose is to uniquely identify each transaction.\n\n\n\ntransaction_id\norder_id\nproduct_id\nquantity\ntotal_sales\n\n\n\n\n1001\n501\n12\n1\n49.99\n\n\n1002\n501\n18\n2\n19.99\n\n\n1003\n502\n12\n1\n49.99\n\n\n1004\n503\n25\n1\n89.99\n\n\n\nSurrogate keys are often used even when natural or composite keys exist.\nThey are preferred in analytical systems because they:\n\nsimplify joins\nreduce index size\nimprove query readability\nremain stable even if business rules change\n\n\n\nForeign Key\nA foreign key is a column (or set of columns) that references a primary key in another table.\nFor example, the product_id column in the sales table references the product_id primary key in the products table.\nForeign keys establish relationships between tables and allow us to join data across entities.\nThus, Foreign key enforces referential integrity between the two tables which is crucial:\n\nprevent invalid data (e.g., sales for non-existent products)\ndefine how tables are related\nenable joins between tables\npreserve data consistency\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn analytical databases, foreign keys are sometimes not physically enforced for performance reasons, but they are always enforced logically in the data model.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#indexes",
    "href": "materials/sql/session2.html#indexes",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Indexes",
    "text": "Indexes\nImagine searching through a book to find all pages starting with the letter A.\nWithout an index, you would scan every page.\nWith an index (like a dictionary), you jump directly to the correct section.\nIndexes work the same way in databases.\nIndexes do not store new data.\nThey are special lookup structures that improve query performance.\n\n\n\n\n\n\n\nNoteFull Table Scan\n\n\n\nWhen a query checks every row instead of using an index, this is called a full scan.\n\n\nIndexes improve read performance but come with trade-offs:\n\nextra storage\nslower inserts and updates\nmaintenance overhead\n\n\nWhen Not to Create Indexes\n\nVery small tables\nFrequently updated columns\nColumns with many NULL values\nColumns with very low cardinality\n\n\n\nSingle-Column Index\nA single-column index is built on one column.\n\n\n\nEmployee ID\nName\nContact Number\nAge\n\n\n\n\n1\nMax\n800692692\n24\n\n\n2\nJessica\n800123456\n35\n\n\n3\nMikeal\n800745547\n49\n\n\n\n\n\nComposite Index\nConsider the same employees table, but now imagine that queries often filter by both name and age at the same time. Thus, composite index spans multiple columns, such as (name, age).\nComposite indexes are most effective when queries filter columns in the same left-to-right order as the index definition.\n\n\n\nemployee_id\nname\nage\ndepartment\n\n\n\n\n1\nMax\n24\nSales\n\n\n2\nJessica\n35\nMarketing\n\n\n3\nMax\n35\nFinance\n\n\n\nFor an index defined as (name, age):\n\nEfficient for queries filtering on name\nEfficient for queries filtering on name and age\nNot efficient for queries filtering on age only\n\n\n\nUnique Index\nUnlike a regular index, a unique index enforces a rule, not just performance. A unique index ensures that values are not duplicated.\n\n\n\ncustomer_id\ncustomer_name\nemail\n\n\n\n\n3001\nAlice Johnson\nalice@email.com\n\n\n3002\nMark Thompson\nmark@email.com\n\n\n3003\nElena Petrova\nelena@email.com\n\n\n\nIn the aboive table:\n\nemail values are unique\nNo two customers share the same email address\n\nCreating a unique index on email:\n\nprevents duplicate emails\nguarantees data integrity\n\nUse a unique index when:\n\nvalues must be unique across rows\nthe column is frequently used for lookup\nuniqueness is part of business logic\n\n\n\n\n\n\n\nImportantAre Keys Also Indexes?\n\n\n\nKeys vs Indexes\nFrom a database engine and storage point of view, keys and indexes are closely related, but they are not the same thing.\n\nA key defines a logical rule (uniqueness, relationships)\nAn index is a physical data structure stored on disk to speed up access\n\n\nHow PostgreSQL Handles Them\n\n\n\n\n\n\n\n\n\nConcept\nEnforces Uniqueness\nImproves Query Speed\nStored as Index\n\n\n\n\nPrimary Key\nYes\nYes\nYes (unique index)\n\n\nUnique Constraint\nYes\nYes\nYes (unique index)\n\n\nForeign Key\nNo\nSometimes\nNo (by default)\n\n\nRegular Index\nNo\nYes\nYes\n\n\n\n\n\nImportant Notes\n\nCreating a primary key automatically creates a unique index\nCreating a unique constraint automatically creates a unique index\nForeign keys do NOT create indexes automatically\nIndexes exist to improve performance; keys exist to enforce rules",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#data-types",
    "href": "materials/sql/session2.html#data-types",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Data Types",
    "text": "Data Types\nData types define what kind of data a column can store.\nThey ensure consistency, enable validation, and affect performance.\nConsider the following example:\n\n\n\nEmployee Name\nSalary ($)\n\n\n\n\nJan\n100000\n\n\nAlex\nOne hundred thousand\n\n\nKim\n70k\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nInconsistent data types make analysis impossible.\nTo calculate averages or totals, the column must enforce a numeric type.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#data-types-in-postgresql",
    "href": "materials/sql/session2.html#data-types-in-postgresql",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Data Types in PostgreSQL",
    "text": "Data Types in PostgreSQL\n\nNumeric\nCharacter\nDate & Time\nBoolean & Categorical\nIdentifier & Auto-Increment\nJSON and Semi-Structured\nArray\nSpecial Data Types\n\n\n\nNumeric Data Types\nIn PostgreSQL, numeric data types fall into three main groups:\n\nIntegers: whole numbers\nExact numerics: precise decimal numbers\nFloating-point numbers: approximate decimals\n\nNumeric data types are used for:\n\ncounts\nquantities\nprices\nmeasurements\n\n\nMain Numeric Data Types\n\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nMemory Size\nTypical Use Case\n\n\n\n\nSMALLINT\nSmall-range whole numbers\n12\n2 bytes\nStatus codes, flags\n\n\nINTEGER / INT\nStandard whole numbers\n2500\n4 bytes\nQuantities, counts\n\n\nBIGINT\nVery large whole numbers\n9876543210\n8 bytes\nIDs, large counters\n\n\nNUMERIC(p, s)\nExact precision decimal\nNUMERIC(10,2) ‚Üí 15432.75\nVariable\nSalary, revenue\n\n\nDECIMAL(p, s)\nSame as NUMERIC (SQL standard)\nDECIMAL(8,3) ‚Üí 123.456\nVariable\nFinancial data\n\n\nREAL\nApproximate floating-point\n3.14159\n4 bytes\nScientific values\n\n\nDOUBLE PRECISION\nHigher-precision floating-point\n0.0000123456789\n8 bytes\nStatistical calculations\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how exact numerics (NUMERIC, DECIMAL) preserve precision, while floating-point types (REAL, DOUBLE PRECISION) may introduce rounding differences.\n\n\n\n\nPerformance vs Precision Trade-off\nWhen choosing a numeric data type, you must balance precision, performance, and storage efficiency.\nThe table below summarizes the most common analytical requirements and the recommended PostgreSQL data types.\n\n\n\n\n\n\n\n\n\nRequirement\nPrecision Needed\nPerformance Priority\nRecommended Data Type\n\n\n\n\nRow counts, simple counters\nExact\nVery high\nINTEGER\n\n\nLarge identifiers (IDs)\nExact\nHigh\nBIGINT\n\n\nFinancial values (salary, revenue)\nExact\nMedium\nNUMERIC(p,s)\n\n\nPrices with decimals\nExact\nMedium\nDECIMAL(p,s)\n\n\nRatios, averages, KPIs\nApproximate acceptable\nHigh\nDOUBLE PRECISION\n\n\nScientific measurements\nApproximate acceptable\nVery high\nREAL / DOUBLE PRECISION\n\n\n\n\n\n\nIndex Size Considerations\nIndexes inherit the storage and performance characteristics of the column type they are built on:\n\nIndex on INTEGER \\(\\rightarrow\\) small \\(\\rightarrow\\) cache-friendly\nIndex on NUMERIC \\(\\rightarrow\\) larger \\(\\rightarrow\\) slower to scan\nComposite indexes amplify the effect of column size\n\n\n\n\n\n\n\n\nImportantBest Practice\n\n\n\nUse the smallest numeric type that safely fits your data.\n\nPrefer INTEGER over BIGINT if values fit\nUse NUMERIC only when precision matters\nAvoid premature use of NUMERIC for IDs or counters\n\n\n\n\n\n\nCharacter Data Types\nCharacter data types are used to store textual information such as names, descriptions, identifiers, and free-form text.\nIn PostgreSQL, character data types fall into three main groups:\n\nFixed-length characters ‚Äî text with a fixed size\n\nVariable-length characters ‚Äî text with flexible size limits\n\nUnbounded text ‚Äî long or unpredictable text\n\nCharacter data types are used for:\n\nnames\ndescriptions\nemails\ncodes\ncomments\n\n\nMain Character Data Types\n\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nMemory Size\nTypical Use Case\n\n\n\n\nCHAR(n)\nFixed-length character string\nCHAR(5) ‚Üí ‚ÄòUS‚Äô\nn bytes (fixed)\nCountry codes, fixed formats\n\n\nVARCHAR(n)\nVariable-length string with limit\nVARCHAR(50) ‚Üí ‚ÄòKaren Hovhannisyan‚Äô\nVariable (‚â§ n)\nNames, emails\n\n\nTEXT\nVariable-length string, no limit\n‚ÄòThis product has been discontinued.‚Äô\nVariable\nDescriptions, comments\n\n\nCHARACTER VARYING(n)\nSQL-standard name for VARCHAR\nCHARACTER VARYING(20) ‚Üí ‚ÄòA123XZ‚Äô\nVariable (‚â§ n)\nCodes, identifiers\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVARCHAR(n) and TEXT behave almost identically in PostgreSQL.\nThe main difference is whether you want to enforce a maximum length.\n\n\n\n\nPerformance vs Flexibility Trade-off\nWhen choosing a character data type, the trade-off is usually between data validation and flexibility, not raw performance.\n\n\n\n\n\n\n\n\n\nRequirement\nLength Control Needed\nFlexibility Priority\nRecommended Data Type\n\n\n\n\nFixed-format values\nYes (fixed)\nLow\nCHAR(n)\n\n\nNames, emails\nYes (reasonable limit)\nMedium\nVARCHAR(n)\n\n\nFree-form text\nNo\nHigh\nTEXT\n\n\nUser comments, notes\nNo\nVery high\nTEXT\n\n\n\n\n\n\nIndex Size Considerations\nIndexes on character columns depend on string length and value distribution:\n\nIndex on CHAR(n) \\(\\rightarrow\\) predictable size \\(\\rightarrow\\) stable performance\nIndex on VARCHAR(n) \\(\\rightarrow\\) variable size \\(\\rightarrow\\) generally efficient\nIndex on TEXT \\(\\rightarrow\\) potentially large \\(\\rightarrow\\) slower scans for long values\nLong strings in composite indexes increase index size significantly\n\n\n\n\n\n\n\n\nImportantBest Practice\n\n\n\nUse constraints, not oversized text types, to control data quality.\n\nPrefer VARCHAR(n) when a reasonable maximum length exists\nUse TEXT for descriptions and comments\nAvoid CHAR(n) unless values are truly fixed-length\n\n\n\n\n\n\nDate & Time Data Types\nDate and time data types are used to store temporal information, such as when an event occurred, started, or ended.\nThey are critical for analytics involving time series, trends, seasonality, and durations.\nIn PostgreSQL, date and time data types fall into four main groups:\n\nDate-only values\nTime-only values\nTimestamps (date + time)\nTime intervals / durations\n\nDate & time data types are used for:\n\nevent dates\ntimestamps\nlog records\ndurations\ntime-based analysis\n\n\nMain Date & Time Data Types\n\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nMemory Size\nTypical Use Case\n\n\n\n\nDATE\nCalendar date (no time)\n2025-03-15\n4 bytes\nBirth dates, order dates\n\n\nTIME\nTime of day (no date)\n14:30:00\n8 bytes\nOpening hours\n\n\nTIME WITH TIME ZONE\nTime with time zone info\n14:30:00+04\n12 bytes\nCross-region schedules\n\n\nTIMESTAMP\nDate and time (no timezone)\n2025-03-15 14:30:00\n8 bytes\nLocal event logs\n\n\nTIMESTAMP WITH TIME ZONE (TIMESTAMPTZ)\nDate and time with timezone handling\n2025-03-15 10:30:00+00\n8 bytes\nAuditing, analytics\n\n\nINTERVAL\nTime duration\n3 days 4 hours\n16 bytes\nSession length, SLA\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTIMESTAMPTZ does not store the time zone itself.\nPostgreSQL converts values to UTC internally and displays them using the session time zone.\n\n\n\n\nAccuracy vs Interpretability Trade-off\nWhen working with time data, the trade-off is often between absolute correctness and human interpretation.\n\n\n\nRequirement\nTime Zone Awareness\nRecommended Data Type\n\n\n\n\nSimple dates\nNot needed\nDATE\n\n\nLocal business timestamps\nNot required\nTIMESTAMP\n\n\nMulti-region systems\nRequired\nTIMESTAMPTZ\n\n\nDurations and differences\nNot applicable\nINTERVAL\n\n\n\n\n\n\nIndex Size Considerations\nDate and time types are fixed-size, making them efficient for indexing:\n\nIndex on DATE \\(\\rightarrow\\) small \\(\\rightarrow\\) very fast range scans\nIndex on TIMESTAMP \\(\\rightarrow\\) small \\(\\rightarrow\\) efficient sorting\nIndex on TIMESTAMPTZ \\(\\rightarrow\\) small \\(\\rightarrow\\) timezone-safe\nIndex on INTERVAL \\(\\rightarrow\\) rarely indexed (use carefully)\n\nDate-based indexes are commonly used with:\n\nWHERE date BETWEEN ‚Ä¶\nGROUP BY date\ntime-window queries\n\n\n\n\n\n\n\n\nImportantBest Practice\n\n\n\nAlways use TIMESTAMPTZ for analytics and logging systems.\n\nAvoid ambiguity in multi-region environments\nUse DATE only when time is irrelevant\nStore durations as INTERVAL, not numeric hacks\n\n\n\n\n\n\nBoolean & Categorical Data Types\nBoolean and categorical data types are used to store logical values and finite categories.\nThey are especially important for filtering, segmentation, and business rules.\nIn PostgreSQL, boolean and categorical data types fall into two main groups:\n\nBoolean values ‚Äî true / false logic\n\nCategorical values ‚Äî limited sets of labels\n\nThese data types are used for:\n\nflags\nstatus indicators\nbinary decisions\ncategorical segmentation\n\n\nMain Boolean & Categorical Data Types\n\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample Value\nMemory Size\nTypical Use Case\n\n\n\n\nBOOLEAN\nLogical true / false value\nTRUE, FALSE\n1 byte\nActive flags, eligibility\n\n\nCHAR(n)\nFixed-length category code\nCHAR(1) ‚Üí ‚ÄòY‚Äô\nn bytes (fixed)\nYes/No indicators\n\n\nVARCHAR(n)\nShort categorical label\n‚Äòpremium‚Äô\nVariable (‚â§ n)\nCustomer segments\n\n\nTEXT\nFree-form category label\n‚Äòhigh_value_customer‚Äô\nVariable\nTags, labels\n\n\nENUM\nPredefined set of values\n(‚Äòlow‚Äô,‚Äòmedium‚Äô,‚Äòhigh‚Äô)\n4 bytes\nControlled categories\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPostgreSQL accepts multiple boolean literals:\nTRUE / FALSE, true / false, 1 / 0, yes / no.\n\n\n\n\nControl vs Flexibility Trade-off\nThe main trade-off for categorical data is between data integrity and flexibility.\n\n\n\nRequirement\nValidation Strictness\nRecommended Data Type\n\n\n\n\nBinary logic\nVery strict\nBOOLEAN\n\n\nSmall fixed category set\nVery strict\nENUM\n\n\nEvolving categories\nMedium\nVARCHAR(n)\n\n\nUser-defined labels\nLow\nTEXT\n\n\n\n\n\nIndex Size Considerations\n\nIndex on BOOLEAN \\(\\rightarrow\\) very small \\(\\rightarrow\\) fast but often low selectivity\nIndex on short categorical strings \\(\\rightarrow\\) efficient\nIndex on high-cardinality categories \\(\\rightarrow\\) more useful than boolean indexes\nAvoid indexing columns with very few distinct values\n\n\n\n\n\n\n\nImportantBest Practice\n\n\n\nUse BOOLEAN only for true binary logic.\n\nDo not encode booleans as ‚ÄòY‚Äô/‚ÄòN‚Äô unless required\nUse ENUM only when categories are stable\n\n\n\n\n\n\nSpecial & Advanced Data Types | OPTIONAL\nBeyond standard numeric, text, and date types, PostgreSQL supports specialized data types designed for specific domains such as\n\ngeospatial analysis\nnetworks\ngraphsand\nsemi-structured data.\n\nThese data types are typically used in advanced analytics, telecom, log analysis, and platform-scale systems.\nSpecial data types are used for:\n\ngeospatial analytics\nnetwork analysis\nsemi-structured data\nsystem-level identifiers\nadvanced domain modeling\n\n\nMain Special Data Types (with Example Values)\n\n\n\n\n\n\n\n\n\nData Type / Extension\nDescription\nExample Value\nTypical Use Case\n\n\n\n\nJSON / JSONB\nSemi-structured JSON data\n{‚Äúplan‚Äù:‚Äúpremium‚Äù,‚Äúusage‚Äù:120}\nLogs, APIs, configs\n\n\nARRAY\nArray of values\n{1,2,3}\nTags, multi-valued attributes\n\n\nUUID\nUniversally unique identifier\n550e8400-e29b-41d4-a716-446655440000\nDistributed IDs\n\n\nINET\nIP address\n192.168.1.1\nNetwork traffic\n\n\nCIDR\nNetwork block\n192.168.0.0/24\nSubnet modeling\n\n\nGEOMETRY (PostGIS)\nGeometric objects\nPOINT(40.18 44.51)\nMaps, locations\n\n\nGEOGRAPHY (PostGIS)\nEarth-based coordinates\nPOINT(44.51 40.18)\nDistance calculations\n\n\nltree\nHierarchical tree paths\nregion.city.store\nOrganizational trees\n\n\npgRouting\nGraph/network extension\nN/A\nNetwork routing, telecom\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost advanced data types are provided via PostgreSQL extensions, not core SQL.\n\n\n\n\nDomain-Specific Trade-offs\nSpecial data types trade generality for domain power.\n\n\n\nRequirement\nDomain\nRecommended Type\n\n\n\n\nFlexible event payloads\nLogging / APIs\nJSONB\n\n\nMulti-valued attributes\nAnalytics\nARRAY\n\n\nGlobally unique IDs\nDistributed systems\nUUID\n\n\nIP & network data\nTelecom / IT\nINET, CIDR\n\n\nLocation-based analytics\nGIS\nPostGIS (GEOMETRY / GEOGRAPHY)\n\n\nGraph traversal\nNetworks\npgRouting, graph models\n\n\n\n\n\n\nIndexing & Performance Considerations\nSpecial data types usually require specialized indexes:\n\nJSONB \\(\\rightarrow\\) GIN index\nGEOMETRY / GEOGRAPHY \\(\\rightarrow\\) GiST / SP-GiST\nARRAY \\(\\rightarrow\\) GIN\nINET / CIDR \\(\\rightarrow\\) GiST\n\nImproper indexing can make these types very slow on large datasets.\n\n\n\n\n\n\nWarningPerformance Warning\n\n\n\nAdvanced data types without proper indexes often lead to full scans, even on indexed tables.\n\n\n\n\nPostGIS (Geospatial Extension)\nPostGIS adds geospatial intelligence to PostgreSQL.\nCommon capabilities:\n\nDistance calculations\nSpatial joins\nRadius searches\nArea coverage\n\n\n\n\n\n\n\nImportant\n\n\n\nAs a data analyst you are not expected to design PostGIS schemas or routing graphs ‚Äî but you will query them.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#data-dictionary",
    "href": "materials/sql/session2.html#data-dictionary",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Data Dictionary",
    "text": "Data Dictionary\nA data dictionary documents metadata such as:\n\ntable names\ncolumn descriptions\nkeys and constraints\nrelationships\n\nAs an analyst, the data dictionary helps you understand where data lives and how to query it correctly.\n\nOverview\nThis data dictionary describes the structure and meaning of the tables used in the sales analytics database.\n\ntable purpose\ncolumn definitions\ndata types\nkeys and relationships\n\n\n\n\nTable: employees\nDescription\nStores information about employees involved in the sales process.\n\n\n\nColumn Name\nData Type\nKey\nDescription\n\n\n\n\nemployee_id\nSERIAL\nPK\nUnique identifier for each employee\n\n\nfirst_name\nTEXT\n\nEmployee first name\n\n\nlast_name\nTEXT\n\nEmployee last name\n\n\nemail\nTEXT\n\nEmployee email address\n\n\nsalary\nNUMERIC\n\nEmployee salary\n\n\n\n\n\n\nTable: customers\nDescription\nStores customer profile and location information.\n\n\n\nColumn Name\nData Type\nKey\nDescription\n\n\n\n\ncustomer_id\nINTEGER\nPK\nUnique identifier for each customer\n\n\ncustomer_name\nTEXT\n\nFull customer name\n\n\naddress\nTEXT\n\nCustomer address\n\n\ncity\nTEXT\n\nCity of residence\n\n\nzip_code\nTEXT\n\nPostal / ZIP code\n\n\n\n\n\n\nTable: products\nDescription\nContains product catalog information.\n\n\n\nColumn Name\nData Type\nKey\nDescription\n\n\n\n\nproduct_id\nINTEGER\nPK\nUnique product identifier\n\n\nproduct_name\nTEXT\n\nName of the product\n\n\nprice\nNUMERIC\n\nUnit price of the product\n\n\ndescription\nTEXT\n\nProduct description\n\n\ncategory\nTEXT\n\nProduct category\n\n\n\n\n\n\nTable: orders\nDescription\nStores order-level information and time attributes.\n\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nKey\nDescription\n\n\n\n\norder_id\nINTEGER\nPK\nUnique order identifier\n\n\norder_date\nTIMESTAMP\n\nDate and time when the order was placed\n\n\nyear\nINTEGER\n\nOrder year (derived)\n\n\nquarter\nINTEGER\n\nOrder quarter (derived)\n\n\nmonth\nTEXT\n\nOrder month name (derived)\n\n\n\n\n\n\nTable: sales\nDescription\nCentral fact table storing individual sales transaction lines.\n\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nKey\nDescription\n\n\n\n\ntransaction_id\nINTEGER\nPK\nUnique transaction identifier (surrogate key)\n\n\norder_id\nINTEGER\nFK\nReferences orders(order_id)\n\n\nproduct_id\nINTEGER\nFK\nReferences products(product_id)\n\n\ncustomer_id\nINTEGER\nFK\nReferences customers(customer_id)\n\n\nemployee_id\nINTEGER\nFK\nReferences employees(employee_id)\n\n\ntotal_sales\nNUMERIC\n\nTotal sales value for the transaction\n\n\nquantity\nINTEGER\n\nNumber of units sold\n\n\ndiscount\nNUMERIC\n\nDiscount applied to the transaction\n\n\n\n\n\n\nRelationships Summary\n\n\n\nFrom Table\nColumn\nTo Table\nColumn\nRelationship Type\n\n\n\n\nsales\norder_id\norders\norder_id\nMany-to-one\n\n\nsales\nproduct_id\nproducts\nproduct_id\nMany-to-one\n\n\nsales\ncustomer_id\ncustomers\ncustomer_id\nMany-to-one\n\n\nsales\nemployee_id\nemployees\nemployee_id\nMany-to-one\n\n\n\n\n\n\nAnalyst Notes\n\nsales is the fact table\nOther tables act as dimensions\ntransaction_id is a surrogate key\nTime attributes are intentionally denormalized\nForeign keys may be logically enforced in analytical systems",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#ddl-data-definition-language",
    "href": "materials/sql/session2.html#ddl-data-definition-language",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "DDL: Data Definition Language",
    "text": "DDL: Data Definition Language\nDDL (Data Definition Language) is responsible for defining and managing the structure of a database.\nDDL statements operate on database objects, not on individual rows.\nThey describe what the database looks like, not what data it contains.\nDDL is foundational: every data operation you perform later depends on decisions made at the DDL level.\n\nCommon DDL Statements\nDDL includes the following core commands:\n\nCREATE\nALTER\nDROP\nTRUNCATE\n\nEach of these affects database schema and metadata.\n\n\nCREATE (DDL Context)\nIn the context of DDL, CREATE is used to define new database objects, such as tables and indexes.\nTypical use cases include:\n\ncreating analytical tables\ncreating staging tables\ncreating indexes to improve performance\n\nExample: create an index to support analytical queries.\nCREATE INDEX idx_sales_product_id\nON sales (product_id);\nThis command does not modify data.\nIt changes how efficiently the database can access existing data.\n\n\nALTER\nThe ALTER statement modifies the structure of an existing database object.\nCommon use cases include:\n\nadding columns\nenforcing constraints\nchanging column properties\n\nALTER TABLE products\nADD CONSTRAINT chk_products_price\nCHECK (price &gt;= 0);\n\n\nDROP\nThe DROP statement permanently removes a database object.\nDROP INDEX idx_sales_product_id;\nDROP TABLE products;\nThis operation is irreversible and should be used with extreme caution.\n\n\nTRUNCATE\nTRUNCATE removes all rows from a table while keeping its structure intact.\nTRUNCATE TABLE sales_staging;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#crud-data-manipulation-in-practice",
    "href": "materials/sql/session2.html#crud-data-manipulation-in-practice",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "CRUD: Data Manipulation in Practice",
    "text": "CRUD: Data Manipulation in Practice\nCRUD describes how we work with data inside existing tables.\nThese are the SQL operations data analysts use most frequently.\nCRUD stands for:\n\nCREATE\nREAD\nUPDATE\nDELETE\n\n\n\n\n\n\n\nImportantCritical Terminology Clarification\n\n\n\nThe word CREATE means different things in different contexts.\n\nDDL CREATE \\(\\rightarrow\\) creates database objects (tables, indexes)\nCRUD CREATE \\(\\rightarrow\\) inserts new data (INSERT)\n\nConfusing these two is one of the most common beginner mistakes.\n\n\n\n\n\n\n\n\nNoteDDL vs CRUD ‚Äî Conceptual Comparison\n\n\n\n\nDDL \\(\\rightarrow\\) defines the structure\nCRUD \\(\\rightarrow\\) manipulates the data\nDDL \\(\\rightarrow\\) happens rarely\nCRUD \\(\\rightarrow\\) happens constantly\n\n\nDDL sets the rules.\nCRUD must follow them.\n\n\n\n\nCREATE (CRUD \\(\\rightarrow\\) INSERT)\nIn the CRUD context, CREATE means adding new rows to an existing table.\nThis is done using the INSERT statement.\nINSERT INTO products (product_id, product_name, price, category)\nVALUES (101, 'Wireless Mouse', 24.99, 'Accessories');\nYou may omit columns that allow NULL or have default values.\nINSERT INTO products (product_id, product_name, price)\nVALUES (102, 'USB-C Cable', 9.99);\n\n\nREAD (SELECT)\nThe READ operation retrieves data and is implemented using SELECT.\nSELECT\n  product_id,\n  product_name\nFROM products;\nFiltering rows is done using WHERE.\nSELECT\n  *\nFROM sales\nWHERE total_sales &lt; 50;\n\n\nUPDATE\nThe UPDATE statement modifies existing records.\nUpdates should always target specific rows.\nUPDATE products\nSET price = 49.99\nWHERE product_id = 12;\n\n\nDELETE\nThe DELETE statement removes records from a table.\nBecause deletions are irreversible, this command must be used with extreme caution.\nDELETE FROM sales\nWHERE transaction_id = 1004;\n\n\n\n\n\n\nImportantMental Model to Remember\n\n\n\n\nDDL \\(\\rightarrow\\) defines tables, columns, constraints, indexes\n\nCRUD \\(\\rightarrow\\) inserts, reads, updates, deletes rows\n\nDML \\(\\rightarrow\\) SQL language (INSERT, SELECT, UPDATE, DELETE) implementing CRUD\n\nThe first part of DML will be introduced in Session 3.\nYou can revisit it here: Session 3 DML Basics\n\n\n\n\n\n\n\n\nNoteTo Quote or Not to Quote?\n\n\n\nWhen writing SQL statements, make sure to place quotes around non-numeric values such as text and dates.\nFor example, in an INSERT command:\n\ntext values like title and description must be enclosed in quotes\nnumeric values like language_id and release_year should not\n\nDifferent database systems handle quotes differently.\nPostgreSQL accepts single quotes only (‚Äô‚Äô) for string literals, while some other systems allow both single and double quotes.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#constraints",
    "href": "materials/sql/session2.html#constraints",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "Constraints",
    "text": "Constraints\nI used the constraints above the lecture note, and decided to do a deep dive here as well :)\nIn this section, you‚Äôll be looking at constraints, which play a crucial role in keeping your data organized.\nConstraints specify what type of data a table or column can accept, and they are typically set when a table is created. When defined correctly, constraints:\n\nenforce data integrity\n\nprevent invalid or inconsistent data\n\nact as built-in data quality checks\n\nBelow are the most common constraints you will encounter when designing relational databases.\n\nUNIQUE Constraint\nThe UNIQUE constraint ensures that all values in a column are distinct.\nIt is commonly used to prevent duplicate records for attributes that must be unique across entities.\nTypical use cases include:\n\nemail addresses\nusernames\nnational identification numbers\n\nExample: ensure each customer email is unique.\nCREATE TABLE customers (\n  customer_id INTEGER PRIMARY KEY,\n  email TEXT UNIQUE,\n  phone_number TEXT\n);\nWith this constraint in place, PostgreSQL will reject any attempt to insert a duplicate email address.\n\n\nNOT NULL Constraint\nThe NOT NULL constraint ensures that a column cannot contain empty (NULL) values.\nUse this constraint for fields that are mandatory and must always be provided.\nTypical use cases include:\n\nprimary identifiers\ncontact information\ntransaction timestamps\n\nExample: ensure every customer has a phone number.\nCREATE TABLE customers (\n  customer_id INTEGER PRIMARY KEY,\n  phone_number TEXT NOT NULL,\n  email TEXT\n);\nIf an insert is attempted without a phone number, PostgreSQL will return an error.\n\n\nPRIMARY KEY Constraint\nA PRIMARY KEY uniquely identifies each row in a table.\nA primary key:\n\nmust be unique\ncannot contain NULL values\nexists only once per table\n\nExample: define a primary key for a products table.\nCREATE TABLE products (\n  product_id INTEGER PRIMARY KEY,\n  product_name TEXT NOT NULL,\n  price NUMERIC(10, 2)\n);\nThe PRIMARY KEY constraint automatically enforces both UNIQUE and NOT NULL.\n\n\nFOREIGN KEY Constraint\nA FOREIGN KEY creates a relationship between two tables by referencing the primary key of another table.\nIt ensures referential integrity, meaning that referenced values must exist in the parent table.\nExample: link sales records to customers.\nCREATE TABLE sales (\n  transaction_id INTEGER PRIMARY KEY,\n  customer_id INTEGER,\n  total_sales NUMERIC(10, 2),\n  FOREIGN KEY (customer_id)\n    REFERENCES customers (customer_id)\n);\nWith this constraint, you cannot insert a sale for a customer that does not exist in the customers table.\n\n\nCHECK Constraint\nThe CHECK constraint restricts the range or condition of values that can be inserted into a column.\nIt validates data based on logical expressions.\nTypical use cases include:\n\nvalue ranges\nminimum or maximum limits\ndomain rules\n\nExample: ensure total sales values are non-negative.\nCREATE TABLE sales (\n  transaction_id INTEGER PRIMARY KEY,\n  total_sales NUMERIC(10, 2) CHECK (total_sales &gt;= 0)\n);\nIf a value violates the condition, the insert or update will fail with an error.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session2.html#sql-rules-and-best-practices",
    "href": "materials/sql/session2.html#sql-rules-and-best-practices",
    "title": "Session 02: Intro to PostgreSQL",
    "section": "SQL Rules and Best Practices",
    "text": "SQL Rules and Best Practices\nNow that you‚Äôve seen the most common SQL statements, let‚Äôs get to grips with some basic rules and best practices for using SQL.\nThis list is not exhaustive‚Äîit is a starting point. As you gain hands-on experience, you will naturally pick up additional techniques and conventions.\n\nNumbers and Underscores\nIn a relational database:\n\nTable names and column names cannot start with a number\nThey cannot contain spaces\nUse underscores (_) to separate words\n\nThis improves compatibility and readability.\nRecommended\nCREATE TABLE customer_lifecycle;\nCREATE TABLE customer;\nNot Recommended\nCREATE TABLE customer lifecycle;\nCREATE TABLE 3customer;\n\n\nCapitalization\nA widely accepted convention is:\n\nSQL keywords ‚Üí UPPERCASE\nTable names ‚Üí lowercase\nColumn names ‚Üí lowercase\n\nThis makes queries easier to scan and understand.\nRecommended\nSELECT product_id, product_name\nFROM products;\nSELECT transaction_id, total_sales\nFROM sales;\nNot Recommended\nSelect PRODUCT_ID, PRODUCT_NAME From PRODUCTS;\nSELECT TRANSACTION_ID, TOTAL_SALES FROM SALES;\n\n\nCommenting\nIt is good practice to comment your SQL code so that other analysts and developers can easily read and understand your queries.\nComments do not affect query execution‚Äîthey exist purely for documentation and readability.\nUse -- for single-line comments.\n-- select and display the first 10 rows from the sales table\nSELECT *\nFROM sales\nLIMIT 10;\nMulti-line comments can also be used to temporarily disable blocks of SQL code during testing or debugging.\n/*\nSELECT\n  s.transaction_id,\n  p.product_name,\n  s.total_sales\nFROM sales s\nJOIN products p\n  ON s.product_id = p.product_id\nWHERE s.total_sales &gt; 100;\n*/\n\n\nAliasing\nAliases assign temporary names to columns or tables within a query.\nThey are especially useful for:\n\nimproving readability\nshortening long names\nresolving ambiguity in joins\n\nColumn aliases rename columns in the query output.\nSELECT\n  product_name AS item_name,\n  price AS unit_price\nFROM products;\nTable aliases are commonly used in joins.\nSELECT\n  s.transaction_id,\n  p.product_name,\n  s.total_sales\nFROM sales AS s\nJOIN products AS p\n  ON s.product_id = p.product_id;\n\n\n\n\n\n\nTip\n\n\n\nHere you will find much more tips",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 02: Intro to PostgreSQL"
    ]
  },
  {
    "objectID": "materials/sql/session8.html",
    "href": "materials/sql/session8.html",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "",
    "text": "Relational databases do not start with JOINs.\nThey start with structure.\nNormalization explains why data is split, and JOINs explain how it is recombined.\nIf this logic is not clear, JOINs feel like a technical burden instead of a logical necessity.\n\n\nThe Core Problem: Redundancy and Anomalies\nImagine storing everything in a single table.\nWe are saying Unnormalized Design:\n\n\n\norder_id\ncustomer_name\ncity\nproduct\nprice\nquantity\n\n\n\n\n1\nAnna\nYerevan\nPhone\n500\n1\n\n\n2\nAnna\nYerevan\nCase\n20\n2\n\n\n3\nArman\nGyumri\nPhone\n500\n1\n\n\n\n\nCustomer data is duplicated\n\nProduct prices are duplicated\n\nUpdates are risky\n\nDeletes may remove important information\n\nInserts may require fake or incomplete values\n\nThis structure is fragile and error-prone.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#normalization",
    "href": "materials/sql/session8.html#normalization",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "",
    "text": "Relational databases do not start with JOINs.\nThey start with structure.\nNormalization explains why data is split, and JOINs explain how it is recombined.\nIf this logic is not clear, JOINs feel like a technical burden instead of a logical necessity.\n\n\nThe Core Problem: Redundancy and Anomalies\nImagine storing everything in a single table.\nWe are saying Unnormalized Design:\n\n\n\norder_id\ncustomer_name\ncity\nproduct\nprice\nquantity\n\n\n\n\n1\nAnna\nYerevan\nPhone\n500\n1\n\n\n2\nAnna\nYerevan\nCase\n20\n2\n\n\n3\nArman\nGyumri\nPhone\n500\n1\n\n\n\n\nCustomer data is duplicated\n\nProduct prices are duplicated\n\nUpdates are risky\n\nDeletes may remove important information\n\nInserts may require fake or incomplete values\n\nThis structure is fragile and error-prone.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#first-normal-form-1nf",
    "href": "materials/sql/session8.html#first-normal-form-1nf",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "First Normal Form (1NF)",
    "text": "First Normal Form (1NF)\nA table is in First Normal Form (1NF) if:\n\nEach column contains atomic values\n\nThere are no repeating groups\n\nEach row is uniquely identifiable\n\n\n\n\n\n\n\nImportant\n\n\n\nThe table above already satisfies 1NF, but it is still poorly designed because multiple entities are mixed together.\n\n\n\n\n\n\n\nerDiagram\n    ORDERS {\n        int order_id\n        string customer_name\n        string city\n        string product\n        float price\n        int quantity\n    }",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#second-normal-form-2nf",
    "href": "materials/sql/session8.html#second-normal-form-2nf",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Second Normal Form (2NF)",
    "text": "Second Normal Form (2NF)\nA table is in Second Normal Form (2NF) if:\n\nIt is already in 1NF\n\nNo non-key attribute depends on part of a composite key\n\nTo understand this, we must first identify the logical key of the original table.\nIn the unnormalized table, the logical key is: (order_id, product)\nbecause:\n\none order can contain multiple products\nquantity is defined per product per order\n\n\nProblem\nLook at the dependencies:\n\ncustomer_name, city depend only on order_id\n\nprice depends only on product\n\nquantity depends on (order_id, product)\n\nThis means we have partial dependencies.\nSome columns depend on only part of the key, not the whole key.\n\\[\\downarrow\\]\nThis violates 2NF.\n\n\n\n\n\nflowchart LR\n    K[(order_id, product)]\n    K --&gt; Q[quantity]\n    order_id --&gt; C[customer_name, city]\n    product --&gt; P[price]\n\n\n\n\n\n\n\n\nDecomposition to 2NF\nTo fix this, we separate data so that each table describes exactly one relationship.\n\nCustomers\n\n\n\ncustomer_id\ncustomer_name\ncity\n\n\n\n\n\nMeaning:\n\ncustomer attributes depend only on customer_id\n\n\n\nProducts\n\n\n\nproduct_id\nproduct\nprice\n\n\n\n\n\nMeaning:\n\nproduct attributes depend only on product_id\n\n\n\nOrders\n\n\n\norder_id\ncustomer_id\n\n\n\n\n\nMeaning:\n\nthis table describes who placed the order\ncustomer_id depends entirely on order_id\n\nThere is no composite key here, so partial dependency is impossible.\n\n\n\nOrder Items\n\n\n\norder_id\nproduct_id\nquantity\n\n\n\n\n\nMeaning:\n\nthis table describes what was ordered\nthe key is (order_id, product_id)\nquantity depends on the entire key, not just one part\n\n\n\n\n\n\nerDiagram\n    CUSTOMERS {\n        int customer_id PK\n        string customer_name\n        string city\n    }\n\n    PRODUCTS {\n        int product_id PK\n        string product\n        float price\n    }\n\n    ORDERS {\n        int order_id PK\n        int customer_id FK\n    }\n\n    ORDER_ITEMS {\n        int order_id FK\n        int product_id FK\n        int quantity\n    }\n\n    CUSTOMERS ||--o{ ORDERS : places\n    ORDERS ||--o{ ORDER_ITEMS : contains\n    PRODUCTS ||--o{ ORDER_ITEMS : included_in\n\n\n\n\n\n\n\n\nWhy This Satisfies 2NF\nAfter decomposition:\n\nCustomer attributes depend only on customers:\n\ncustomer_name\ncity\n\nProduct attributes depend only on products\n\nproduct_name\nprice\n\nOrder attributes depend only on orders\n\norder_id\ncustomer_id\n\nQuantity depends on both order and** product**\n\norder_id\nproduct_id\nquantity\n\n\nAs a rusult:\n\nThere are no partial dependencies left.\nEach non-key attribute now depends on the entire key of its table.\n\n\\[\\downarrow\\]\nThis is exactly what Second Normal Form requires.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#third-normal-form-3nf",
    "href": "materials/sql/session8.html#third-normal-form-3nf",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Third Normal Form (3NF)",
    "text": "Third Normal Form (3NF)\nAfter reaching Second Normal Form (2NF), we removed partial dependencies.\nHowever, another type of problem can still exist: transitive dependencies.\nA table is in Third Normal Form (3NF) if:\n\nIt is already in 2NF\n\nNo non-key attribute depends on another non-key attribute\n\nIn other words:\n\nEvery non-key attribute must depend only on the primary key and nothing else.\n\n\nTransitive Dependency\nA transitive dependency occurs when:\n\nA non-key attribute depends on another non-key attribute\n\nInstead of depending directly on the primary key\n\nFormally:\n\n\n\n\n\nflowchart LR\n    customer_id --&gt; city\n    city --&gt; country\n    customer_id --&gt; country\n\n\n\n\n\n\nThis is a transitive dependency.\nConsider the Customers table after 2NF:\n\n\n\ncustomer_id\ncustomer_name\ncity\ncountry\n\n\n\n\n\nLet‚Äôs analyze the dependencies:\n\ncustomer_id ‚Üí city\n\ncity ‚Üí country\n\n\\[\\downarrow\\]\ncustomer_id ‚Üí country indirectly\nSo country does not depend directly on customer_id.\nIt depends on city.\nThis violates Third Normal Form.\n\n\n\nProblems\nThis structure creates anomalies:\n\nIf a city changes its country name, multiple rows must be updated\n\nIf the last customer from a city is deleted, the city‚Äìcountry relationship is lost\n\nIf a new city is added, a customer must exist first\n\nThese are update, delete, and insert anomalies caused by transitive dependency.\n\n\nDecomposition to 3NF\nTo remove the transitive dependency, we split the table based on real-world entities.\n\n\nCities\n\n\n\ncity_id\ncity\ncountry_id\n\n\n\n\n\nMeaning:\n\ncity attributes depend only on city_id\nthe relationship between city and country is stored once\n\n\n\n\nCountries\n\n\n\ncountry_id\ncountry\n\n\n\n\n\n\ncountry attributes depend only on country_id\n\n\n\n\nCustomers\n\n\n\ncustomer_id\ncustomer_name\ncity_id\n\n\n\n\n\n\n\n\ncustomer attributes depend only on customer_id\ncity information is referenced, not duplicated\n\n\n\n\n\n\n\nerDiagram\n    CUSTOMERS {\n        int customer_id PK\n        string customer_name\n        int city_id FK\n    }\n\n    CITIES {\n        int city_id PK\n        string city\n        int country_id FK\n    }\n\n    COUNTRIES {\n        int country_id PK\n        string country\n    }\n\n    CUSTOMERS ||--|| CITIES : lives_in\n    CITIES ||--|| COUNTRIES : belongs_to\n\n\n\n\n\n\n\n\n\nWhy This Satisfies 3NF\nAfter decomposition:\n\ncustomer_name depends only on customer_id\n\ncity_id depends only on customer_id\n\ncountry depends only on country_id\n\nThere are no indirect dependencies\n\nEach table now represents one concept and one level of dependency.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#normalization-summary",
    "href": "materials/sql/session8.html#normalization-summary",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Normalization Summary",
    "text": "Normalization Summary\n\nFirst Normal Form removing duplicate rows\nSecond Normal Form removes partial dependencies.\n\nThird Normal Form removes transitive dependencies.\n\nAt this point:\n\nData redundancy is minimized\n\nAnomalies are eliminated\n\nRelationships are explicit\n\n\n\n\n\n\nflowchart TB\n    A[Unnormalized] --&gt; B[1NF&lt;br/&gt;Atomic Values]\n    B --&gt; C[2NF&lt;br/&gt;No Partial Dependencies]\n    C --&gt; D[3NF&lt;br/&gt;No Transitive Dependencies]\n\n\n\n\n\n\n\n\n\n\n\n\nTipAfter 3NF‚Ä¶\n\n\n\n3NF commonly considered sufficient in practice for many transactional systems.\n\nBoyce-Codd Normal Form (BCNF) BCNF is a stricter refinement of 3NF A table:\n\nFor every functional dependency (X ‚Üí Y), X must be a superkey (a unique identifier for the table).\nThis eliminates certain anomalies that can still exist in 3NF designs.\n\nFourth Normal Form (4NF): Removes multi-valued dependencies (if the table is already in BCNF).\nFifth Normal Form (5NF): Eliminates join dependencies beyond 4NF.\nSixth Normal Form (6NF) and others exist mostly for theoretical completeness.\n\nFor more information you can visit here",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#new-analytical-schema",
    "href": "materials/sql/session8.html#new-analytical-schema",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "New Analytical Schema",
    "text": "New Analytical Schema\nThis schema is intentionally designed to support:\n\nINNER / LEFT / RIGHT / FULL OUTER joins\n\nSELF joins\n\nSPATIAL joins\n\nWindow functions\n\nSubqueries\n\nCTEs\n\n\nCreating a schema and tables\nPopulate data using CSV-based loading\n\n\nDocker Update\nAs usal, we are starting from starting our docker containers:\n\nthe database\npgadmin (viewer)\n\n\nStep 1: Stop and clean existing containers\nWe first stop the containers and remove volumes (-v ensures that old database data is removed).\ndocker compose down -v\n\n\nStep 2: Update the Docker image\nIn docker-compose.yaml, update the database service:\n\nBefore\nimage: postgres:17\nAfter\nimage: postgis/postgis:17-3.4\nAdd also a new volume\n- ./data/analytics_schema:/data:ro\n\nThis image includes:\n\nPostgreSQL 17\n\nPostGIS 3.4\n\nAll required spatial libraries\n\n\n\nStep 3: Remove persisted data folders\nTo ensure a clean initialization, delete the following folders if they exist:\n\npostgres_data/\n\npgadmin_data/\n\nThese folders store old volumes and may conflict with the new image.\n\n\nStep 4: Start containers again\nFirst start normally and verify everything works:\ndocker compose up\nOnce confirmed, stop and restart in detached mode:\ndocker compose up -d\n\n\n\nAdd PostGIS Extention\nOnce the PostGIS-enabled container is running, we enable the extension inside the database.\nCREATE EXTENSION IF NOT EXISTS postgis;\nTo verify that PostGIS is installed correctly:\nSELECT PostGIS_Version();\nThe above code should provide a table with postgis_version: 3.4 USE_GEOS=1 USE_PROJ=1 USE_STATS=1\nIf this returns a version string, PostGIS is ready.\n\n\nCreating New Schema\nWe do not use the default public schema for analytics.\nInstead, we create a dedicated schema called analytics.\n\nif you right click on the schemas\n\nCREATE SCHEMA IF NOT EXISTS analytics;\n\n\n\n\n\n\n\nCautionSET search_path TO analytics\n\n\n\nPostgreSQL databases can contain multiple schemas.\nWhen you write:\nSELECT * FROM customers;\nPostgreSQL must decide which schema to search for the customers table.\nBy default, it searches the public schema.\nBy running: SET search_path TO analytics; we tell PostgreSQL:\n\n‚ÄúLook in the analytics schema first when resolving table names.‚Äù\n\nSELECT \n  *\nFROM orders o\nJOIN customers c ON o.customer_id = c.customer_id;\ninstead of:\nSELECT \n  *\nFROM analytics.orders o\nJOIN analytics.customers c ON o.customer_id = c.customer_id;\n\n\n\n\nSync with GitHub\nRemember to push the changes into GitHub: 1. git add docker-compose.yam 2. git commit -m \"adding postgis extention\" 3. git push",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#creating-geographical-tables",
    "href": "materials/sql/session8.html#creating-geographical-tables",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Creating Geographical Tables",
    "text": "Creating Geographical Tables\nNow we define the analytical data model that will be used throughout this module.\nThe schema follows a normalized (3NF) design with a clear geographic hierarchy:\nCountry ‚Üí Region ‚Üí City ‚Üí Customer\n\nYou can check the newly created analytics schema, and refresh it after each new table creation.\n\n\nCountires\nThis table represents sovereign countries and serves as the top-level geographic entity.\nCREATE TABLE analytics.countries (\n    country_id   INT PRIMARY KEY,\n    country_name TEXT NOT NULL\n);\n\n\nRegions\nCREATE TABLE analytics.regions (\n    region_id   INT PRIMARY KEY,\n    region_name TEXT NOT NULL,\n    country_id  INT NOT NULL REFERENCES analytics.countries(country_id)\n);\n\n\nCities\nCREATE TABLE analytics.cities (\n    city_id   INT PRIMARY KEY,\n    city_name TEXT NOT NULL,\n    region_id INT NOT NULL REFERENCES analytics.regions(region_id)\n);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#creating-non-geographical-tables",
    "href": "materials/sql/session8.html#creating-non-geographical-tables",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Creating Non Geographical Tables",
    "text": "Creating Non Geographical Tables\n\nCustomers\nCREATE TABLE analytics.customers (\n    customer_id INT PRIMARY KEY,\n    first_name  TEXT NOT NULL,\n    last_name   TEXT NOT NULL,\n    age         INT CHECK (age BETWEEN 16 AND 100),\n    email       TEXT UNIQUE,\n    city_id     INT REFERENCES analytics.cities(city_id),\n    signup_date DATE NOT NULL\n);\n\n\nProducts\nCREATE TABLE analytics.products (\n    product_id   INT PRIMARY KEY,\n    product_name TEXT NOT NULL,\n    category     TEXT NOT NULL,\n    price        NUMERIC(10,2) NOT NULL\n);\n\n\nOrders\nCREATE TABLE analytics.orders (\n    order_id    INT PRIMARY KEY,\n    customer_id INT REFERENCES analytics.customers(customer_id),\n    order_date  DATE NOT NULL,\n    status      TEXT NOT NULL\n);\n\n\nOrder Items\nCREATE TABLE analytics.order_items (\n    order_item_id INT PRIMARY KEY,\n    order_id      INT NOT NULL REFERENCES analytics.orders(order_id),\n    product_id    INT NOT NULL REFERENCES analytics.products(product_id),\n    quantity      INT NOT NULL CHECK (quantity &gt; 0)\n);\n\n\nCountry Boundaries\nThese tables extend the relational model with geometries, enabling spatial joins.\nCREATE TABLE analytics.country_boundaries (\n    country_id INT PRIMARY KEY REFERENCES analytics.countries(country_id),\n    geom       GEOMETRY(MultiPolygon, 4326)\n);\n\n\nRegion Boundaries\nCREATE TABLE analytics.region_boundaries (\n    region_id INT PRIMARY KEY REFERENCES analytics.regions(region_id),\n    geom      GEOMETRY(Polygon, 4326)\n);\n\n\nCity Boundaries\nCREATE TABLE analytics.city_boundaries (\n    city_id INT PRIMARY KEY REFERENCES analytics.cities(city_id),\n    geom    GEOMETRY(Polygon, 4326)\n);\n\n\nCustomer Locations\nCREATE TABLE analytics.customer_locations (\n    customer_id INT PRIMARY KEY REFERENCES analytics.customers(customer_id),\n    geom        GEOMETRY(Point, 4326)\n);",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#design-summary",
    "href": "materials/sql/session8.html#design-summary",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Design Summary",
    "text": "Design Summary\nAt this stage, the database contains:\n\nA fully qualified analytics schema\n\nNormalized relational tables (3NF)\n\nHierarchical geographic dimensions\n\nFact tables for analytical workloads\n\nSpatial tables for PostGIS joins\n\nCheck out the this repository on my side\n\nERD\nIn order to generate and check the ERD you need to:\n\nright click on analytisc schema\nselect ERD\nyou must see below image\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn case you are encountering an error please do the following\nfunction postgis_typmod_type(integer) does not exist\nLINE 1: SELECT postgis_typmod_type(i) FROM\n^\nHINT: No function matches the given name and argument types. You might need to add explicit type casts.\nDROP EXTENSION IF EXISTS postgis CASCADE;\nSET search_path TO public;\nCREATE EXTENSION postgis;\nSELECT PostGIS_Version();\nIt should return: postgis_version: 3.4 USE_GEOS=1 USE_PROJ=1 USE_STATS=1\n\n\n\n\nSync with GitHub\nSave the quiries in proper file similar this repository and push do GitHub.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#populating-the-data",
    "href": "materials/sql/session8.html#populating-the-data",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Populating the Data",
    "text": "Populating the Data\nIn your folder now you need to have the following structure\ndata/\n‚îú‚îÄ‚îÄ public_schema/\n‚îÇ   ‚îî‚îÄ‚îÄ ...            # existing dummy data (unchanged)\n‚îî‚îÄ‚îÄ analytics_schema/\n    ‚îú‚îÄ‚îÄ countries.csv\n    ‚îú‚îÄ‚îÄ regions.csv\n    ‚îú‚îÄ‚îÄ cities.csv\n    ‚îú‚îÄ‚îÄ customers.csv\n    ‚îú‚îÄ‚îÄ products.csv\n    ‚îú‚îÄ‚îÄ orders.csv\n    ‚îú‚îÄ‚îÄ order_items.csv\n    ‚îú‚îÄ‚îÄ country_boundaries.csv\n    ‚îú‚îÄ‚îÄ region_boundaries.csv\n    ‚îú‚îÄ‚îÄ city_boundaries.csv\n    ‚îî‚îÄ‚îÄ customer_locations.csv\n\nChecking\ntry this:\nSELECT pg_ls_dir('/data');\n\n\n\nanalytics.countries\nCOPY analytics.countries\nFROM '/data/countries.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.countries;\n\n\n\nanalytics.regions\nCOPY analytics.regions\nFROM '/data/regions.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.regions;\n\n\n\nanalytics.cities\nCOPY analytics.cities\nFROM '/data/cities.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.cities;\n\n\n\nanalytics.customers\nCOPY analytics.customers\nFROM '/data/customers.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.customers LIMIT 10;\n\n\n\nanalytics.products\nCOPY analytics.products\nFROM '/data/products.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.products;\n\n\n\nanalytics.orders\nCOPY analytics.orders\nFROM '/data/orders.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.orders LIMIT 10;\n\n\n\nanalytics.order_items\nCOPY analytics.order_items\nFROM '/data/order_items.csv'\nCSV HEADER;\n\nSELECT * FROM analytics.order_items LIMIT 10;\n\n\nBoundarie Tables\nHere we need to do something important transformation, by creating temporary tables\n\nCreating Staging Tables\nCREATE TABLE IF NOT EXISTS analytics._stg_country_boundaries (\n    country_id INT,\n    wkt TEXT\n);\n\n\nCREATE TABLE IF NOT EXISTS analytics._stg_region_boundaries (\n    region_id INT,\n    wkt TEXT\n);\n\nCREATE TABLE IF NOT EXISTS analytics._stg_city_boundaries (\n    city_id INT,\n    wkt TEXT\n);\n\nCREATE TABLE IF NOT EXISTS analytics._stg_points (\n    point_id INT,\n    wkt TEXT\n);\n\n\n\nCOPY CSVs into staging tables\nCOPY analytics._stg_country_boundaries\nFROM '/data/country_boundaries.csv'\nCSV HEADER;\n\nSELECT * FROM analytics._stg_country_boundaries;\n\nCOPY analytics._stg_region_boundaries\nFROM '/data/region_boundaries.csv'\nCSV HEADER;\n\nSELECT * FROM analytics._stg_region_boundaries;\n\nCOPY analytics._stg_city_boundaries\nFROM '/data/city_boundaries.csv'\nCSV HEADER;\n\nSELECT * FROM analytics._stg_city_boundaries;\n\n\nCOPY analytics._stg_points\nFROM '/data/customer_locations.csv'\nCSV HEADER;\n\nSELECT * FROM analytics._stg_points;\n\n\nanalytics.country_boundaries\nINSERT INTO analytics.country_boundaries (country_id, geom)\nSELECT\n  country_id,\n  ST_GeomFromText(wkt, 4326)\nFROM analytics._stg_country_boundaries;\n\n\nanalytics.region_boundaries\nINSERT INTO analytics.region_boundaries (region_id, geom)\nSELECT\n  region_id,\n  ST_GeomFromText(wkt, 4326)\nFROM analytics._stg_region_boundaries;\n\n\n\nanalytics.city_boundaries\nINSERT INTO analytics.city_boundaries (city_id, geom)\nSELECT\n  city_id,\n  ST_GeomFromText(wkt, 4326)\nFROM analytics._stg_city_boundaries;\n\n\nanalytics.customer_locations\nINSERT INTO analytics.customer_locations (customer_id, geom)\nSELECT\n  point_id,\n  ST_GeomFromText(wkt, 4326)\nFROM analytics._stg_points;\n\n\nGeometry checks\nSELECT\n  COUNT(*) FILTER (WHERE ST_IsValid(geom)) AS valid_geom,\n  COUNT(*) AS total\nFROM analytics.country_boundaries;\nSELECT\n  ST_GeometryType(geom),\n  COUNT(*)\nFROM analytics.country_boundaries\nGROUP BY 1;\n\n\n\nGeometry Validation\nSELECT\n  COUNT(*) FILTER (WHERE ST_IsValid(geom)) AS valid_geometries,\n  COUNT(*) AS total_geometries\nFROM analytics.city_boundaries;\nSELECT\n  COUNT(*) FILTER (WHERE ST_SRID(geom) = 4326) AS correct_srid,\n  COUNT(*) AS total_geometries\nFROM analytics.city_boundaries;",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#table-joins",
    "href": "materials/sql/session8.html#table-joins",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Table JOINs",
    "text": "Table JOINs\n\nINNER JOIN | Only Matching Rows\nQuestion: Which customers have placed orders?\nflowchart LR\n  C[customers] --&gt;|customer_id| O[orders]\nOnly rows where customers.customer_id = orders.customer_id are kept.\nSELECT\n  c.customer_id,\n  c.first_name,\n  o.order_id,\n  o.order_date\nFROM analytics.customers c\nINNER JOIN analytics.orders o\n  ON c.customer_id = o.customer_id;\n\nCustomers without orders are excluded\nOrders without customers are excluded\n\n\n\n\nLEFT JOIN | Preserve the Base Table\nQuestion: Show all customers, even if they never ordered.\n\n\n\n\n\nflowchart LR\n  C[customers] --&gt;|customer_id| O[orders]\n  C --&gt; R[Result]\n  O --&gt; R\n\n\n\n\n\n\nAll customers are preserved.\nOrders are optional.\nSELECT\n  c.customer_id,\n  c.first_name,\n  o.order_id\nFROM analytics.customers c\nLEFT JOIN analytics.orders o\n  ON c.customer_id = o.customer_id;\n\nCustomers without orders appear with NULL values\nBase table = customers\n\n\n\n\nLEFT JOIN + NULL FILTER | Anti-Join Pattern\nQuestion: Which customers have never ordered?\n\n\n\n\n\nflowchart LR\n  C[customers] --&gt;|customer_id| O[orders]\n  O -. no match .-&gt; N[NULL]\n\n\n\n\n\n\nSELECT\n  c.customer_id,\n  c.first_name\nFROM analytics.customers c\nLEFT JOIN analytics.orders o\n  ON c.customer_id = o.customer_id\nWHERE o.order_id IS NULL;\n\nVery common interview and analytics pattern\nIdentifies absence of relationships\n\n\n\n\nOne-to-Many JOIN | Orders to Order Items\nQuestion: What products were sold in each order?\nflowchart LR\n  O[orders] --&gt;|order_id| OI[order_items] --&gt;|product_id| P[products]\nSELECT\n  o.order_id,\n  p.product_name,\n  oi.quantity\nFROM analytics.orders o\nJOIN analytics.order_items oi\n  ON o.order_id = oi.order_id\nJOIN analytics.products p\n  ON oi.product_id = p.product_id;\n\nOne order ‚Üí many order items\nRows multiply by design\n\n\n\n\nAggregation After JOIN | Controlling Row Explosion\nQuestion: Total revenue per order.\n\n\n\n\n\nflowchart LR\n  O[orders] --&gt; OI[order_items] --&gt; P[products]\n  P --&gt; SUM[Sum of quantity times price]\n\n\n\n\n\n\nSELECT\n  o.order_id,\n  SUM(oi.quantity * p.price) AS order_revenue\nFROM analytics.orders o\nJOIN analytics.order_items oi\n  ON o.order_id = oi.order_id\nJOIN analytics.products p\n  ON oi.product_id = p.product_id\nGROUP BY o.order_id;\n\nAggregation collapses multiplied rows\nCritical in analytical SQL\n\n\n\n\nHierarchical JOIN | Customer Geography\nQuestion: Where is each customer located?\n\n\n\n\n\nflowchart LR\n  CO[countries] --&gt; R[regions] --&gt; CI[cities] --&gt; C[customers]\n\n\n\n\n\n\nSELECT\n  c.customer_id,\n  ci.city_name,\n  r.region_name,\n  co.country_name\nFROM analytics.customers c\nJOIN analytics.cities ci\n  ON c.city_id = ci.city_id\nJOIN analytics.regions r\n  ON ci.region_id = r.region_id\nJOIN analytics.countries co\n  ON r.country_id = co.country_id;\n\nDimension-style hierarchy\nCommon in BI models\n\n\n\n\nSpatial JOIN | Geometry Meets Business Data\nQuestion: Is a customer physically inside their declared city?\n\n\n\n\n\nflowchart LR\n  CL[customer_locations] --&gt;|ST_Within| CB[city_boundaries]\n\n\n\n\n\n\nSELECT\n  c.customer_id,\n  ci.city_name,\n  ST_Within(cl.geom, cb.geom) AS inside_city\nFROM analytics.customers c\nJOIN analytics.customer_locations cl\n  ON c.customer_id = cl.customer_id\nJOIN analytics.cities ci\n  ON c.city_id = ci.city_id\nJOIN analytics.city_boundaries cb\n  ON ci.city_id = cb.city_id;\n\nCombines relational joins with spatial predicates\nTypical PostGIS analytical pattern\n\n\n\n\nMany-to-Many Effect | Why Row Counts Explode\n\n\n\n\n\nflowchart LR\n  O[orders] --&gt; OI[order_items]\n  OI --&gt; P[products]\n\n\n\n\n\n\nSELECT COUNT(*) AS joined_rows\nFROM analytics.orders o\nJOIN analytics.order_items oi\n  ON o.order_id = oi.order_id;\n\nEach order may contain multiple products\nEach product may appear in many orders\nAnalysts must expect multiplication",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/sql/session8.html#final-sql-project-kickoff",
    "href": "materials/sql/session8.html#final-sql-project-kickoff",
    "title": "Session 08: Data Analysis with SQL | JOINs",
    "section": "Final SQL Project | Kickoff",
    "text": "Final SQL Project | Kickoff\n\nProject Goal\nDesign and deliver a production-grade analytical SQL project that demonstrates:\n\nProper normalization\n\nRelational and spatial modeling\n\nAnalytical querying\n\nDocumentation and reproducibility\n\nThis project simulates a real analytics workflow, from raw data to insights.\n\n\n\nStep 1 | Select a Business Domain\nChoose one primary business table that genuinely interests your team.\nExamples:\n\nCustomer transactions\n\nProduct sales\n\nService usage events\n\nLocation-based activity\n\nMandatory requirement:\n\nThe domain must include geography\nGeography can be:\n\nCustomer locations\nService areas\nStore regions\nDelivery zones\n\n\nIf unsure, propose your domain and validate it before proceeding.\n\n\nStep 2 | Normalize the Data Model\nStart from a single wide table (conceptual or real).\nExample (not final):\n\ncustomer_name\n\nproduct_name\n\ncity\n\nregion\n\ncountry\n\norder_date\n\nprice\n\nNormalize into logical entities:\n\nCountries\n\nRegions\n\nCities\n\nCustomers\n\nEvents / Orders / Activities\n\nProducts (if applicable)\n\nSpatial boundaries\n\nRules:\n\nRemove redundancy\n\nUse surrogate or natural keys consistently\n\nEnforce referential integrity\n\n\n\n\nStep 3 | Create a New Database and Schema\nCreate a dedicated database for the project.\nCREATE DATABASE final_sql_project; -- any meaningful name which goes according to your project\nCreate a dedicated schema.\nCREATE SCHEMA analytics;\nSET search_path TO analytics;\n\nAll tables must live under this schema\nNever use public\n\n\n\n\nStep 4 | Create Tables (DDL)\nDefine all tables explicitly:\n\nPrimary keys\n\nForeign keys\n\nConstraints\n\nGeometry types with SRID\n\nCREATE TABLE analytics.example_table (\n  id INT PRIMARY KEY,\n  geom GEOMETRY(Point, 4326)\n);\n\nSpatial tables are mandatory\nUse PostGIS correctly\n\n\n\n\nStep 5 | Populate the Database\nPopulate tables using CSV-based loading.\n\nUse COPY or staging tables\nData volume should be realistic: more that 10k rows\n\nCOPY analytics.table_name\nFROM '/data/table_name.csv'\nCSV HEADER;\n\nValidate row counts\nValidate referential integrity\nValidate geometries using ST_IsValid\n\n\n\n\nStep 6 | Create the ERD\nProduce an Entity Relationship Diagram that shows:\n\nTables\n\nPrimary keys\n\nForeign keys\n\nCardinality\n\n\n\n\nStep 7 | Push the changes into GitHub\nCreate a dedicated GitHub repository.\nRepository structure example:\n\n/queries\n/init\n/data\n/erd\n/docs\nREADME.md\ndocker-comose.yaml\n.env\n\nPush:\n\nDDL scripts\n\nCSV files (or generation scripts)\n\nERD\n\nDocumentation\n\n\n\n\n\n\n\nWarning\n\n\n\nCommit early and often.\n\n\n\n\n\nStep 8 | Define Analytical Questions\nBefore writing complex SQL, define business questions.\nExamples:\n\nRevenue by country / region / city\n\nCustomer distribution by geography\n\nTop products per region\n\nCustomers outside expected boundaries\n\nGrowth trends over time\n\nEach question should map to specific joins.\n\n\nStep 9 | Construct a Denormalized Analytical Table\nCreate a flattened analytical table.\n\nJoin all necessary dimensions\nOne row per analytical grain (e.g., order, customer-day)\n\nCREATE TABLE analytics.fact_orders AS\nSELECT\n  o.order_id,\n  o.order_date,\n  c.customer_id,\n  ci.city_name,\n  r.region_name,\n  co.country_name,\n  SUM(oi.quantity * p.price) AS revenue\nFROM analytics.orders o\nJOIN analytics.customers c ON o.customer_id = c.customer_id\nJOIN analytics.cities ci ON c.city_id = ci.city_id\nJOIN analytics.regions r ON ci.region_id = r.region_id\nJOIN analytics.countries co ON r.country_id = co.country_id\nJOIN analytics.order_items oi ON o.order_id = oi.order_id\nJOIN analytics.products p ON oi.product_id = p.product_id\nGROUP BY\n  o.order_id, o.order_date,\n  c.customer_id, ci.city_name,\n  r.region_name, co.country_name;\n\nThis table will be reused later for:\n\nSubqueries\nWindow functions\nAdvanced analytics\n\n\n\n\n\nStep 10 | Visualization\nUse any tool: SQL client charts is recommended\nFocus on:\n\nGeography-based insights\nTrends\nComparisons\n\n\n\n\nStep 11 | Create a Data Dictionary\nDocument every table and column.\nExample:\n\nTable name\n\nColumn name\n\nData type\n\nDescription\n\nConstraints\n\nStore as: Markdown (.md)\nThis is a professional requirement, not optional.\n\n\n\nStep 12 | Final Review and Presentation\nDeliverables:\n\nSQL database\n\nERD\n\nGitHub repository\n\nAnalytical tables\n\nAnswered business questions\n\nVisuals\n\nData dictionary\n\nFinal step:\n\nPresent:\n\nDesign decisions\nNormalization logic\nJoin strategy\nAnalytical findings\n\n\n\n\nEvaluation Focus\n\nCorrect normalization\n\nCorrect joins\n\nProper use of geography\n\nClean SQL\n\nReproducibility\n\nAnalytical thinking\n\nThis project simulates real-world analytics work.",
    "crumbs": [
      "Syllabus",
      "SQL",
      "SQL",
      "Session 08: Data Analysis with SQL | JOINs"
    ]
  },
  {
    "objectID": "materials/lab/git.html",
    "href": "materials/lab/git.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "macOS: Install Xcode Command Line Tools. Open Terminal and run:\n\nxcode-select --install\n\n\nLinux: Git may already be installed. If not, install it using:\n\nsudo apt install git\n\n\nWindows: Download Git for Windows from\nhttps://git-scm.com/download/win and install it with default settings.\n\n\nTo verify that Git is installed, run:\ngit --version\n\n\n\n\n\n\nFollow these steps to create a GitHub account:\n\nOpen the GitHub website:\nhttps://github.com/\n\n\n\nClick Sign up in the top-right corner. \nEnter your:\n\nEmail address\n\nPassword\n\nUsername\n\n\n\n\nChoose your email verification preferences.\nComplete the verification challenge. \nClick Create account.\nGitHub will send a verification code to your email. Enter the code to activate your account. \nChoose the Free plan (sufficient for this course).\n\nAfter this, your GitHub account is fully ready.\n\n\n\n\n\n\nRun the following command to see your current configuration (should be mostly empty on new installations):\ngit config --global --list\n\n\n\n\ngit config --global user.name \"your_name\"\n\n\n\n\ngit config --global user.email \"your_email\"\n\n\n\n\nCheck again:\ngit config --global --list\n\n\n\n\n\n\nGitHub no longer accepts passwords for Git operations.\nYou must generate a Personal Access Token (PAT).\n\n\n\nSign in to your GitHub account.\nNavigate to:\nProfile ‚Üí Settings ‚Üí Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)\n\n\n\nClick Generate new token. \nSelect scopes (minimum recommended):\n\nrepo\nread:org\nworkflow\nor simply select all\n\n\n\n\nSet No expiration.\n\n\n\nClick Generate token.\nCopy the token and store it securely.\n\n\nYou will paste this token when executing:\ngit push\n\n\n\n\n\nSSH keys allow secure password-less authentication with GitHub.\n\n\n\nOpen Terminal or PowerShell.\nGenerate a new SSH key:\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n\n\nWhen prompted for a location, press ENTER to use the default:\n\n~/.ssh/id_rsa\n\n\nWhen asked for a passphrase:\n\nleave empty for no passphrase\n\nor enter a secure passphrase\n\nDisplay your public key:\n\ncat ~/.ssh/id_rsa.pub\n\n\n\n\nmacOS:\npbcopy &lt; ~/.ssh/id_rsa.pub\nWindows PowerShell:\nGet-Content ~/.ssh/id_rsa.pub | Set-Clipboard\n\n\n\n\n\nGo to GitHub ‚Üí Settings ‚Üí SSH and GPG keys\nClick New SSH key\nPaste your key\nSave\n\n\n\n\n\n\n\nLog in to your GitHub account.\nClick the New repository button. \nFill in:\n\nRepository name\n\nDescription (optional)\n\nSet it to Public\n\nDo NOT initialize with README (we will create files locally). \nClick Create repository.\n\nGitHub will show you the remote URL, e.g.:\nhttps://github.com/username/myrepo.git\n\n\nChoose a folder on your computer.\nInitialize Git in that folder:\n\ngit init\n\n\nConnect your local repo to the remote repo:\n\ngit remote add origin https://github.com/username/myrepo.git\n\n\nVerify remote:\n\ngit remote -v"
  },
  {
    "objectID": "materials/lab/git.html#installation-and-configuration",
    "href": "materials/lab/git.html#installation-and-configuration",
    "title": "Git and GitHub",
    "section": "",
    "text": "macOS: Install Xcode Command Line Tools. Open Terminal and run:\n\nxcode-select --install\n\n\nLinux: Git may already be installed. If not, install it using:\n\nsudo apt install git\n\n\nWindows: Download Git for Windows from\nhttps://git-scm.com/download/win and install it with default settings.\n\n\nTo verify that Git is installed, run:\ngit --version"
  },
  {
    "objectID": "materials/lab/git.html#sign-up-for-github",
    "href": "materials/lab/git.html#sign-up-for-github",
    "title": "Git and GitHub",
    "section": "",
    "text": "Follow these steps to create a GitHub account:\n\nOpen the GitHub website:\nhttps://github.com/\n\n\n\nClick Sign up in the top-right corner. \nEnter your:\n\nEmail address\n\nPassword\n\nUsername\n\n\n\n\nChoose your email verification preferences.\nComplete the verification challenge. \nClick Create account.\nGitHub will send a verification code to your email. Enter the code to activate your account. \nChoose the Free plan (sufficient for this course).\n\nAfter this, your GitHub account is fully ready."
  },
  {
    "objectID": "materials/lab/git.html#git-configuration",
    "href": "materials/lab/git.html#git-configuration",
    "title": "Git and GitHub",
    "section": "",
    "text": "Run the following command to see your current configuration (should be mostly empty on new installations):\ngit config --global --list\n\n\n\n\ngit config --global user.name \"your_name\"\n\n\n\n\ngit config --global user.email \"your_email\"\n\n\n\n\nCheck again:\ngit config --global --list"
  },
  {
    "objectID": "materials/lab/git.html#generate-personal-access-token-pat",
    "href": "materials/lab/git.html#generate-personal-access-token-pat",
    "title": "Git and GitHub",
    "section": "",
    "text": "GitHub no longer accepts passwords for Git operations.\nYou must generate a Personal Access Token (PAT).\n\n\n\nSign in to your GitHub account.\nNavigate to:\nProfile ‚Üí Settings ‚Üí Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)\n\n\n\nClick Generate new token. \nSelect scopes (minimum recommended):\n\nrepo\nread:org\nworkflow\nor simply select all\n\n\n\n\nSet No expiration.\n\n\n\nClick Generate token.\nCopy the token and store it securely.\n\n\nYou will paste this token when executing:\ngit push"
  },
  {
    "objectID": "materials/lab/git.html#generate-ssh-key-optional-but-recommended",
    "href": "materials/lab/git.html#generate-ssh-key-optional-but-recommended",
    "title": "Git and GitHub",
    "section": "",
    "text": "SSH keys allow secure password-less authentication with GitHub.\n\n\n\nOpen Terminal or PowerShell.\nGenerate a new SSH key:\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n\n\nWhen prompted for a location, press ENTER to use the default:\n\n~/.ssh/id_rsa\n\n\nWhen asked for a passphrase:\n\nleave empty for no passphrase\n\nor enter a secure passphrase\n\nDisplay your public key:\n\ncat ~/.ssh/id_rsa.pub\n\n\n\n\nmacOS:\npbcopy &lt; ~/.ssh/id_rsa.pub\nWindows PowerShell:\nGet-Content ~/.ssh/id_rsa.pub | Set-Clipboard\n\n\n\n\n\nGo to GitHub ‚Üí Settings ‚Üí SSH and GPG keys\nClick New SSH key\nPaste your key\nSave\n\n\n\n\n\n\n\nLog in to your GitHub account.\nClick the New repository button. \nFill in:\n\nRepository name\n\nDescription (optional)\n\nSet it to Public\n\nDo NOT initialize with README (we will create files locally). \nClick Create repository.\n\nGitHub will show you the remote URL, e.g.:\nhttps://github.com/username/myrepo.git\n\n\nChoose a folder on your computer.\nInitialize Git in that folder:\n\ngit init\n\n\nConnect your local repo to the remote repo:\n\ngit remote add origin https://github.com/username/myrepo.git\n\n\nVerify remote:\n\ngit remote -v"
  },
  {
    "objectID": "materials/lab/git.html#homework",
    "href": "materials/lab/git.html#homework",
    "title": "Git and GitHub",
    "section": "Homework",
    "text": "Homework\nThis task ensures that you can create repositories, work locally, work remotely, and synchronize changes between GitHub and your machine.\nFollow the steps below exactly in order.\n\n\nCreate a Remote Repository on GitHub\n\nLog in to your GitHub account.\nClick New Repository.\nSet:\n\nRepository name: choose any name\n\nVisibility: Public\n\nDo NOT initialize with README.\nClick Create Repository.\n\n\nCopy your remote URL, e.g.:\nhttps://github.com/yourusername/myrepo.git\n\n\n\nClone the Repository to Your Computer\nIn a folder of your choice, open Terminal or PowerShell and run:\ngit clone https://github.com/yourusername/myrepo.git\nThis will create a new local folder with the repository inside.\n\n\n\n\nCreate a Dummy File Locally\nNavigate into the cloned folder:\ncd myrepo\nCreate a simple text file:\necho \"This is my first Git test file.\" &gt; test.txt\nStage and commit the file:\ngit add test.txt\ngit commit -m \"Add test file\"\nPush it to GitHub:\ngit push\n\nCheck GitHub ‚Äî you should now see test.txt in the repository.\n\n\n\nAdd a New File on GitHub (Remote)\n\nOpen your repo on GitHub.\nClick Add file ‚Üí Create new file.\nName it: remote_file.txt\nAdd any text inside it.\nCommit the new file.\n\n\n\n\nPull the New File to Your Computer\nReturn to your local repo folder and run:\ngit pull\nYou should now see remote_file.txt in your local directory.\n\nThis confirms that your local and remote repositories are synchronized.\n\n\n\nSubmission\nSubmit your GitHub repository and GitHub profile links here.\nExample format:\nhttps://github.com/yourusername/myrepo"
  },
  {
    "objectID": "materials/lab/vscode.html",
    "href": "materials/lab/vscode.html",
    "title": "Vs Code",
    "section": "",
    "text": "Download Visual Studio Code from the official website:\ndownload link\n\nChoose the installer for your operating system:\n\nWindows (System Installer)\nmacOS (Universal)\n\n\n\n\n\n\n\nRun the installer. \nEnable the following options during setup:\n\nAdd to PATH (Environment Variables)\nAdd Open with Code to File Explorer context menu\nRegister VS Code as the default editor for supported file types \n\n\n\n\n\nOpen PowerShell or Command Prompt and run:\ncode .\n\n\n\n\nIf the terminal reports that code is not recognized:\n\nOpen Edit the system environment variables. \nSelect Environment Variables.\nUnder the Path variable, add the directory where Code.exe is located, for example:\n\nC:\\Users\\&lt;username&gt;\\AppData\\Local\\Programs\\Microsoft VS Code\\\nRestart the terminal after updating.\n\n\n\n\n\n\n\nDownload the .dmg file from here \nDrag Visual Studio Code.app into the Applications folder. \n\n\n\n\n\nOpen VS Code.\nPress Command+Shift+P to open the Command Palette.\nRun:\n\nShell Command: Install 'code' command in PATH\n\n\n\n\nOpen Terminal and run:\ncode .\n\n\n\n\n\n\n\n\nPython\nPylance\nJupyter \n\n\n\n\n\nGitLens\nGitHub Pull Requests and Issues \n\n\n\n\n\nDocker\nDev Containers (Remote Containers) \n\n\n\n\n\nMarkdown All in One\nMarkdown Preview Enhanced\nYAML \n\n\n\n\n\nProject Manager\nBetter Comments\nCode Spell Checker"
  },
  {
    "objectID": "materials/lab/vscode.html#download",
    "href": "materials/lab/vscode.html#download",
    "title": "Vs Code",
    "section": "",
    "text": "Download Visual Studio Code from the official website:\ndownload link\n\nChoose the installer for your operating system:\n\nWindows (System Installer)\nmacOS (Universal)"
  },
  {
    "objectID": "materials/lab/vscode.html#windows-installation",
    "href": "materials/lab/vscode.html#windows-installation",
    "title": "Vs Code",
    "section": "",
    "text": "Run the installer. \nEnable the following options during setup:\n\nAdd to PATH (Environment Variables)\nAdd Open with Code to File Explorer context menu\nRegister VS Code as the default editor for supported file types \n\n\n\n\n\nOpen PowerShell or Command Prompt and run:\ncode .\n\n\n\n\nIf the terminal reports that code is not recognized:\n\nOpen Edit the system environment variables. \nSelect Environment Variables.\nUnder the Path variable, add the directory where Code.exe is located, for example:\n\nC:\\Users\\&lt;username&gt;\\AppData\\Local\\Programs\\Microsoft VS Code\\\nRestart the terminal after updating."
  },
  {
    "objectID": "materials/lab/vscode.html#macos-installation",
    "href": "materials/lab/vscode.html#macos-installation",
    "title": "Vs Code",
    "section": "",
    "text": "Download the .dmg file from here \nDrag Visual Studio Code.app into the Applications folder. \n\n\n\n\n\nOpen VS Code.\nPress Command+Shift+P to open the Command Palette.\nRun:\n\nShell Command: Install 'code' command in PATH\n\n\n\n\nOpen Terminal and run:\ncode ."
  },
  {
    "objectID": "materials/lab/vscode.html#recommended-extensions",
    "href": "materials/lab/vscode.html#recommended-extensions",
    "title": "Vs Code",
    "section": "",
    "text": "Python\nPylance\nJupyter \n\n\n\n\n\nGitLens\nGitHub Pull Requests and Issues \n\n\n\n\n\nDocker\nDev Containers (Remote Containers) \n\n\n\n\n\nMarkdown All in One\nMarkdown Preview Enhanced\nYAML \n\n\n\n\n\nProject Manager\nBetter Comments\nCode Spell Checker"
  },
  {
    "objectID": "materials/lab/docker.html",
    "href": "materials/lab/docker.html",
    "title": "Docker",
    "section": "",
    "text": "Download Docker Desktop from the official website:\ndownload link\nChoose the installer for your operating system:\n\nWindows (Docker Desktop for Windows)\nmacOS (Docker Desktop for Mac ‚Äì Intel or Apple Silicon)\n\n\n\n\n\n\n\n\n\n\nDownload Docker Desktop for Windows. \nRun the installer. \nDuring installation, ensure the following:\n\nEnable Use WSL 2 instead of Hyper-V (recommended)\nAllow required components to install (WSL 2, Linux kernel updates) \n\nRestart the system if prompted.\n\n\n\n\nOpen PowerShell or Command Prompt and run:\ndocker --version\n\nThen verify Docker is running:\ndocker run hello-world\n\nIf you see a ‚ÄúHello from Docker!‚Äù message, the installation is successful.\n\n\n\nIf Docker reports that WSL 2 is not installed:\nInstall WSL 2 manually:\nwsl --install\n\nRestart your machine and try again.\nIf Docker fails to start, ensure:\n\nVirtualization is enabled in BIOS.\nWSL is enabled in Windows Features.\n\n\n\n\n\n\n\n\n\n\n\nDownload Docker Desktop for macOS. \nOpen the .dmg file.\nDrag Docker.app into the Applications folder. \nStart Docker Desktop from Applications.\n\n\n\n\nOpen Terminal and run:\ndocker --version\n\nThen run:\ndocker run hello-world\n\nIf the container runs successfully, Docker is installed correctly.\n\n\n\n\n\n\n\nOpen Docker Desktop ‚Üí Settings ‚Üí Resources, and adjust:\n\nCPU\nMemory\nDisk image size\n\n\nFor database-heavy workloads, set at least:\n\n2 CPUs\n\n4 GB RAM\n\n\n\n\nOpen Docker Desktop ‚Üí Settings ‚Üí Kubernetes ‚Üí Enable Kubernetes.\n\nWe will not cover it in this course.\n\n\n\n\n\n\n\n\nDocker Compose (included in Docker Desktop)\nDev Containers (VS Code extension)\nDocker CLI\n\n\n\n\n\n\ndocker stats\ndocker ps -a\ndocker logs &lt;container&gt;\ndocker exec -it &lt;container&gt; /bin/bash\n\n\nWe will go over these during live sessions.\n\n\n\n\npostgres:latest\npgadmin:latest\npython:3.12-slim\njupyter/base-notebook"
  },
  {
    "objectID": "materials/lab/docker.html#download",
    "href": "materials/lab/docker.html#download",
    "title": "Docker",
    "section": "",
    "text": "Download Docker Desktop from the official website:\ndownload link\nChoose the installer for your operating system:\n\nWindows (Docker Desktop for Windows)\nmacOS (Docker Desktop for Mac ‚Äì Intel or Apple Silicon)"
  },
  {
    "objectID": "materials/lab/docker.html#windows-installation",
    "href": "materials/lab/docker.html#windows-installation",
    "title": "Docker",
    "section": "",
    "text": "Download Docker Desktop for Windows. \nRun the installer. \nDuring installation, ensure the following:\n\nEnable Use WSL 2 instead of Hyper-V (recommended)\nAllow required components to install (WSL 2, Linux kernel updates) \n\nRestart the system if prompted.\n\n\n\n\nOpen PowerShell or Command Prompt and run:\ndocker --version\n\nThen verify Docker is running:\ndocker run hello-world\n\nIf you see a ‚ÄúHello from Docker!‚Äù message, the installation is successful.\n\n\n\nIf Docker reports that WSL 2 is not installed:\nInstall WSL 2 manually:\nwsl --install\n\nRestart your machine and try again.\nIf Docker fails to start, ensure:\n\nVirtualization is enabled in BIOS.\nWSL is enabled in Windows Features."
  },
  {
    "objectID": "materials/lab/docker.html#macos-installation",
    "href": "materials/lab/docker.html#macos-installation",
    "title": "Docker",
    "section": "",
    "text": "Download Docker Desktop for macOS. \nOpen the .dmg file.\nDrag Docker.app into the Applications folder. \nStart Docker Desktop from Applications.\n\n\n\n\nOpen Terminal and run:\ndocker --version\n\nThen run:\ndocker run hello-world\n\nIf the container runs successfully, Docker is installed correctly."
  },
  {
    "objectID": "materials/lab/docker.html#post-installation-setup",
    "href": "materials/lab/docker.html#post-installation-setup",
    "title": "Docker",
    "section": "",
    "text": "Open Docker Desktop ‚Üí Settings ‚Üí Resources, and adjust:\n\nCPU\nMemory\nDisk image size\n\n\nFor database-heavy workloads, set at least:\n\n2 CPUs\n\n4 GB RAM\n\n\n\n\nOpen Docker Desktop ‚Üí Settings ‚Üí Kubernetes ‚Üí Enable Kubernetes.\n\nWe will not cover it in this course."
  },
  {
    "objectID": "materials/lab/docker.html#recommended-docker-tools",
    "href": "materials/lab/docker.html#recommended-docker-tools",
    "title": "Docker",
    "section": "",
    "text": "Docker Compose (included in Docker Desktop)\nDev Containers (VS Code extension)\nDocker CLI\n\n\n\n\n\n\ndocker stats\ndocker ps -a\ndocker logs &lt;container&gt;\ndocker exec -it &lt;container&gt; /bin/bash\n\n\nWe will go over these during live sessions.\n\n\n\n\npostgres:latest\npgadmin:latest\npython:3.12-slim\njupyter/base-notebook"
  },
  {
    "objectID": "materials/tableau/index.html",
    "href": "materials/tableau/index.html",
    "title": "Tableau",
    "section": "",
    "text": "Tableau Session 01: Introduction To Tableau\n\n\nWorkbooks, Data Types, Fields, Basic Charts, Filtering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Session 02:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Session 03:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Session 04: Dashboard Design & Performance\n\n\nStrorytelling, Color Theory, UX Principles For Dashboard Design\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Syllabus",
      "Tableau"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#visual-storytelling-principles",
    "href": "materials/tableau/slides/session4.html#visual-storytelling-principles",
    "title": "Data Analyst",
    "section": "Visual Storytelling Principles",
    "text": "Visual Storytelling Principles\nThe Role of Edward Tufte in Visual Storytelling\nIn the field of visual storytelling, Edward R. Tufte plays a foundational role.\nHis work connects data, design, and narrative, showing how information can be communicated not only clearly but also elegantly.\nTufte‚Äôs philosophy encourages designers and analysts to transform raw data into visual stories that inform and move audiences.\n\n‚ÄúAbove all else, show the data.‚Äù\n\nWho is Edward Tufte?\nEdward R. Tufte is an American statistician, artist, and former Yale professor, often called the father of data visualization.\nHe argues that visual storytelling should combine:\n\nTruth ‚Äì no distortion of data\n\nClarity ‚Äì no unnecessary complexity\n\nDesign integrity ‚Äì every element has a purpose\n\nHis most influential books include:\n\nThe Visual Display of Quantitative Information\n\nEnvisioning Information\n\nBeautiful Evidence\n\nTufte‚Äôs principles are not about decoration but purposeful simplicity ‚Äî every mark, color, or shape should tell part of the story.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#color-theory-fundamentals",
    "href": "materials/tableau/slides/session4.html#color-theory-fundamentals",
    "title": "Data Analyst",
    "section": "Color Theory Fundamentals",
    "text": "Color Theory Fundamentals\nColor is Deceptive\n\nColor is DeceptiveColor perception is context-dependent:\n\nThe same color may appear lighter/darker depending on its background.\n\nLighting, screen quality, and surrounding colors influence perception.\n\nWhen designing dashboard palettes, always consider:\n\nContext (background, other visuals)\n\nAudience (accessibility, color vision)\n\nGoal of the visualization (highlight vs neutral tone)",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#color-harmony-schemes-for-dashboards",
    "href": "materials/tableau/slides/session4.html#color-harmony-schemes-for-dashboards",
    "title": "Data Analyst",
    "section": "Color Harmony Schemes for Dashboards",
    "text": "Color Harmony Schemes for Dashboards\nUnderstanding color relationships helps you create balanced, readable dashboards.\nMonochromatic Scheme\n\n\n\nMonochromiatic scheme\n\n\n\nVariations of a single hue (tints, shades, tones).\n\nVery cohesive, calm, and professional.\n\nIdeal for minimalist dashboards and background colors.\n\nAnalogous Scheme\n\n\n\nAnalogous scheme\n\n\n\nUses colors next to each other on the wheel.\n\nCreates smooth, natural transitions.\n\nWorks well for gradients or multi-series charts with subtle differences.\n\nComplementary Scheme\n\n\n\nComplementary scheme\n\n\n\nColors opposite each other on the wheel.\n\nHigh contrast and strong visual energy.\n\nGreat for highlighting key metrics or ‚Äúgood vs bad‚Äù signals.\n\nCompound Scheme\n\n\n\nCompound scheme\n\n\n\nMix of two or more non-adjacent colors.\n\nOften forms a rectangle or square on the color wheel.\n\nBalances variety and harmony.\n\nTetradic Scheme\n\n\n\nTetradic scheme\n\n\n\nUses four colors evenly spaced on the wheel.\n\nVery vibrant and expressive.\n\nShould be used carefully to avoid visual overload.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#design-principles-for-color-in-dashboards",
    "href": "materials/tableau/slides/session4.html#design-principles-for-color-in-dashboards",
    "title": "Data Analyst",
    "section": "Design Principles for Color in Dashboards",
    "text": "Design Principles for Color in Dashboards\nEffective dashboards combine color harmony, functionality, readability, consistency, and accessibility.\nColor Harmony\nChoose colors that work well together and reflect the brand or context.\nLimit the number of hues; vary intensity instead.\nFunctionality\nColor should:\n\nGuide attention\n\nSignal status (e.g., red vs green)\n\nHelp group related elements\n\nEvery color should have a clear role.\nReadability\nUse color to support comprehension, not to decorate.\nSemantic example:\n\nGreen ‚Üí growth, success, ‚Äúgood‚Äù\n\nRed ‚Üí decline, risk, ‚Äúbad‚Äù\n\nSimplicity Through Consistency\n\nReuse the same colors for the same concepts across dashboards.\n\nToo many colors reduce clarity and create cognitive overload.\n\nAccessibility\nDesign for users with color vision deficiencies:\n\nDo not rely on color alone to encode information.\n\nEnsure strong contrast between foreground and background.\n\nTest palettes with tools like Coblis.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#ux-principles-for-dashboard-design",
    "href": "materials/tableau/slides/session4.html#ux-principles-for-dashboard-design",
    "title": "Data Analyst",
    "section": "UX Principles for Dashboard Design",
    "text": "UX Principles for Dashboard Design\nThis section summarizes 7 key UX principles for effective dashboards.\n1. User-Centricity\nDesign for specific users and use cases, not for ‚Äúeveryone‚Äù.\nAsk:\n\nWho is my audience? (role, data literacy, time)\n\nWhat decisions will they make with this dashboard?\n\nDo they need a quick overview, detailed analysis, or both?\n\nAnalysts may want detail and complexity; executives prefer clarity and summaries.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#dashboard-layout-best-practices",
    "href": "materials/tableau/slides/session4.html#dashboard-layout-best-practices",
    "title": "Data Analyst",
    "section": "Dashboard Layout Best Practices",
    "text": "Dashboard Layout Best Practices\nUsing Layout Containers in Tableau\nContainers help you organize and align content.\nTypes:\n\nHorizontal containers ‚Äì arrange elements side by side.\n\nVertical containers ‚Äì stack elements top to bottom.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#types-of-dashboards",
    "href": "materials/tableau/slides/session4.html#types-of-dashboards",
    "title": "Data Analyst",
    "section": "Types of Dashboards",
    "text": "Types of Dashboards\nFollowing Stephen Few, we classify dashboards by function:\n\nOperational\n\nTactical\n\nAnalytical\n\nStrategic\n\nMultifunctional / Self-Service\n\nEach serves a different decision-making level.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#reflection-and-discussion",
    "href": "materials/tableau/slides/session4.html#reflection-and-discussion",
    "title": "Data Analyst",
    "section": "Reflection and Discussion",
    "text": "Reflection and Discussion\n\nWhich dashboard type do you use most often in your current work or study?\n\nWhere do you see chartjunk or poor color use in real dashboards around you?\n\nHow could you redesign one of your own dashboards using Tufte‚Äôs principles and color harmony?\n\nTake notes or share examples in class.",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session4.html#week-4-assignment-end-to-end-business-dashboard",
    "href": "materials/tableau/slides/session4.html#week-4-assignment-end-to-end-business-dashboard",
    "title": "Data Analyst",
    "section": "Week 4 Assignment ‚Äì End-to-End Business Dashboard",
    "text": "Week 4 Assignment ‚Äì End-to-End Business Dashboard\nBuild a business dashboard in Tableau that:\n\nUses one clear narrative (e.g., ‚ÄúSales performance‚Äù, ‚ÄúNetwork health‚Äù, ‚ÄúCustomer churn‚Äù).\n\nIncludes at least three sheets (e.g., KPI view, trend view, breakdown view).\n\nApplies color theory (choose a scheme and document it briefly).\n\nImplements at least two interactive elements:\n\nFilters / quick filters\n\nParameters\n\nHighlight or filter actions\n\nURL or sheet-swap actions\n\n\nDemonstrates good layout and hierarchy using containers and padding.\n\nIncludes a short story caption (2‚Äì3 sentences) summarizing the main insight.\n\nOptional: publish to Tableau Public and share the link in the course chat..",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session2.html#slide-1",
    "href": "materials/tableau/slides/session2.html#slide-1",
    "title": "Data Analyst",
    "section": "Slide 1",
    "text": "Slide 1",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/slides/session2.html#slide-2",
    "href": "materials/tableau/slides/session2.html#slide-2",
    "title": "Data Analyst",
    "section": "Slide 2",
    "text": "Slide 2",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Slides",
      "Data Analyst"
    ]
  },
  {
    "objectID": "materials/tableau/session2.html",
    "href": "materials/tableau/session2.html",
    "title": "Tableau Session 02:",
    "section": "",
    "text": "Tableau Tutorial\nComming Soon",
    "crumbs": [
      "Syllabus",
      "Tableau",
      "Tableau",
      "Tableau Session 02:"
    ]
  },
  {
    "objectID": "materials/capstone/index.html",
    "href": "materials/capstone/index.html",
    "title": "Capstone Project",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "materials/capstone/slides/session1.html#comming.-soon",
    "href": "materials/capstone/slides/session1.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon"
  },
  {
    "objectID": "materials/capstone/slides/session3.html#comming.-soon",
    "href": "materials/capstone/slides/session3.html#comming.-soon",
    "title": "Data Analyst",
    "section": "Comming. Soon",
    "text": "Comming. Soon"
  }
]